\documentclass{patmorin}
\usepackage{pat}
\usepackage[utf8]{inputenc}
\usepackage[noend]{algorithmic}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\keywords}[1]{\vspace{2em}\noindent\textbf{Keywords:} #1}
%\newcommand{\from}{\colon}

\title{\MakeUppercase{Encoding Arguments}}
\author{Pat Morin, Tommy Reddad, and Wolfgang Mulzer}
\date{}

\begin{document}
\begin{titlepage}
\maketitle


\begin{abstract}
\setlength{\baselineskip}{15.84pt}
This expository article surveys ``encoding arguments.'' In their most most
basic form an encoding argument proves an upper bound on the probability
of an event using the fact a uniformly random choice from a set of size
$N$ can not be encoded with fewer than $\log N$ bits on average.

We survey many applications of this basic argument, give a generalization
of this argument for the case of non-uniform distributions, and give a
rigorous justification for the use of non-integer codeword lengths in
encoding arguments.  These latter two results allow encoding arguments to be applied more widely and to produce tighter results.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}
\setlength{\baselineskip}{15.84pt}
There is no doubt that probability theory plays a fundamental role
in computer science: Some of the fastest and simplest fundamental
algorithms and data structures are randomized; average-case analysis of
algorithms relies entirely on tools from probability theory; and many
difficult combinatorial questions have strikingly simple solutions using
probabilistic arguments.

Unfortunately, many of these beautiful results are inaccessible to most
computer scientists because of a view that ``the math is too hard.''
For instance, ACM's CS2013 Final Report does not require a full course
in probability theory \cite[Page~50]{computing-curricula:computer}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on discrete
probability, as part of the discrete structures curriculum.

In this expository paper, we survey applications of ``encoding arguments''
that tranforms the problem of upper-bounding the probability of a specific
event, $\mathcal{E}$, into the problem of devising a code for the set
of elementary events in 
$\mathcal{E}$.  Encoding arguments have several advantages over
traditional probabilistic analysis:

\begin{enumerate}
  \item Encoding arguments are almost ``probability-free.''  Except for
  applying a simple \emph{Uniform Encoding Lemma}, there is no probability
  involved.  In particular, there is no chance to make common mistakes
  such as multiplying probabilities of non-independent events or
  (equivalently) multiplying expectations.

  The proof of the Uniform Encoding Lemma itself is trivial and the only
  probability it uses is that the fact, if a finite set $X$ contains $r$
  special elements and we pick an element uniformly at random from $X$,
  then the probability of picking a special element is $r/|X|$.

  \item Encoding arguments usually yield strong results;
  $\Pr\{\mathcal{E}\}$ typically decreases at least exponentially in
  the parameter of interest. Traditionally, these strong concentration
  results require (at least) careful calculations on probabilities of
  independent events and/or the application of concentration inequalities.
  The subject of concentration inequalities is advanced enough to be
  the topic of entire textbooks \cite{boucheron.lugosi.ea:concentration,dubhashi.panconesi:concentration}.
  
  \item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing an
  efficient code---an algorithmic problem. Consider the following 
  two problems:
    \begin{enumerate}

    \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
       a random graph on $n$ vertices contains a clique of size $k=\lceil
       4\log n\rceil$.

    \item Design a encoding for graphs on $n$ vertices so that a graph,
       $G$, that contains a clique of size $k=\lceil 4\log n\rceil$
       is encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note:
       Your encoding and decoding algorithms don't have to be efficient,
       just correct.)
    \end{enumerate}
  Many computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that
  they can use Boole's Inequality will still be stuck
  wrestling with the formula $\binom{n}{4\log n}2^{-\binom{k}{2}}$.  
\end{enumerate}

Our motivation for this work is that encoding arguments are an easily
accessible, yet versatile tool for answering many questions.  Most of
these arguments can be applied after learning almost no probability
theory beyond the Encoding Lemma mentioned above.

The remainder of this article is organized as follows: In \secref{uel},
we present necessary background, including the \emph{Uniform
Encoding Lemma}, which is the basis of most of our encoding arguments.
\Secref{applications-i} presents  applications of the Uniform Encoding
Lemma to a variety of problems.  \Secref{nuel} presents a more
general Non-Uniform Encoding Lemma that can handle a larger variety of
applications, some of which are presented in \secref{applications-ii}.
\Secref{weights} presents an alternative view of encoding arguments in
terms of weight functions; this alternative view justifies the use of
non-integer codeword lengths.  \Secref{summary} summarizes and concludes
with some directions for future research.



\section{Background}
\seclabel{uel}

This section presents the necessary background on prefix-free codes and binomial coefficients.

\subsection{Prefix-free Codes and the Uniform Encoding Lemma}

A \emph{code}, $C\from X\to \{0,1\}^*$ is a one-to-one function from a set
$X$ to the set of binary strings.  The elements of the range of $C$ are
called $C$'s \emph{codewords}.  In most cases, there are some elements of
the set $X$ that are not of interest to us.  In these cases, we consider
partial codes. A \emph{partial code} $C\from X\nrightarrow \{0,1\}^*$ is
a one-to-one partial function.  When discussing partial codes we will use
the convention that $|C(x)|=\infty$ if $x$ is not in the domain of $C$.

A (partial) code, $C$, is \emph{prefix-free} if, for every pair $x\neq y$
in the domain of $C$ the binary string $C(x)$ is not a prefix of $C(y)$.
It can be helpful to think of prefix-free codes as (rooted ordered)
binary trees whose leaves are labelled with the elements in $S$.
The codeword for a particular $x\in X$ is obtained by tracing the
root-to-leaf path leading to $x$ and outputting a 0 (respectively, 1)
each time this path goes from a parent to its left (respectively, right)
child. (See \figref{bintree}.)

\begin{figure}
  \centering{\includegraphics{bintree}}
  \caption{A prefix-free code for the set
    $S=\{\mathtt{a},\mathtt{b},\mathtt{c},\mathtt{d},\mathtt{e},\mathtt{f}\}$
    and the corresponding leaf-labelled binary tree.}
  \figlabel{bintree}
\end{figure}

If $C$ is prefix-free, then the number of $C$'s codewords that have length
\emph{at most} $k$ is not more than $2^k$. To see this why this is so,
observe that $C$ can be modified into a code $\hat C$, in which every
codeword of length $\ell <k$ is extended---by appending $k-\ell$ zeros---so that
it has length exactly $k$. The prefix-freeness of $C$ ensures that $\hat
C$ is also a prefix-free code). The number of $\hat C$'s codewords of
length $k$ is equal to the the number of $C$'s codewords of length at
most $k$; since codewords are just binary strings, there are not more
than $2^k$ of these.

Observe that every finite set $X$ has a prefix-free code in which every
codeword has length $\lceil\log |X|\rceil$. We simply enumerate the
elements of $X$ in some order $x_0,x_1,\ldots,x_{|X|-1}$ and assign to
each $x_i$ the binary representation of $i$ (padded with leading zeros),
which has length $\lceil\log |X|\rceil$, since $i\in\{0,\ldots,|X|-1\}$.
We will use this type of \emph{fixed-length code} implicitly in many arguments.


As we will see, the following lemma, which is folklore, is surprisingly
versatile:
\begin{lem}[Uniform Encoding Lemma]\lemlabel{uel}
  Let $C\from X\nrightarrow \{0,1\}^*$ be a prefix-free partial code. If
  an element $x\in X$ is chosen uniformly at random, then $\Pr\{|C(x)|\le
  \log|X|-s\}\le 2^{-s}$.
\end{lem}

\begin{proof}
  Let $k=\log|X|-s$ and recall that $C$ has at most $2^{k}$ codewords
  length at most $k$.  Since $C$ is one-to-one each such codeword has
  at most one preimage in $X$.  Since $x$ is chosen uniformly at random
  from $X$, the probability that it is the preimage of one of these
  short codewords is at most
  \[
     \frac{2^k}{|X|} = \frac{2^{\log|X|-s}}{|X|} = 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

\subsection{Runs in Binary Strings}

As a warm-up exercise to illustrate the use of the Uniform Encoding
Lemma we will show that a random $n$-bit string is unlikely to contain
a run of significantly more than $\log n$ one bits.  (See \figref{runs-i}.)

\begin{figure}
  \centering{\includegraphics{runs-i}}
  \caption{Illustration of \thmref{runs-i} and its proof.}
  \figlabel{runs-i}
\end{figure}

\begin{thm}\thmlabel{runs-i}
  Let $x=\langle x_1,\ldots,x_n\rangle\in\{0,1\}^n$ be chosen
  uniformly at random and let $t=\lceil\log(n-t)\rceil + s$. Then, the
  probability that there exists an $i\in\{1,\ldots,n-t-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  We will prove this theorem by constructing a partial prefix-free
  code for strings having a run of $t$ or more 1's.  For such a string
  string $x=\langle x_1,\ldots,x_n\rangle$ let $i$ be the minimum index
  such that $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$. The codeword, $C(x)$,
  for $x$ is the binary string that consists of the ($\lceil\log
  (n-t)\rceil$-bit binary encoding of the) index $i$ followed by the $n-t$
  bits $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$. (see \figref{runs-i}.)

  Observe that $C(x)$ has length 
  \[
      \lceil\log n\rceil + n - t = n-s \enspace .
  \]
  It is easy to check that, for any such $x$, we can reconstruct
  $x_1,\ldots,x_n$ from $C(x)$, so $C$ is indeed a partial code whose
  domain is the set of binary strings of length $n$ having a run of $t$
  or more 1's.  It is also easy to see that $C$ is prefix free, since
  all codewords are unique and have the same length.

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, the probability
  that there exists any index $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+k-1}=1$ is at most
  \[
      \Pr\{|C(x)|\le n-s\} \le 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

Simple as it is, the proof of \thmref{runs-i} contains the main ideas
used in most encoding arguments:

\begin{enumerate}
\item The arguments usually show that a particular \emph{bad event} is
  unlikely.  In \thmref{runs-i} the bad event is the occurrence of a
  substring of $t$ consecutive 1's.

\item The code is partial prefix-free code whose domain contains the 
  set of bad events.  In this case, the code $C$ is only capable of
  encoding strings containing a run of $t$ consecutive 1's.

\item The code usually begins with a
  concise description of the bad event, and is then followed by a
  straightforward encoding of the information that is not implied by the
  bad event. In \thmref{runs-i}, the bad event is completely described by
  the index $i$ at which the run of $t$ 1 bits begins, and this implies
  that the bits $x_i,\ldots,x_{i+t-1}$ are all equal to 1, so these bits
  do not need to be specified in the second part of the codeword.
\end{enumerate}

Note that \thmref{runs-i} also has an easy proof using the union bound:
If we let $\mathcal{E}_i$ denote the event $x_i=x_{i+1}=\cdots x_{i+t}=1$, then
\begin{align*}
\Pr \bigcup_{i=0}^{n-t-1} \mathcal{E}_i  
   & \le \sum_{i=0}^{n-t-1} \Pr\mathcal{E}_i & \text{(using the union bound)}\\
   & = \sum_{i=0}^{n-t-1} 2^{-t} & \text{(using the independence of the $x_i$'s)}\\
   & = (n-t)2^{-t} \\
   & = (n-t)2^{-\lceil\log(n-t)\rceil-s} \\
   & \le 2^{-s} \enspace .
\end{align*}
This traditional proof also works with the sometimes smaller value
$t=\log(n-t)+s$ (note the lack of a ceiling over the logarithmic term),
in which case the final inequality becomes an equality.

In the encoding proof of \thmref{runs-i}, the ceiling in the expression
for $t$ is an artifact of encoding of the integer $i$ which is taken
from a set of $n-t$. When sketching an encoding argument, we think of
this as requiring $\log (n-t)$ bits but when it comes time to carefully
write down a proof we include a ceiling over this term since bits are
a discrete quantity.

In \secref{weights}, however, we will see that the informal intuition we
use in blackboard proofs is actually valid; we can think of encoding $i$
using $\log(n-t)$ bits even if $\log(n-t)$ is not an integer.  In general
we can imagine encoding a choice from among $r$ options using $\log r$ bits for
any $r\in\N$.  From this point onwards, we omit ceilings this way in all
our theorems and proofs. This makes calculations simpler and provides
tighter results.

\subsection{Stirling Approximations}
\seclabel{stirling}

Before moving on to some more advanced encoding arguments, it will
be helpful to remind the reader of a few inequalities that can be
derived from Stirling's Approximation of $n!$.  Recall that Stirling's
Approximation states that
\begin{equation}
  n! = \left(\frac{n}{e}\right)^n\sqrt{2\pi n}\left(1+O\left(\frac{1}{n}\right)\right) 
   \eqlabel{stirling}
\end{equation}

In many cases, we are interested in representing a set of size $n!$
using a fixed-length code.  The length of the codewords in such a code
is given by the ceiling of
\begin{align}
  \log n!
      & \le n\log n - n\log e + (1/2)\log n + \log(1+O(1/n)) \notag \\
      & \le n\log n - n\log e + (1/2)\log n + O(1/n)  
             & \text{(since $1+x < e^x$)}
               \eqlabel{stirling-tight} \\
      & \le n\log n - n\log e + O(\log n)  
             \eqlabel{stirling-loose} \enspace .
\end{align}

Similarly, for any $k\in\{1,\ldots,n-1\}$, there is a code for sets of 
size $\binom{n}{k}$ in which every codeword has length given by the ceiling of
\begin{align*}
  \log \binom{n}{k}
     & = \log n! - \log k! - \log (n-k)! \\
     & \le n\log n - k\log k - (n-k)\log(n-k) + (1/2)\log\left(\frac{n}{k(n-k)}\right) + O(1/n) \\
     & \le n\log n - k\log k - (n-k)\log(n-k) + O(1/n) \\
     & = n\log n - k\log k - (n-k)\log n + (n-k)(\log n-\log(n-k)) + O(1/n) \\
     & = k\log n - k\log k + \underbrace{(n-k)(\log n-\log(n-k))}_\text{awkward}  + O(1/n) 
         \numberthis \eqlabel{hassle} \\ 
\end{align*}

The subexpression $(n-k)(\log n-\log(n-k))$ is awkward here. We can
simplify it as follows:
\begin{align*}
   (n-k)(\log n-\log(n-k))
      & = (n-k)\log \left(\frac{n}{n-k}\right) \\
      & = (n-k)\log \left(1+\frac{k}{n-k}\right) \\
      & \le (n-k)(k/(n-k))\log e & \text{(since $1+x \le e^x$)} \\
      & = k\log e 
\end{align*}
Using this simplification, \eqref{hassle} becomes
\begin{equation}
  \left\lceil\log \binom{n}{k}\right\rceil 
    \le k\log n - k\log k + k\log e + O(1/n) \eqlabel{log-n-choose-k}
     \enspace .
\end{equation} 


\section{Applications of the Uniform Encoding Lemma}
\seclabel{applications-i}

We now start with some applications of the Uniform Encoding Lemma.

\subsection{Permutation Statistics}

We consider the use of encoding arguments to study statistics of
equiprobable permutations of $\{1, 2, \ldots, n\}$.

\subsubsection{Ascending Runs}

Given a permutation $x = \langle x_1, x_2, \ldots, x_n \rangle$, we
say a subsequence $\langle x_{k_1}, x_{k_2}, \ldots, x_{k_t} \rangle$
is an {\em ascending run} if $x_{k_1} < x_{k_2} < \ldots < x_{k_t}$.

The length of the longest ascending run has been well-studied. Logan
and Shepp \cite{logan.shepp:runs} together with Vershik and Kerov
\cite{vershik.kerov:runs} first established that the expected length
of the longest ascending run in a random permutation is asymptotically
$2 \sqrt{n}$. Several new original proofs of this fact have appeared
more recently; accordingly, we present an encoding argument to study
this problem.

\begin{thm}\thmlabel{runs}
  Let $x = \langle x_1, x_2, \ldots, x_n \rangle$ be a uniformly
  random permutation of the integers from $1$ through $n$, and let $t$
  satisfy the implicit inequality
  $2t\log(t/e\sqrt{n}) - \frac{t^2}{n - t}\log e \geq s + O(1/n)$.
  Then, the probability that $x$ has an ascending run of length $t$ is
  $2^{-s}$.
\end{thm}

\begin{proof}
  We encode the string
  \[x = x_1, x_2, \ldots, x_n\]

  which is drawn uniformly at random from a set of size $n!$. In our
  partial prefix-free code, we provide first the positions of the $t$
  values in the ascending run; then the values in the run; and finally
  the rest of the permutation. In total, this costs:
  \begin{align*}
    b & = 2\log \binom{n}{t} + \log(n - t)! \\
      & = \log n! + 2 \log \binom{n}{t} - \sum_{0 \leq i < t} \log (n - i) \\
      & \le \log n! + 2t \log n - 2t \log t + 2t \log e - \sum_{0 \leq i < t} \log(n - i) + O(1/n)  & \text{(using \eqref{log-n-choose-k})}\\
      & \le \log n! + t \log n - 2t \log t + 2t \log e + t \log \left(1 + \frac{t}{n - t}\right) +  O(1/n) \\
      & \le \log n! - 2t \log(t/e\sqrt{n}) + \frac{t^2}{n - t} \log e + O(1/n) & \text{(since $1+x \le e^x$)} \\
      & \le \log n! - s
  \end{align*}

  bits, as long as $t$ is such that:
  \begin{align*}
    2t\log(t/e\sqrt{n}) - \frac{t^2}{n - t}\log e \ge s + O(1/n)
  \end{align*}

  We obtain the result by applying \lemref{streamlined}.
\end{proof}

\begin{rem}
  A valid solution to the implicit inequality of \thmref{runs} is
  $t = c \sqrt{n}$, for any $c > e$ and for sufficiently large $n$. We
  know that the expectation of random ascending run length is instead
  concentrated around $2 \sqrt{n}$ (CITATION NEEDED). Indeed, the
  proof of \thmref{runs} exhibits a recurring artifact in the
  extraneous `$e$' factor.
\end{rem}

One might na\"{i}evly attempt to use the above encoding to obtain a
result about the expected number of records in a uniformly chosen
permutation -- clearly, such an attempt would be fruitless, since this
statistic is known to be concentrated around $\ln n$ (CITATION
NEEDED). Instead, a more careful encoding is surely required. We leave
this as an open problem. Indeed, it is interesting to acknowledge the
relative difficulty of producing an encoding argument to study this
statistic; the ordinary argument without the burden of encoding is
comparatively simple.

\subsubsection{Records}

Given a permutation $x = \langle x_1, x_2, \ldots, x_n \rangle$, a
(max-)record of $x$ is a value $x_i$ such that $x_i > x_j$ for all
$j < i$. The subsequence of
$\langle x_{k_1}, x_{k_2}, \ldots, x_{k_t} \rangle$ forms an ascending
run of $x$. INCLUDE SHIT ABOUT HOW EASY IT IS TO CALCULATE THE
EXPECTED NUMBER OF RECORDS IN A RANDOM PERMUTATION.

Let $\langle x_1, x_2, \ldots, x_n \rangle$ have records
$\langle x_{n_1}, x_{n_2}, \ldots, x_{n_t} \rangle$. Define
$f : [n]^n \to [n]^n$ to be
$f(x_1, x_2, \ldots, x_n) = (y_1, y_2, \ldots, y_n)$ where:
\[
y_i = \left\{ \begin{array}{ll}
                x_i & \mbox{if $x_i$ is not a record of $x$};\\
                x_{n_j} - x_{n_{j - 1}} & \mbox{if $x_i = x_{n_j}$ for some $j$}.\end{array} \right.
\]

Let $f^{(n)}(x)$ denote the $n$\textsuperscript{th} iterated
application of $f$ to $x$, {\em i.e.} $f^{(1)}(x) = f(x)$, and
$f^{(n)}(x) = f( f^{(n - 1)}(x))$.

\subsection{Graphs with no Large Clique or Independent Set}


A \emph{Erd\H{o}s-R\'enyi random graph}, $G_{n,p}$ is a graph with vertex
set $V=\{1,\ldots,n\}$ and in which each edge $uw\in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.  Erd\H{o}s \cite{erdos:some} used the random
graph $G_{n,\frac{1}{2}}$ to prove the existence of graphs having no
large clique and no large independent set. Here we show how this can be
done using an encoding argument.

\begin{thm}\thmlabel{erdos-renyi-i}
  For all $n\ge 4$ and $s\ge 1$, the probability that the random graph,
  $G_{n,\frac{1}{2}}$ contains a clique or an independent set of size $t =
  \lceil 3\log n + \sqrt{2s}\rceil$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that compresses the $r=\binom{n}{2}$
  bits of $G$'s adjacency matrix, as they appear in row-major order.
  
  $G$ contains a clique or independent set, $S$, of size $t$. Hence,
  the code begins with a bit indicating whether $S$ is a clique or
  independent set; the vertices of $S$; then the adjacency matrix of
  $G$ in row major-order but leaving out all of the $\binom{t}{2}$
  bits implied by the edges or non-edges in $S$.
  
  This encoding requires
  \begin{align*}
     b & = 1 + t\log n + \binom{n}{2}-\binom{t}{2} \\
       & = \binom{n}{2} + 1 + t\log n - (1/2)(t^2 - t) \\
       & = \binom{n}{2} + 1 + t\log n 
            - (1/2)\left(9\log^2 n +3\sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 1 
            - (1/2)\left(3\log^2 n + \sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 1 
            - (1/2)\left(t\log n + 2s - t\right) \\
       & = \binom{n}{2} + 1 
            - (1/2)\left(t(\log n-1)\right) - s \\
       & \le \binom{n}{2} - s 
  \end{align*}
  bits. Applying the Uniform Encoding Lemma completes the proof.
\end{proof}

\begin{rem}
  The bound in \thmref{erdos-renyi-i} can be strengthened a little,
  since the elements of $S$ can be encoded with fewer than $t\log n$
  bits; in particular $\lceil\log\binom{n}{t}\rceil$ bits are sufficient.
  With a more careful calculation, using \eqref{log-n-choose-k}, the
  proof then works with $t=2\log n +O(\log\log n) + \sqrt{s}$. This
  comes closer to Erdős' original result, which was at the threshold
  $2\log n - 2\log\log n + O(1)$.
\end{rem}



\subsection{Balls in Urns}
\seclabel{urns}

The question of what happens when one throws $n$ balls, randomly and
indepently into $n$ urns is a useful abstraction of many algorithmic,
data structures and load-balancing problem.  Here we show how an encoding
argument can be used to prove the classic result that, when we do this,
no urn contains more than $O(\log n/\log\log n)$ balls.

\begin{thm}\thmlabel{urns}
  Let $t$ be such that $t\log(t/e) \ge 1+s+\log n$ and suppose we throw
  $n$ balls independently and uniformly at random into $n$ urns. Then,
  for sufficiently large $n$, the probability that any urn contains more 
  than $t$ balls is at most $2^{-s}$.
\end{thm}

Before proving \thmref{urns}, we note that, for any constant $\epsilon
>0$ and all sufficiently large $n$, taking
\[
   t = \frac{(1+\epsilon)\log n}{\log\log n}
\] 
satisfies the requirements of \thmref{urns}.

\begin{proof}
  For each $i\in\{1,\ldots,n\}$, let $b_i$ denote the index of the urn in
  which the $i$th ball lands. Notice that the sequence $b_1,\ldots,b_n$
  is chosen uniformly at random from a set $X$ of size $n^n$, and it is this
  choice that will be used in our encoding argument.

  If the urn labelled $i$ contains $t$ or more balls, then our code
  begins with the value $i$, followed by a code that describes $t$ of
  the balls in urn $i$, followed by the remaining $n-t$ values of
  $b_1,\ldots,b_n$ that cannot be deduced from the preceding
  information.  In total, this is
  \begin{align*}
    b &\le \log n + \log\binom{n}{t} 
           + (n-t)\log n \\
     & = \log n + t\log n - t\log t + t\log e  
           + (n-t)\log n + O(1/n)
             & \text{(using \eqref{log-n-choose-k})} \\
     & = \log n + t\log n - t\log t + t\log e  
           + (n-t)\log n + 1
             & \text{(for large enough $n$)} \\
     & = (n+1)\log n - t\log t + t\log e + 1 \\
     & \le  n\log n - s &\text{(by the choice of $t$)} \\
     & =  \log n^n - s \enspace . & \qedhere
  \end{align*}
\end{proof}

\subsection{Linear Probing}

Studying balls in urns is a useful technique to analyze hashing with
chaining (see, e.g., \cite[Section~5.1]{morin:open}), a more practically
efficient form of a hashing is \emph{linear probing}.  In a linear-probing
hash table, we hash $n$ items $x_1,\ldots,x_n$ into a linear probing hash
table of size $m=cn$, $c\ge 1$. When we insert $x_i$ we try and place
it at table position $j=h(x_i)$ unless it is already occupied by one of
$x_1,\ldots,x_{i-1}$ in which case we try table location $(j+1)\bmod m$,
$(j+2)\bmod m$, and so on until we find an empty spot for $x_i$.  We want
to study the expected search time for some item $x_i$ in this hash table.

We call a maximal consecutive (the table locations $m-1$ and
$0$ are considered consecutive) sequence of occupied table locations
a \emph{block}.

\begin{thm}\thmlabel{linear-probing}
  Fix some $x\in\{x_1,\ldots,x_n\}$. Then the probability that the block
  containing $x$ has size $t\ge 1+(s+\log t + O(1))/\log(c/e)$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that encodes the vector 
  \[
     R = h(x_1),h(x_2),\ldots,h(x_n)
  \]
  which is drawn uniformly at random from a set $X$ of size $n^{m}=n^{cn}$.
  
  We encode $R$ by the first index $b$ of the block containing $x$;
  followed by the $t-1$ elements, $y_1,\ldots,y_{t-1}$, of this block
  (excluding $x$); followed by enough information to decode the hash
  values of $h(x)$ and $h(y_1),\ldots,h(y_{t-1})$; followed by
  $h(x_1),\ldots,h(x_n)$ excluding the $t$ hash values we can compute
  from the preceding information.

  Notice that $h(x),h(y_1),h(y_2),\ldots,h(y_{t-1})$ are in the range
  $b,\ldots,b+t-1$ (modulo $m$) so these can be encoded using $\lceil
  t\log t\rceil$ bits.  Therefore, if the block containing $x$ has size $t$,
  then we obtain a codeword whose length is
  \begin{align*}
    b & = \log m + \log\binom{n}{t-1} + t\log t + (n-t)\log m + O(1) \\
       & \le \log m + (t-1)\log n - 
           {\color{red}(t-1)\log(t-1)} + (t-1)\log e + {\color{red}t\log t} + (n-t)\log m + {\color{red}O(1)}\\
       & \le {\color{blue}\log m} + {\color{blue}(t-1)\log n} + (t-1)\log e + {\color{blue}(n-t)\log m} + {\color{red}\log t}+ {\color{red} O(1)}\\
       & = {\color{blue}n\log m} - {\color{blue}(t-1)\log c} + (t-1)\log e + \log t + O(1) \\
       & = n\log m - (t-1)\log(c/e) + \log t + O(1) \\
       & \le n\log m - s \enspace ,
  \end{align*}
  for $t\ge (s+\log t+O(1))/\log (c/e) + 1$.
\end{proof}

\begin{rem}
  The proof of \thmref{linear-probing} requires that the linear
  probing hash table has size $cn$ for some $c>e$.  We know from
  previous analysis that this is not necessary, any $c>1$ is
  sufficient. We leave it as an open problem to find an encoding proof
  of \thmref{linear-probing} that works for any $c>1$.
\end{rem}

\subsection{2-Choice Hashing}

We showed earlier how, when throwing $n$ balls uniformly at random and
independently into $n$ urns, no urn contains more than
$O(\log n/\log \log n)$ balls. In 2-choice hashing, each ball is
instead given a choice of two of these $n$ urns, and the urn
containing fewer balls is preferred.

\begin{thm}\thmlabel{2-choice-hashing}
  Fix some $x \in \{x_1, \ldots, x_n\}$. Then, in the 2-choice hashing
  scheme, the probability that the urn containing $x$ has size $t$ is
  $2^{-s}$, where:
  \[t 2^t - 2^t(\log e + 2) \geq s + O(1/n)\]
\end{thm}

Perhaps surprisingly, we note that $t = O(\log \log n)$ satisfies the
conditions of \thmref{2-choice-hashing} -- an exponential improvement
over the single choice scheme considered in \secref{urns}. In fact,
Azar {\em et al.} \cite{azar:multiplechoice} showed that the expected
maximum size of an urn is $\log_2 \ln n + O(1)$. More general and more
precise results have since appeared.

\begin{proof}
  We encode the vector:
  \[
  R = h_1(x_1), h_2(x_1), h_1(x_2), h_2(x_2) \ldots, h_1(x_n), h_2(x_n)
  \]

  which is drawn uniformly at random from a set of size $n^{2n}$.

  If an element $x$ has search time $t$, then it corresponds to a
  witness tree $T$ with root $x$ and height $t$. If $y$ is $x$'s
  $i$\textsuperscript{th} child, then we know that $h_j(y) = h_i(x)$
  for either $j = 1$ or $2$.

  We encode $R$ as follows: first we give $y_1, y_2, \ldots, y_{2^t}$,
  the sequence of leaves in the witness tree given in increasing
  order; then we include each pair $(i, j)$ for each edge $(x, y)$ in
  the tree, where $y$ is $x$'s $i$\textsuperscript{th} child and
  $h_j(y) = h_i(x)$; then we include the values $h_1(x)$ and
  $h_2(x)$. From this, we can deduce $2^t$ values of the hash
  function. Finally, we include the remaining $2n - 2 - 2^t$ hash
  values. In total, this requires:

  \begin{align*}
    b & = \log \binom{n}{2^t} + 2^{t + 1} - 2 + 2\log n + (2n - 2 - 2^t)\log n \\
      & \le \log n^{2n} - 2^t \log 2^t + 2^t \log e + 2^{t + 1} + O(1/n) \\
      & = \log n^{2n} - t 2^t + 2^t (\log e + 2) + O(1/n) \\
      & \le \log n^{2n} - s
  \end{align*}

  bits, as long as:
  \[t 2^t - 2^t(\log e + 2) \geq s + O(1/n)\]
\end{proof}

\subsection{Robin-Hood Hashing}

Soon.


\subsection{Bipartite Expanders}

Consider a random bipartite multigraph $G=(A,B,E)$ where $|A|=|B|=n$
and the $3n$ edges in $E$ are obtained as follows:  Each vertex $u\in A$
chooses 3 random neighbours, $x(u)$, $y(u)$, and $z(u)$, in $B$, with
replacement.  All choices of edges are independent. For a set $A'\subseteq
A$, the \emph{neighbourhood} of $A'$, denoted $N(A')$, is the subset of
vertices in $B$ that are adjacent to at least one element of $A'$.

The following theorem shows that $G$ is an expander.  The proof of
this theorem usually involves a messy sum that contains binomial
coefficients and probabilities.  See, for example, Motwani and
Raghavan \cite[Theorem~5.3]{motwani.raghavan:randomized}, Pinsker
\cite[Lemma 1]{pinsker:on}, or Hoory, Linial, and Wigderson
\cite[Lemma~1.9]{hoory.linial.ea:expander}.

\begin{thm}
  There exists a constant $\alpha >0$ such that, with probability at
  least $O(n^{-1/2})$, $|N(A')| \ge 3|A'|/2$ for all $A'\subset A$
  with $|A'|\le \alpha n$.
\end{thm}

\begin{proof}
Without loss of generality, let $A=\{1,\ldots,n\}$.  We use an encoding argument on the bit string
\[
   R = x(1), y(1), z(1), x(2), y(2), z(2), \ldots, x(n), y(n), z(n) \enspace ,
\]
which is chosen uniformly from a set $X$ of size $n^{3n}$.

If some set $A'$, $|A'|=k\le \alpha n$, violates the conditions of the
lemma, then we encode $k$, $A'$, $N(A')$ and then the edges between
$A'$ and $N(A')$. Then we encode the rest of $R$, skipping the
$3k\log n$ bits devoted to elements in $A'$.  The key savings here
comes because $N(A')$ should take $3k\log n$ bits to encode, but can
actually be encoded in roughly $3k\log(3k/2)$ bits.

The total size of the first part of our encoding is, which includes $k$,
$A'$ and $N(A')$ is
\begin{align*}
    h & = 2\log k + \log\binom{n}{k} + \log\binom{n}{3k/2} + 3k\log (3k/2) \\
       & \le k\log n - k\log k + (3/2)k\log n - (3/2)k\log k + 3k\log k + k\log(3/2) \\
      & = (5/2)k\log n + (1/2)k\log k + k\log(3/2)  \enspace .
\end{align*}
The size of the second part of our encoding is $3n\log n - 3k\log n$.  The savings, $s=s(k)$, we obtain is therefore
\begin{align*}
     s(k) & = 3k\log n - (5/2)k\log n - (1/2)k\log k - k\log(3/2) \\
       & = (1/2)k(\log n - \log(2k/3)) \\
       & = (1/2)k(\log n - \log k - \log(3/2)) \\
       & = (1/2)k\log(2n/(3k)) \enspace .
\end{align*}
The function $s(k)$ is concave and is therefore minimized when $k$
is extremal, with the extremes being $k=1$ and $k=\alpha
n$. For $k=1$, we have
\[
    s(1)=(1/2)\log n - \log(2/3)
\]
and $2^{-s(1)} = O(n^{-1/2})$.  For $k=\alpha n$ we have
\[
   s(\alpha n) = (\alpha n/2)\log(2/(3\alpha))
\]
and $2^{-s(\alpha n)} = 2^{-\Omega(n)}$ for $\alpha < 2/3$.
\end{proof}

\subsection{Analysis of Cuckoo Hashing}

Cuckoo hashing is relatively new kind of hash table that offers an
alternative to classic perfect hashing \cite{pagh.rodler:cuckoo}. Here we present a clever proof,
due to Mihai Pătraşcu, that cuckoo hashing succeeds with probability
$1-O(1/n)$ \cite{patrascu:cuckoo}.

The main object of study in cuckoo hashing is a random bipartite
multigraph $G=(A,B,E)$ where $|A|=|B|=2n$ and $|E|=n$.  For each
$i\in\{1,\ldots,n\}$, each edge $e_i$ of $E$ is obtained by randomly
choosing (with replacement) an endpoint in $a_i\in A$ and an endpoint
in $b_i\in B$.  The cuckoo hashing algorithm may fail if this graph
contains a long edge-simple path or if it contains a component with more
edges than vertices.  (An edge-simple path is a path that uses each edge
of the multiset $E$ at most once.)

\begin{thm}\thmlabel{edge-simple}
  Fix some vertex $x\in A$, then the probability that $x$ is the
  endpoint of an edge-simple path of length $t=s$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  We use an encoding argument on the sequence
  \[
     R=a_1,b_1,a_2,b_2,\ldots,a_n,b_n
  \]
  which is drawn uniformly from a set $X$ of size $m^{2n}$.

  The code begins with the $t$ edges and vertices (excluding $x$) on a
  path, $P$, of length $t$ that starts at $x$, followed by the $2n-2t$
  entries in $R$ that are not edges of this path.

  The path $P$ consists of $t+1$ vertices (the first of which is $x$) and
  $t$ edges. The encoding of $P$ specifies the vertices (excluding $x$)
  as well as the indices of the edges. The former requires $t\log m$ bits
  and the latter requires $t\log n$ bits, for a total of $2t\log n-t$ bits.

  This encoding requires
  \[
    b = t\log m + t\log n + 2(n-t)\log m < 2n\log m - s
  \]
  bits, for $t=s$. The proof finishes, as usual, by applying the
  Uniform Encoding Lemma.
\end{proof}

\thmref{edge-simple} and the union bound establish that, with probability
at least $1-1/n$, the cuckoo graph has no edge-simple path of length
greater than $1+2\log n$. The following theorem shows that the cuckoo
graph has no subgraph with more edges than vertices.

\begin{thm}
  Fix a vertex $x\in A$.  Then the probability that $x$ is part of a
  minimal subgraph that has more edges than vertices is $O(1/n^2)$.
\end{thm}

\begin{proof}
  We encode the same sequence $R$ as in the previous proof. Now, $x$
  is a part of a minimal subgraph having $t$ vertices and $t+1$
  edges. Note that, after removing two edges, this subgraph consists
  of two paths, of length $t_1$ and $t_2$, (with $t_1+t_2=t-1$) that
  have $x$ as an endpoint.  In this case, we encode $R$ as $t_1$ and
  $t_2$; the vertices and edges of each of these two paths, excluding
  $x$; and enough information to determine the two edges we removed
  (this takes $2\log n + O(\log t)$ bits); finally, the code includes
  all the $2n-2(t+1)$ entries in $R$ that cannot be derived from the
  previous information. This gives a total of
  \begin{align*}
     b &= (t-1)(\log n+\log m) + (2n-2(t+1))\log m + 2\log n + O(\log t) \\
        &= 2n\log m + O(\log t) - t - 2\log m \\
       & \le 2n\log m - 2\log m + O(1) \enspace .
  \end{align*}
  Applying the Uniform Encoding Lemma with $s=2\log m+O(1)$ completes
  the proof.
\end{proof}

\subsection{Analysis of Insertion Sort}

Recall the insertion-sort algorithm for sorting a list, $A_1,\ldots,A_n$
of $n$ elements:

\noindent{$\textsc{InsertionSort}(A_1,\ldots,A_n)$}
\begin{algorithmic}[1]
  \FOR{$i\gets 2$ \TO $n$}
     \STATE{$j \gets i$}
     \WHILE{$j>1$ \AND $A_{j-1} > A_j$}
         \STATE{$A_j \leftrightarrow A_{j-1}$
            \COMMENT{ swap }}
         \STATE{$j\gets j-1$}
     \ENDWHILE
  \ENDFOR
\end{algorithmic}

A typical question asks the expected number of times Line~4 executes
if $A_1,\ldots,A_n$ is a uniformly random permutation of $n$ distinct
elements.  The answer $\binom{n}{2}/2$ is an easy application of
linearity of expectation: For every one of the $\binom{n}{2}$ pairs
$p,q\in\{1,\ldots,n\}$ with $p<q$, the values initially stored at
positions $A_p$ and $A_q$ will eventually be swapped if and only if $A_p >
A_q$, which happens with probability $1/2$ in a random permutation.

A more advanced question is to ask for a concentration result on the
number of executions of Line~4. This is a harder question to tackle;
because $>$ is transitive, the $\binom{n}{2}$ events being studied have
a lot of interdependence. In the following, we show how to obtain a
concentration result with an encoding argument.  The argument presented
here follows the same outline as Vitanyi's analysis of bubble sort
\cite{vitanyi:analysis}, though without all the trappings of Kolmogorov complexity.

\begin{thm}\thmlabel{insertion-sort}
  For a random permutation, $A_1,\ldots,A_n$ of $n$ distinct elements,
  the probability that Line~4 of \textsc{InsertionSort} executes fewer
  than $\alpha n^2 - n + 2$ times is at most $2^{n\log(\alpha e^2)+O(\log
  n)}$.  In particular, for a fixed $\alpha < 1/e^2$, this probability
  is $2^{-\Omega(n)}$.
\end{thm}

\begin{proof}
  Let $\pi$ be the permutation of $\{1,\ldots,n\}$ that defines the sorted
  order of $A_1,\ldots,A_n$, so that $A_{\pi_j}$ is the element that appears
  at position $j$ after sorting. Notice that $\pi$ is chosen uniformly at
  random among all $n!$ permutations of of $\{1,\ldots,n\}$.
  
  We encode the permutation $\pi$ by recording the execution of
  InsertionSort on this permutation. In particular, we record, for each
  $i\in\{2,\ldots,n\}$, the number of times, $m_i$, that Line~4 executes
  during the $i$th iteration of \textsc{InsertionSort}. With this information,
  one can run the following version of \textsc{InsertionSort} to recover $\pi$:
  
  \noindent{$\textsc{InsertionSortReconstruct}(m_2,\ldots,m_n)$}:
  \begin{algorithmic}[1]
    \STATE{$\pi \gets \langle 1,\ldots,n\rangle$}
    \FOR{$i\gets 2$ \TO $n$}
       \FOR{$j\gets i \textbf{ down to } i-m_i+1$}
           \STATE{$\pi_j \leftrightarrow \pi_{j-1}$
              \COMMENT{ swap }}
       \ENDFOR
    \ENDFOR
  \end{algorithmic}
   
  To make this work, we have to be slightly clever with this
  encoding. Rather than encode $m_2,m_3,\ldots,m_n$ directly, we first
  encode $m=\sum_{i=2}^{n} m_i$ using $\lceil 2\log n\rceil$ bits (since $m < n^2$). Given $m$, what
  remains is to describe the partition of $m$ into $n-1$ non-negative
  integers $m_2,\ldots,m_n$; there are $\binom{m+n-2}{n-2}$ such
  partitions.\footnote{To see this, draw $m+n-2$ white dots on a line,
  then choose $n-2$ dots to colour black. This splits the remaining $m$ white dots
  up into $n-1$ groups, which determine the values of $m_2,\ldots,m_n$.}
  
  Therefore, the values of $m_2,\ldots,m_n$ can be encoded using
  \[
      b = \lceil 2\log n\rceil + \log\binom{m+n-2}{n-2}
  \]
  bits and this is sufficient to recover the permutation that defines
  $A_1,\ldots,A_n$.  By applying \eqref{log-n-choose-k} to $b$, we obtain
  \begin{align*}
    b & \le (n-2)\log(m+n-2) - (n-2)\log(n-2)  + (n-2)\log e + O(\log n) \\
      & \le n\log(m+n-2) - n\log n   + n\log e + O(\log n) \\
      & \le n\log(\alpha n^2) - n\log n  + n\log e + O(\log n) \\
      & = 2n\log n + n\log\alpha - n\log n  + n\log e + O(\log n) \\
      & = n\log n + n\log\alpha + n\log e + O(\log n) \\
      & = \log n! + n\log\alpha + 2n\log e + O(\log n) \\
      & = \log n! + n(\log \alpha e^2) + O(\log n)  \qedhere
  \end{align*}
\end{proof}

\begin{rem}
  Although it doesn't require any advanced probability,
  \thmref{insertion-sort} is not sharp; it only gives a non-trivial
  probability when $\alpha < 1/e^2$.  To obtain a sharp bound, one can
  use the fact that $m_2,\ldots,m_n$ are independent and $m_i$ is uniform
  over $\{0,\ldots,i-1\}$ and then use the method of bounded differences
  \cite{mcdiarmid:on} to show that $m$ is concentrated in an interval
  of size $O(n^{3/2})$.
\end{rem}




\section{The Non-Uniform Encoding Lemma and Shannon-Fano Codes}
\seclabel{nuel}

Thus far, we have been lucky to study applications that could always
be modelled as choosing some element $x$ uniformly at random from a
set $X$. To encompass even more applications, it is helpful to have
an Encoding Lemma that deals with non-uniform distributions over $X$.
The following generalization of the Uniform Encoding Lemma fills this
need:

\begin{lem}[Non-Uniform Encoding Lemma]\lemlabel{nuel}  
  Let $C\from X\to\{0,1\}^*$ be a prefix code and let $x\in X$ be
  drawn randomly where $p_x$ denotes the probability of drawing $x$.
  Then $\Pr\{ |C(x)| \le \log(1/p_x)-s\} \le 2^{-s}$.
\end{lem}

\begin{proof}
  We use Chernoff's trick, Markov's Inequality, and Kraft's Inequality,
  as follows: 
  \begin{align*}
     \Pr\{ |C(x)| \le\log(1/p_x)-s \} 
      & = \Pr\{|C(x)| -\log(1/p_x) \le -s \} \\
      & = \Pr\{\log(1/p_x)-|C(x)| \ge s \} \\
      & = \Pr\left\{2^{\log(1/p_x)-|C(x)|} \ge 2^s \right\} & \text{(Chernoff's trick)} \\
      & \le \frac{\E\left[2^{\log(1/p_x)-|C(x)|}\right]}{2^s} & \text{(Markov's Inequality)} \\
      & = \frac{\sum_{x\in X}p_x\cdot 2^{\log(1/p_x)-|C(x)|}}{2^s} \\
      & = \frac{\sum_{x\in X}2^{-|C(x)|}}{2^s} \\
      & \le \frac{1}{2^s} & \text{(Kraft's Inequality).}  & \qedhere
  \end{align*}
\end{proof}

Notice that the Non-Uniform Encoding Lemma is a strict generalization
of the Uniform Encoding Lemma; take $p_x=1/|X|$ for all $x\in X$
and we obtain the Uniform Encoding Lemma.

The \emph{Shannon-Fano code} \cite{fano:transmission,shannon:mathematical}
for a finite set $X$ is prefix-free code that is constructed by a top-down
algorithm from a probability distribution over the elements of $X$. An
important property of the Shannon-Fano code is that it is \emph{locally
optimal}: If $C\from X\to\{0,1\}^*$ is a Shannon-Fano code, then for every $x\in X$,
\[
    |C(x)| \le \left\lceil\log(1/p_x)\right\rceil \enspace 
\]
where, as before, $p_x$ denotes the probability of $x$.

When designing efficient encoding schemes it helps, at least
informally, to think of $\log(1/p)$ as the cost of encoding a 1 bit and
$\log(1/(1-p))$ as the cost of encoding a 0 bit.  For example, $p=1/n$

We will find it particularly useful to apply Shannon-Fano codes to
$\mathrm{Bernoulli}(p)$ strings consisting of indepenent random bits that
are each set to 1 with probability $p$ and 0 with probability $1-p$.
A particular binary string $x=x_1,\ldots,x_n$, occurs as
a $\mathrm{Bernoulli}(p)$ string with
probability exactly
\[
   p_x= p^{r_1}(1-p)^{r_0} \enspace ,
\]
where $r_1$ and $r_0$ are the numbers of 1 and 0 bits in $x$, respectively. In this case, the Shannon-Fano code for $x$ has length at most
\[
    |C(x)|\le \left\lceil r_1\log (1/p) + r_0\log(1/(1-p)) \right\rceil \enspace .
\]


\section{Applications of the Non-Uniform Encoding Lemma}
\seclabel{applications-ii}

\subsection{Chernoff's Bounds}

The following theorem is the so-called \emph{additive version} of the
Chernoff bound on the tail of a binomial random variable:

\begin{thm}\thmlabel{chernoff}
  If $B$ is a $\mathrm{binomial}(n,p)$ random variable, then $\Pr\{B\ge
  (p+\epsilon)n\} \le 2^{-nD(p+\epsilon||p)}$, where 
  \[ 
    D(\hat p||p)=\hat p\log(\hat p/p) + (1-\hat p)\log((1-\hat p)/(1-p)) 
  \] is
  the Kullbach-Liebler divergence between Bernoulli random variables
  with probabilities $\hat p$ and $p$.
\end{thm}

\begin{proof}
By definition, $B=\sum_{i=1}^nx_i$, where $x_1,\ldots,x_n$ are independent $\mathrm{Bernoulli}(p)$ random variables.   We will use an encoding argument on the bitstring $x=x_1,\ldots,x_n$.

We can encode $x$ by using a Shannon-Fano code for the probability
$p+\epsilon$.  If $B=\hat pn$, then this takes
\[
   b = n(\hat p\log(1/(p+\epsilon))+ (1-\hat p)\log(1/(1-p-\epsilon)))
\]
bits.

On the other hand,
\[
    p_x = p^{\hat pn}(1-p)^{(1-\hat p)n} \enspace ,
\]
and
\[
    \log (1/p_x) = n(\hat p\log(1/p)+ (1-\hat p)\log(1/(1-p)) \enspace .
\]
For $\hat p\ge p+\epsilon$, the savings from this code is
\begin{align*}
  \log(1/p_x) - b
    & =  n(\hat p\log((p+\epsilon)/p) + (1-\hat p)\log((1-p-\epsilon)/(1-p))) \\
    & \ge  n((p+\epsilon)\log((p+\epsilon)/p) + (1-p-\epsilon)\log((1-p-\epsilon)/(1-p)))\\
    & =  nD(p+\epsilon||p)
\end{align*}
bits.  Therefore, the probability that $B\ge (p+\epsilon)n$ is at most $2^{-nD(p+\epsilon||p)}$.
\end{proof}

It is also possible to derive the multiplicative form of the Chernoff
bound this way. This is left an exercise to the reader.

\subsection{Percolation on the Torus}

The following is a version of Peierls' argument that proves that
percolation occurs on the torus if the edge survival rate is greater
than $2/3$:

\begin{thm}
  Let $G$ be the $\sqrt n\times\sqrt n$ torus and let $G'$ be the graph
  obtained by sampling each edge of $G$ independently with probability
  $p<1/3$.  Then the probability that $G'$ contains a simple cycle of
  length at least $t=(s+\log n+O(1))/\log(1/(3p))$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  Let $x$ be the bitstring of length $r=2n$ that encodes the existence
  of the edges in $G'$.  Suppose such a cycle, $C$, exists in $G'$
  and its length is $t'$.  Then we can encode $G'$ by specifying a
  single vertex, $u$, on $C$; the sequence of $t'$ steps (directions)
  that the cycle moves along from there; and then the rest of $G$ using
  a variable length code with parameter $p$.

  When specifying the cycle, there are four possibilities for the first
  step, but only three possibilities for each the subsequent step. Therefore,
  all the steps in the cycle can be specified using $\lceil 2+(t'-1)\log
  3\rceil$ bits.

  The total lenght of the codeword is then
  \[
      b = \log n + t'\log 3 + (r_1-t')\log(1/p) + r_0\log(1/(1-p)) + O(1) 
  \]
  bits, where $r_1$ is the number of edges in $G'$ and $r_0=r-r_1$
  is the number of missing edges in $G'$.  But then,
  \[
     p_x = p^{r_1}(1-p)^{r_0}
  \]
  so 
  \[
     \log(1/p_x) = r_1\log(1/p) + r_0\log(1/(1-p)) \enspace .
  \]
  Therefore, $\log(1/p_x)-b = t'(\log(1/p)-\log 3) - \log n - O(1)\ge
  s$, for $t'\ge t$.  Applying the Non-Uniform Encoding Lemma completes
  the proof.
\end{proof}

Since the torus is self-dual, this implies that if we keep each edge of the torus with probability greater than $2/3$ then, with high probability, there is only one component whose size is $\omega(\log^2 n)$.  This is because, having more than one such component would imply the existence of a dual cycle of missing edges whose length is $\omega(\log n)$.  The preceding theorem shows that, with high probability, no such dual cycle exists.


\subsection{Triangles in $G_{n,p}$}

The expected number of triangles (cycles of length 3) in $G_{n,p}$
is easily seen to be $p^3\binom{n}{3}$.  For $p=(6k)^{1/3}/n$, this
expectation is $k-O(1/n)$.  Unfortunately, even when $k$ is a large
constant, it still takes some work to show that there is a constant
probability that $G_{n,p}$ contains at least one triangle. Indeed, this
typically requires the use of the second moment method, which involves
computing the variance of the number of triangles in $G_{n,p}$.

Here we show how this can be accomplished with an encoding argument.

\begin{thm}
  For $p=c/n$, $G_{n,p}$ contains at least one triangle with probability
  at least $1-2^{-\Omega(c^3)}$.
\end{thm}

\begin{proof}
   Without loss of generality assume that $G$'s vertex set is
   $V=\{1,\ldots,n\}$.  In this argument, we will produce an encoding
   of $G$'s adjacency matrix, $x$.

   Refer to \figref{triangles}.  If $G$ contains no triangles, then we
   look at the number of 1's in the $n/2\times n/2$ submatrix
   determined by rows $1,\ldots,n/2$ and columns $n/2+1,\ldots,n$.
   There are two cases to consider (which result in codewords
   beginning with 10 or 11, respectively):

   \begin{figure}
     \centering{\includegraphics{triangles}}
     \caption{The random graph $G_{n,c/n}$ contains triangles when $c$ is
       large enough.  The highlighted 0 bits in the last five rows can
       be deduced from pairs of 1 bits in the first 5 rows.}
     \figlabel{triangles} 
   \end{figure}

   \begin{enumerate}\setcounter{enumi}{-1}
     \item The number of 1's in the submatrix is less than $cn/8$.
      In this case, the number of 1's in this submatrix is much less than
      the expected number, $cn/4$.  Here one can apply the same argument
      used to prove Chernoff's Bounds (\thmref{chernoff}) or simply
      apply Chernoff's bounds. We leave this as an exercise to the reader.

     \item The number of 1's in the submatrix is greater than $cn/8$.
      Notice that, for $i<j<k$ if $M_{i,j}=1$ and $M_{i,k}=1$, then the
      fact that there are no triangles implies tha $M_{j,k}=0$.

      Let $m_i$ be the number of 1's in the $i$th row of the submatrix.
      By specifying rows $1,\ldots,n/2$, we eliminate the need to specify
      \[
          m = \sum_{i=1}^{n/2}\binom{m_i}{2} \ge (n/2)\binom{c/4}{2} = \Omega(c^2n) \enspace ,
      \]
      zeros in rows $n/2+1,\ldots,n$.
      Using a Shannon-Fano code to specify the first $n/2$ rows and the bits of the remaining rows that can not be deduced from the first $n/2$ rows, we obtain a codeword of length 
      \[
          b = r_1\log(1/p) + (r_0-m)\log(1/(1-p))  + O(1)
      \]
      which results in a savings of
      \[
          s = \log(1/p_x) - b = m\log(1/(1-p)) -O(1) \ge \Omega(c^2n)\log(1/(1-p)) - O(1) = \Omega(c^3) \enspace .
      \]
   \end{enumerate}
\end{proof}


\subsection{The Components of $G_{n,p}$}

[The following result is really boring. It's easier to prove just using linearity of expectation and Markov's Inequality.]

\begin{thm}
  For $p=\alpha/n$, the probability that $G_{n,p}$ contains a component of size at least $t$ is at most $blah$.
\end{thm}

\begin{proof}
  We use an encoding argument on the bits of the adjacency matrix, $x$,
  of $G$.

  If $G$ does contain a component of size $t$ or greater, then we
  find, within this component, a tree, $T$, of size exactly $t$.  The
  codeword then consists of a list of the vertices in $T$ as they are
  encountered during an preorder traversal, a description of the shape
  of $T$, and then finally, a Shannon-Fano code that gives the
  $\binom{n}{2}-t+1$ bits of the adjacency matrix corresponding to the
  edges not in $T$.

  The shape of $T$ can be described using $2n$ bits by tracing the
  execution of the traversal of $T$, using a 1 to indicate that the
  traversal takes a step deeper into the tree and a 0 to indicate that
  the traversal moves up in the tree.

  In total, this results in a codeword of length
  \[   
    b \le 2 + t\log n + 2t + (r_1-t+1)\log(1/p) + r_0\log(1/(1-p))  \enspace . 
  \]
  On the other hand
  \[   
     \log (1/p_x) = r_1\log(1/p) + r_0\log(1/(1-p)) \enspace ,
  \]
  so
  \begin{align*}
     s & = \log(1/p_x) - b \\
       & =  (t-1)\log(1/p) - t\log n - 2t - 2 \\
       & = t(\log(1/p)-\log n - 2) - \log n - 2 \\
       & = t\log(1/(4\alpha)) - \log n - 2 \\
  \end{align*}
\end{proof}

[This is also really boring\ldots]

\begin{lem}
  The probability that $G_{n,p}$ has a partition of $(U,W)$ of its vertex
  set such that $|U|=k$ and there is no edge with one endpoint in $U$
  and one endpoint in $W$ is at most $2n^k\cdot (1-p)^{k(n-k)}$.
\end{lem}

\begin{proof}
  We encode the adjacency matrix, $x$, of $G_{n,p}$ as follows.
  If $G_{n,p}$ does not contain such a partition, then the encoding is
  zero bit followed by the obvious encoding.

  Otherwise, the encoding is a one bit, followed by the list of $k$
  vertices, followed by the remaining $\binom{n}{2}-k(n-k)$ bits of the
  adjacency matrix that are not implied by the information already given.
  These latter bits are encoded using a Shannon--Fano code that is
  optimized for $p$.

  If $G$ has $r_1$ edges and $r_0=\binom{n}{2}-r_1$ non-edges, then the codeword has length
  \begin{align*}
      b & = 1 + k\log n + r_1\log(1/p) + (r_0-k(n-k))\log(1/(1-p)) \enspace .
  \end{align*}
  On the other hand,
  \[
      \log p_x = r_1\log(1/p) + r_0\log(1/(1-p)) \enspace ,
  \]
  so the savings obtained by this encoding is 
  \[
      s = \log p_x - b = k(n-k)\log(1/(1-p)) - k\log n - 1 \enspace ,
  \]
  and the result follows from the Non-Uniform Encoding Lemma.
\end{proof}




\subsection{}


\section{Weight Functions}
\seclabel{weights}

\section{Summary and Conclusions}
\seclabel{summary}


\section*{Acknowledgement}

This research was initiated in response to an invitation to give
a talk at the Ninth Annual Probability, Combinatorics and Geometry
Workshop, held April 4--11 at McGill University's Bellairs Institute.
Many ideas that appear in the current paper were developed during the
workshop. The author is grateful to the other workshop participants for
providing a stimulating working environment.  In particular, Xing~Shi~Cai
pointed out the application to runs in binary strings (\thmref{runs-i})
and G\'abor~Lugosi stated and proved the Non-Uniform Encoding Lemma
(\lemref{nuel}) in response to the author's half-formed ideas about a
non-uniform version of \lemref{uel}.

\bibliographystyle{plain}
\bibliography{encoding}

\end{document}
