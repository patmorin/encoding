\documentclass[lotsofwhite]{patmorin}
\usepackage{pat}
\usepackage[utf8]{inputenc}
\usepackage[noend]{algorithmic}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\keywords}[1]{\vspace{2em}\noindent\textbf{Keywords:} #1}
%\newcommand{\from}{\colon}

\title{\MakeUppercase{Encoding Arguments}}
\author{Pat Morin}
\date{}

\begin{document}
\begin{titlepage}
\maketitle

\begin{abstract}
\setlength{\baselineskip}{15.84pt}
This expository article surveys a number of applications of ``encoding
arguments,'' in which probabilistic statements are proven using the fact
a uniformly random choice from a set of size $N$ can not be encoded with
fewer than $\log N$ bits on average.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}

There is no doubt that probability theory plays a fundamental role
in computer science: Some of the fastest and simplest fundamental
algorithms and data structures are randomized; average-case analysis of
algorithms relies entirely on tools from probability theory; and many
difficult combinatorial questions have strikingly simple solutions using
probabilistic arguments.

Unfortunately, many of these beautiful results are inaccessible to most
computer scientists because of a view that ``the math is too hard.''
For instance, ACM's CS2013 Final Report does not require a full course
in probability theory \cite[Page~50]{acm2013}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on discrete
probability, as part of the discrete structures curriculum.

In this expository paper, we survey applications of ``encoding arguments''
that tranforms the problem of upper-bounding the probability of a
specific event, $\mathcal{E}$, into the problem of devising a code that
is particularly short whenever $\mathcal{E}$ occurs.  Encoding arguments
have several advantages over traditional probabilistic analysis:

\begin{enumerate}
  \item Encoding arguments are almost ``probability-free.''  Except for
  applying a simple Encoding Lemma, there is no probability involved.
  In particular, there is no chance to make common mistakes such as
  multiplying probabilities of non-independent events or (equivalently)
  multiplying expectations.

  \item Encoding arguments usually yield strong results;
  $\Pr\{\mathcal{E}\}$ typically decreases at least exponentially in
  the parameter of interest. Traditionally, these strong concentration
  results require (at least) careful on independent events and/or the
  application of concentration inequalities.
  
  \item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing an
  efficient encoding---an algorithmic problem. Consider the following 
  two problems:
    \begin{enumerate}

    \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
       a random graph on $n$ vertices contains a clique of size $k=\lceil
       4\log n\rceil$.

    \item Design a encoding for graphs on $n$ vertices so that a graph,
       $G$, that contains a clique of size $k=\lceil 4\log n\rceil$
       is encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note:
       Your encoding and decoding algoritms don't have to be efficient,
       just correct.)
    \end{enumerate}
  Most computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that
  they can use Boole's Inequality will still be stuck
  wrestling with the formula $\binom{n}{4\log n}2^{-\binom{k}{2}}$.  
\end{enumerate}

Our motivation for doing this is that encoding arguments are an easily
accessible, yet versatile tool for answering many questions.  Most of
these arguments can be applied after learning almost no probability
theory beyond the fact that, if a finite set $X$ contains $r$ special
elements and we pick an element uniformly at random from $X$, then the
probability of picking a special element is $r/|X|$.

In \secref{uel}, we present the \emph{Uniform Encoding Lemma}, which is
the basis of most of our encoding arguments.  \Secref{applications-i}
presents  applications of the Uniform Encoding Lemma to a variety of
problems.  \Secref{nuel} presents a more general Non-Uniform Encoding
Lemma that can handle a larger variety of applications, some of which are
presented in \secref{applications-ii}.  \Secref{summary} summarizes
and concludes with some directions for future research.



\section{The Uniform Encoding Lemma}
\seclabel{uel}

A \emph{code}, $C\from X\to \{0,1\}^*$ is a one-to-function from a set
$X$ to the set of binary strings.  The elements of the range of $C$ are
called $C$'s \emph{codewords}.

A code, $C$, is \emph{prefix-free} if, for every $x,y\in X$ the binary
string $C(x)$ is not a prefix of $C(y)$.  It can be helpful to think
of prefix-free codes as (rooted ordered) binary trees whose leaves
are labelled with the elements in $S$.  The codeword for a particular
$x\in X$ is obtained by tracing the root-to-leaf path leading to $x$
and outputting a 0 (respectively, 1) each time this path goes from a
parent to its left (respectively, right) child. (See \figref{bintree}.)

\begin{figure}
  \centering{\includegraphics{bintree}}
  \caption{A prefix-free code for the set
    $S=\{\mathtt{a},\mathtt{b},\mathtt{c},\mathtt{d},\mathtt{e},\mathtt{f}\}$
    and the corresponding leaf-labelled binary tree.}
  \figlabel{bintree}
\end{figure}

If $C$ is prefix-free, then the number of $C$'s codewords that have
length \emph{at most} $k$ is not more than $2^k$. To see this why this
is so, note that $C$ can be modified into a code $\hat C$, in which
every codeword of length less than $k$ is extended so that it has length
exactly $k$ (the prefix-freeness of $C$ ensures that $\hat C$ is also a
prefix-free code). The number of $\hat C$'s codewords of length $k$ is
equal to the the number of $C$'s codewords of length at most $k$; since
codewords are just binary strings, there are not more than $2^k$ of these.

\begin{lem}[Uniform Encoding Lemma]
  Let $C\from S\to \{0,1\}^*$ be a prefix-free code. When an element $x$
  is chosen uniformly at random from $S$, $\Pr\{|C(x)|\le \log|S|-s\}\le
  2^{-s}$.
\end{lem}

\begin{proof}
  Since $C$ is a one-to-one function, choosing $x$ uniformly from $S$
  implies that $C(x)$ is chosen uniformly from $C$'s codewords. Let
  $k=\log|S|-s$. Since $C$ has $|S|$ codewords and at most $2^{k}$ of these
  have length at most $k$, the probability that we choose one of these 
  codewords is at most
  \[   \frac{2^k}{|S|} = \frac{2^{\log|S|-s}}{|S|} = 2^{-s} \enspace . \qedhere \]
\end{proof}

\section{Applications of the Uniform Encoding Lemma}
\seclabel{applications-i}


\subsection{Runs in a Random Binary String}

As a warm-up exercise, we show how the Uniform Encoding Lemma can be
used to show that a random $n$-bit string is unlikely to contain a run
of significantly more than $\log n$ one bits.

\begin{thm}\thmlabel{one-runs}
  Let $x=\langle x_1,\ldots,x_n\rangle\in\{0,1\}^n$ be chosen
  uniformly at random and let $t=1+\lceil\log n\rceil + s$. Then, the
  probability that there exists an $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  Let $\mathcal{E}_i$ be the event ``$x_i=x_{i+1}=\cdots=x_{i+t-1}=1$'',
  so that the theorem asks us to show that $\Pr \bigcup_{i=1}^{n-k-1}
  \mathcal{E}_i \le 2^{-s}$.  We show this by constructing a prefix-free
  code $C\from\{0,1\}^n\to\{0,1\}^*$.  If $\mathcal{E}_i$ does not
  occur for any index $i\in\{1,\ldots,n-k-1\}$, then $C(x)$ is a zero
  bit followed by the $n$ bits of $x$.  However, if $\mathcal{E}_i$
  does occur for some index $i$, then $C(x)$ is a one bit, followed
  by (the binary encoding of) $i$, followed by the $n-k$ bits
  $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$.

  Observe that, in the latter case, $C(x)$ has length 
  \[
      1 + \lceil\log n\rceil + n - k = n-s \enspace .
  \]
  It is easy to check that, for any $x$, we can reconstruct
  $x_1,\ldots,x_n$ from $C(x)$, so $C$ is indeed a code for $\{0,1\}^n$.
  It is also easy to check that $C$ is prefix free, since the first bit
  of $C(x)$ determines the length of $C(x)$.

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, 
  \[
      \Pr\{|C(x)|\le n-s\} = \Pr \bigcup_{i=1}^{n-k-1} \mathcal{E}_i \le 2^{-s} \enspace . \qedhere \]
\end{proof}

\subsection{Graphs with no Large Clique or Independent Set}


A \emph{Erd\H{o}s-R\'enyi random graph}, $G_{n,p}$ is a graph with vertex
set $V=\{1,\ldots,n\}$ and in which each edge $uw\in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.  Erd\H{o}s \cite{X} used the random
graph $G_{n,\frac{1}{2}}$ to prove the existence of graphs having no
large clique and no large independent set. Here we show how this can be
done using an encoding argument.

\begin{thm}\thmlabel{erdos-renyi-i}
  For any $\epsilon>0$ and for all sufficiently large $n$, the probability
  that the random graph, $G_{n,\frac{1}{2}}$ contains a clique or an
  independent set of size $t = \lceil(2+\epsilon)\log n + \sqrt{s}\rceil$
  is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that compresses the $r=\binom{n}{2}$ bits
  of $G$'s adjacency matrix, as they appear in row-major order.  If $G$
  has no clique or independent set of size $t$, then the code consists of
  a 0 bit followed by the $\binom{n}{2}$ bits of $G$'s adjacency matrix
  in row-major order.
  
  Otherwise, $G$ contains a clique or independent set, $S$, of size $t$.
  Then the code consists of a 1; another bit indicating whether $S$ is a
  clique or IS; the vertices of $S$; then the adjacency matrix of $G$ in
  row major-order but leaving out all of the $\binom{t}{2}$ bits implied
  by the edges or non-edges in $S$.
  
  In the second case, this encoding requires 
  \begin{align*}
     b & = 2 + t\log n + \binom{n}{2}-\binom{t}{2} \\
       & \le \binom{n}{2} - s 
  \end{align*}
  bits (because of our choice of $t$).   Applying the Uniform Encoding
  Lemma completes the proof.
\end{proof}

\begin{rem}
The bound in \thmref{erdos-renyi-i} can be strengthened a little, since
the elements of $S$ can be encoded with fewer than $t\log n$ bits; in
particular $\lceil\log\binom{n}{t}\rceil=t\log n - t\log t + t\log e -
O(\log t)$ bits suffice.  With a careful calculation, using the approximations
described in the next section, the proof then
works with $t=2\log n +O(\log\log n) + \sqrt{s}$. This comes closer to
ErdÅ‘s' original result, which was at the threshold $2\log n - 2\log\log
n + O(1)$.
\end{rem}

\subsection{Some Binomial Approximations}

For some of our encoding arguments we use Stirling's approximation for $n!$, 
which tells us that we can encode any one of $n!$ possibilities using

\begin{align}
  \lceil \log n!\rceil 
      & \le n\log n - n\log e + (1/2)\log(2\pi n) + O(1)  
             \eqlabel{stirling-tight} \\
      & \le n\log n - n\log e + O(\log n)  
             \eqlabel{stirling-loose}
\end{align}
bits.

Similarly, we can encode any $k$-subset of a set of size $n$ using

\begin{align*}
  \left\lceil\log \binom{n}{k}\right\rceil 
     & = \log n! - \log k! - \log (n-k)! \\
     & = n\log n - k\log k - (n-k)\log(n-k) + \log(2\pi n) - \log (2\pi k) - \log (2\pi (n-k)) + O(1) \\
     & = n\log n - k\log k - (n-k)\log(n-k) - \log(2\pi n) + O(1)  \\
     & = n\log n - k\log k - (n-k)\log n + (n-k)(\log n-\log(n-k)) - \log(2\pi n) + O(1) \\
     & = k\log n - k\log k + \underbrace{(n-k)(\log n-\log(n-k))}_\text{yuck!}  - \log(2\pi n) + O(1) 
         \numberthis \eqlabel{hassle} \\ 
\end{align*}

The subexpression $(n-k)(\log n-\log(n-k))$ is awkward here. We can
simplify it as follows:
\begin{align*}
   (n-k)(\log n-\log(n-k))
      & = (n-k)\log \left(\frac{n}{n-k}\right) \\
      & = (n-k)\log\left(\frac{t+k}{t}\right) & \text{(substituting $t=n-k$)} \\
      & = (n-k)\log(1+k/t) \\
      & \le (n-k)(k/t)\log e & \text{(since $1+x \le e^x$)} \\
      & = (n-k)(k/(n-k))\log e \\
      & = k\log e 
\end{align*}

Using this, \eqref{hassle} becomes
\begin{equation}
  \left\lceil\log \binom{n}{k}\right\rceil 
    \le k\log n - k\log k + k\log e - \log (2\pi n) + O(1) \eqlabel{log-n-choose-k}
\end{equation} 

\subsection{Balls in Urns}

\begin{thm}
  Suppose we throw $n$ balls indepedently and uniformly at random into $n$
  urns. Then, the probability that any urn contains more than $t=blah$
  balls is at most $2^{-s}$.
\end{thm}

\begin{proof}
  For each $i\in\{1,\ldots,n\}$, let $b_i$ denote the index of the urn in
  which the $i$th ball lands. Notice that the sequence $b_1,\ldots,b_n$
  is chosen uniformly at random from a set of size $n^n$, and it is this
  choice that will be used in our encoding argument.

  If no urn contains $t$ or more balls, then our code is a 0 bit followed
  by the obvious encoding. If some urn, $i$, contains $t$ or more balls,
  then our code is a 1 bit followed by the value $i$ ($\lceil \log
  n\rceil$ bits) followed by a code for $t$ of the balls in urn $i$
  ($\log\binom{n}{t}$) bits, followed by the remaining $n-t$ values of
  $b_1,\ldots,b_n$ that cannot be deduced from the preceding information.
  In total, this is
  \begin{align*}
    b &\le 1 + \lceil\log n\rceil + \log\binom{n}{t} 
           + \lceil(n-t)\log n\rceil \\
     & = 1 + \lceil\log n\rceil + t\log n - t\log t + t\log e  
           + (n-t)\log n - \log(2\pi n) + O(1)
             & \text{(by applying \eqref{log-n-choose-k})} \\
     & = n\log n - t\log t + t\log e + O(1)
  \end{align*}
\end{proof}

\subsection{Analysis of Insertion Sort}

Recall the insertion-sort algorithm for sorting a list, $A_1,\ldots,A_n$
of $n$ elements:

\noindent{$\textsc{InsertionSort}(A_1,\ldots,A_n)$}
\begin{algorithmic}[1]
  \FOR{$i\gets 2$ \TO $n$}
     \STATE{$j \gets i$}
     \WHILE{$j>1$ \AND $A_{j-1} > A_j$}
         \STATE{$A_j \leftrightarrow A_{j-1}$
            \COMMENT{ swap }}
         \STATE{$j\gets j-1$}
     \ENDWHILE
  \ENDFOR
\end{algorithmic}

A typical question asks the expected number of times Line~4 executes
if $A_1,\ldots,A_n$ is a uniformly random permutation of $n$ distinct
elements.  The answer $\binom{n}{2}/2$ is an easy application of
linearity of expectation.\footnote{For every one of the $\binom{n}{2}$
pairs $p,q\in\{1,\ldots,n\}$ with $p<q$, the values initially stored at
positions $A_p$ and $A_q$ will eventually be swapped if and only if $A_p >
A_q$, which happens with probability $1/2$ in a random permutation.}

A more advanced question is to ask for a concentration result on the
number of executions of Line~4. This is a harder question to tackle;
because $>$ is transitive, the $\binom{n}{2}$ events being studied have
a lot of interdependence.  With some intutition about independence, and
the use of the right concentration inequality, one can obtain a fairly
tight bound with a fairly short proof.  In the following, we show how
this can be done with an encoding argument:

\begin{thm}\thmlabel{insertion-sort}
For a random permutation, $A_1,\ldots,A_n$ of $n$ distinct elements,
the probability that Line~4 of \textsc{InsertionSort} executes fewer than
$\alpha n^2 - n + 2$ times is at most $2^{n\log(\alpha e^2)+O(\log n)}$.
\end{thm}

\begin{proof}
Let $\pi$ be the permutation of $\{1,\ldots,n\}$ that defines the
sorted order of $A_1,\ldots,A_n$, so that $A_{\pi_j}$ is the element
that appears at position $j$ after sorting.

We encode the permutation $\pi$ by recording the execution of
InsertionSort on this permutation. In particular, we record, for each
$i\in\{2,\ldots,n\}$, the number of times, $m_i$, that Line~4 executes
during the $i$th iteration of \textsc{InsertionSort}. With this information,
one can run the following version of \textsc{InsertionSort} to recover $\pi$:

\noindent{$\textsc{InsertionSortReconstruct}(m_2,\ldots,m_n)$}:
\begin{algorithmic}[1]
  \STATE{$\pi \gets \langle 1,\ldots,n\rangle$}
  \FOR{$i\gets 2$ \TO $n$}
     \FOR{$j\gets i \textbf{ down to } i-m_i+1$}
         \STATE{$\pi_j \leftrightarrow \pi_{j-1}$
            \COMMENT{ swap }}
     \ENDFOR
  \ENDFOR
\end{algorithmic}
 
To make this work, we have to be slightly clever with this
encoding. Rather than encode $m_2,m_3,\ldots,m_n$ directly, we first
encode $m=\sum_{i=2}^{n} m_i$ using $\lceil 2\log n\rceil$ bits (since $m < n^2$). Given $m$, what
remains is to describe the partition of $m$ into $n-1$ non-negative
integers $m_2,\ldots,m_n$; there are $\binom{m+n-2}{n-2}$ such
partitions.\footnote{To see this, draw $m+n-2$ white dots on a line,
then choose $n-2$ dots to colour black. This splits the remaining $m$ white dots
up into $n-1$ groups, which determine the values of $m_2,\ldots,m_n$.}

Therefore, the values of $m_2,\ldots,m_n$ can be encoded using
\[
    b = \lceil 2\log n\rceil + \log\binom{m+n-2}{n-2}
\]
bits and this is sufficient to recover the permutation that defines
$A_1,\ldots,A_n$.  By applying \eqref{log-n-choose-k} to $b$, we obtain
\begin{align*}
  b & \le (n-2)\log(m+n-2) - (n-2)\log(n-2)  + (n-2)\log e + O(\log n) \\
    & \le n\log(m+n-2) - n\log(n-2)  + n\log e + O(\log n) \\
    & \le n\log(\alpha n^2) - n\log(n-2)  + n\log e + O(\log n) \\
    & \le n\log(\alpha n^2) - n\log n  + n\log e + O(\log n) \\
    & = 2n\log n + n\log\alpha - n\log n  + n\log e + O(\log n) \\
    & = n\log n + n\log\alpha + n\log e + O(\log n) \\
    & = \log n! + n\log\alpha + 2n\log e + O(\log n) \\
    & = \log n! + n(\log \alpha e^2) + O(\log n)  \qedhere
\end{align*}
\end{proof}

\begin{rem}
Although it doesn't require any advanced probability,
\thmref{insertion-sort} is not sharp; it only gives a non-trivial
probability when $\alpha < 1/e^2$.  To obtain a sharp bound, one can use
the fact that $m_2,\ldots,m_n$ are independent and $m_i$ is uniform over
$\{0,\ldots,i-1\}$ and then use the method of bounded differences \cite{S}
to show that $m$ is concentrated in an interval of size $O(n^{3/2})$.
\end{rem}



\section{The Non-Uniform Encoding Lemma}
\seclabel{nuel}

\section{Applications of the Non-Uniform Encoding Lemma}
\seclabel{applications-ii}


\section{Summary and Conclusions}
\seclabel{summary}


\end{document}
