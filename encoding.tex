\documentclass{patmorin}
 
\usepackage{pat}
\usepackage[utf8]{inputenc}
\usepackage[noend]{algorithmic}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{url}
\usepackage{comment}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% taken from http://goo.gl/JVNEL2
\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\title{\MakeUppercase{Encoding Arguments}\thanks{This research is partially funded by NSERC.}}
\author{Pat Morin, Wolfgang Mulzer, and Tommy Reddad}
\date{}

\newcommand{\aremark}[3]{\textcolor{blue}{\textsc{#1 #2:}}
  \textcolor{red}{\textsf{#3}}}
\newcommand{\pat}[2][says]{\aremark{Pat}{#1}{#2}}
\newcommand{\tommy}[2][says]{\aremark{Tommy}{#1}{#2}}
\newcommand{\wolfgang}[2][says]{\aremark{Wolfgang}{#1}{#2}}


\begin{document}
\begin{titlepage}
\maketitle


\begin{abstract}
  \setlength{\baselineskip}{15.84pt}

  This expository article surveys ``encoding arguments.'' In its
  most most basic form an encoding argument proves an upper bound on
  the probability of an event using the fact a uniformly random choice
  from a set of size $n$ can not be encoded with fewer than $\log_2 n$
  bits on average.

  We survey many applications of this basic argument, give a
  generalization of this argument for the case of non-uniform
  distributions, and give a rigorous justification for the use of
  non-integer codeword lengths in encoding arguments.  These latter
  two results allow encoding arguments to be applied more widely and
  to produce tighter results.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, 
          incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}
\setlength{\baselineskip}{15.84pt}

There is no doubt that probability theory plays a fundamental role in
computer science: Some of the fastest and simplest fundamental
algorithms and data structures are randomized~\cite{mitzenmacher.upfal:probability, motwani.raghavan:randomized}; 
average-case analysis of algorithms relies entirely on tools from probability 
theory~\cite{flajolet.sedgewick:aofa}; and
many difficult combinatorial questions have strikingly simple
solutions using probabilistic arguments~\cite{alon:probabilistic}.

\wolfgang{Do you really want to phrase it as bluntly as this?
It sounds a bit like ``Computer scientists do not understand
mathematics, so we need to dumb it down for them.''
I could also see an expository approach where we say that
most of these arguments are basically counting, and that
encoding arguments are a particularly ``computer sciency''
way of thinking about the usual injection approach to counting.}
Unfortunately, many of these beautiful results are inaccessible to
most computer scientists because of a view that ``the math is too
hard.''  For instance, the 2013 edition of ACM/IEEE Curriculum
Guidelines for Undergraduate Degree Progams in Computer Science does
not require a full course in probability theory
\cite[Page~50]{computing-curricula:computer}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on
discrete probability, as part of the discrete structures curriculum
\cite[Page~77]{computing-curricula:computer}.

\wolfgang{As above, maybe one could also emphasize that
the uniform coding lemma is basically a bijection counting argument.}
In this expository paper, we survey applications of ``encoding
arguments'' that transform the problem of upper-bounding the
probability of a specific event, $\mathcal{E}$, into the problem of
devising a code for the set of elementary events in $\mathcal{E}$.
Encoding arguments have several advantages over traditional
probabilistic analysis:

\begin{enumerate}
\item Encoding arguments are almost ``probability-free.''  Except for
  applying a simple \emph{Uniform Encoding Lemma}, there is no
  probability involved.  In particular, there is no chance of
  common mistakes such as multiplying probabilities of non-independent
  events or (equivalently) multiplying expectations.

  The proof of the Uniform Encoding Lemma itself is trivial and the
  only probability it uses is the fact that, if a finite set $X$
  contains $r$ special elements and we pick an element uniformly at
  random from $X$, then the probability of picking a special element
  is $r/|X|$.

\item Encoding arguments usually yield strong results;
  $\Pr\{\mathcal{E}\}$ typically decreases at least exponentially in
  the parameter of interest. Traditionally, these strong concentration
  results require (at least) careful calculations on probabilities of
  independent events and/or the application of concentration
  inequalities.  The subject of concentration inequalities is advanced
  enough to be the topic of entire textbooks
  \cite{boucheron.lugosi.ea:concentration,dubhashi.panconesi:concentration}.
  
\item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing
  an efficient code---an algorithmic problem. Consider the following
  two problems:
  \begin{enumerate}

  \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
    a random graph on $n$ vertices contains a clique of size $k=\lceil
    4\log n\rceil$.\footnote{Since we are overwhelmingly concerned with
    binary encoding, we will
    agree now that the base of logarithms in $\log x$ is $2$, except when
    explicitly stated otherwise.}

  \item Design an encoding for graphs on $n$ vertices so that a graph,
    $G$, that contains a clique of size $k=\lceil 4\log n\rceil$, is
    encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note: Your
    encoding and decoding algorithms don't have to be efficient, just
    correct.)
  \end{enumerate}
  Many computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that they
  can use Boole's Inequality will still be stuck wrestling with the
  formula $\binom{n}{4\log n}2^{-\binom{4\log n}{2}}$.
\end{enumerate}

Our motivation for this work is that encoding arguments are an easily
accessible, yet versatile tool for answering many questions.  Most of
these arguments can be applied after learning almost no probability
theory beyond the Encoding Lemma mentioned above.

The remainder of this article is organized as follows: In
\secref{uel}, we present necessary background, including the
\emph{Uniform Encoding Lemma}, which is the basis of most of our
encoding arguments. In \Secref{applications-i} we show how the Uniform
Encoding Lemma can be applied to a variety of problems. In
\Secref{nuel}, we introduce a more general Non-Uniform Encoding Lemma
that can handle a larger variety of applications, some of which are
given in \secref{applications-ii}.  \Secref{el} presents an
alternative view of encoding arguments, justifying the use of
non-integer codeword lengths.  \Secref{summary} summarizes and
concludes with some directions for future research.

\section{Background}
\seclabel{uel}

This section presents the necessary background on prefix-free codes
and binomial coefficients.

\subsection{Basic Definitions, Prefix-free Codes and the Uniform 
Encoding Lemma}

A finite length \emph{binary string}, or \emph{bit string}, is a
finite (possibly empty) sequence of elements from $\{0, 1\}$. For
$k \in \N$, we denote by $\{0, 1\}^k$ the set of all binary strings of
length $k$, and by $\{0,1\}^*$ the set of all finite strings.
For a binary string, $x$, we use $|x|$ to denote the length of $x$.  A
binary string of length $n$ will be called an $n$-bit string. 
Furthermore, we denote by $n_1(x)$ the number of $1$-bits in
$x$, and by $n_0(x)$ the number of zero bits.
Given a (finite or countable) set $X$, a
\emph{code} $C\from X\to \{0,1\}^*$ is a one-to-one function from 
$X$ to the set of finite length binary strings.  The elements of
the range of $C$ are called $C$'s
\emph{codewords}. Often, there are some
elements of the set $X$ that are not of interest to us.  In these
cases, we consider partial codes. A \emph{partial code}
$C\from X\nrightarrow \{0,1\}^*$ is a one-to-one partial function.
When discussing partial codes we will use the convention that
$|C(x)|=\infty$ if $x$ is not in the domain of $C$.

A binary string
$x$ is called a \emph{prefix} of another binary string $y$ if there
is some binary string $z$ such that $xz = y$. A (partial) code $C$
is \emph{prefix-free} if, for every pair $x\neq y$ in the domain of
$C$, the codeword $C(x)$ is not a prefix of the codeword $C(y)$.  It can be
helpful to visualize prefix-free
codes as (rooted ordered) binary trees whose leaves are labelled with
the elements of $X$.  The codeword for a particular
$x\in X$ is obtained by tracing the root-to-leaf path leading to $x$
and outputting a 0 each time this path goes from a parent to its left
child, and a 1 each time it goes to a right child. (See
\figref{bintree}.)

\begin{figure}
  \centering{\includegraphics{bintree}}
  \caption{A prefix-free code for the set
    $X=\{\mathtt{a},\mathtt{b},\mathtt{c},\mathtt{d},\mathtt{e},\mathtt{f}\}$
    and the corresponding leaf-labelled binary tree (which can also be
    viewed as a partial prefix-free code for the set
    $\{\mathtt{a},\mathtt{b},\mathtt{c},\ldots,\mathtt{z}\}$).}
  \figlabel{bintree}
\end{figure}

We claim that if $C$ is prefix-free, then the
number of $C$'s codewords that have length at most $k$ is not more
than $2^k$. To see this, observe that $C$ can be
modified into a code $\widehat C$, in which every codeword of length
$\ell <k$ is extended---by appending $k-\ell$ zeros---so that it has
length exactly $k$. The prefix-freeness of $C$ ensures that $\widehat C$
is also prefix-free. The number of $\widehat C$'s codewords of
length $k$ is equal to the number of $C$'s codewords of length at
most $k$; since codewords are just binary strings, there are not more
than $2^k$ of these.

Observe that every finite set $X$ has a prefix-free code in which
every codeword has length $\lceil\log |X|\rceil$. We simply enumerate
the elements of $X$ in some order $x_0,x_1,\ldots,x_{|X|-1}$ and
assign to each $x_i$ the binary representation of $i$ (padded with
leading zeros), which has length $\lceil\log |X|\rceil$, since
$i\in\{0,\ldots,|X|-1\}$.  We call this encoding the \emph{fixed-length
code} for $X$, and we will use it implicitly in many arguments.

Given a (finite or countable) set $X$, a \emph{probability distribution} 
$p: X \rightarrow [0,1]$ on $X$ is a function with
$\sum_{x \in X} p(x) = 1$. We sometimes write $p_x$
instead of $p(x)$. 
The \emph{uniform distribution} is given by $p_x = 1/|X|$ for
all $x \in X$.
Fix a number $n \in \N$ and a
parameter $\alpha \in [0,1]$.
A probability distribution on $X = \{0, 1\}^n$
that is of particular interest to us is the \emph{Bernoulli
distribution} with parameter $\alpha$, denoted 
$\mathrm{Bernoulli}(\alpha)$. In this distribution, a
random bit string $x \in \{0, 1\}^n$ is sampled by setting each bit
to $1$ with probability $\alpha$ and to $0$ with probability $1-\alpha$,
independently of the other bits.

The following lemma provides the foundation on which this
survey is built. The lemma is folklore, but as we will see in
the following sections, it has an incredibly wide range of
applications and can lead to surprisingly powerful results.
\begin{lem}[Uniform Encoding Lemma]\lemlabel{uel}
  Let $X$ be a finite set, and 
  let $C\from X\nrightarrow \{0,1\}^*$ be a partial prefix-free
  code. If an element $x\in X$ is chosen uniformly at random, then
  \[
    \Pr\{|C(x)|\le \log|X|-s\}\le 2^{-s} \enspace .
  \]
\end{lem}

\begin{proof}
  Let $k=\lfloor \log|X|-s \rfloor$ and recall that $C$ has at most $2^{k}$ codewords
  of length at most $k$.  Since $C$ is one-to-one each such codeword
  has at most one preimage in $X$.  Since $x$ is chosen uniformly at
  random from $X$, the probability that it is the preimage of one of
  these short codewords is at most
  \[
  \frac{2^k}{|X|} \leq \frac{2^{\log|X|-s}}{|X|} = 2^{-s} \enspace
  . \qedhere
  \]
\end{proof}

\subsection{Runs in Binary Strings}

As a warm-up exercise to illustrate the use of the Uniform Encoding
Lemma we will show that a random $n$-bit string is unlikely to contain
a run of significantly more than $\log n$ one bits.  (See
\figref{runs-i}.)

\begin{figure}
  \centering{\includegraphics{runs-i}}
  \caption{Illustration of \thmref{runs-i} and its proof.}
  \figlabel{runs-i}
\end{figure}

\begin{thm}\thmlabel{runs-i}
  Let $x=(x_1,\ldots,x_n)\in\{0,1\}^n$ be chosen uniformly at random
  and let $t = \Ceil{\ceil{\log n} + s}$. Then, the probability that
  there exists an $i\in\{1,\ldots,n-t+1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  We will prove this theorem by constructing a partial prefix-free
  code for strings having a run of $t$ or more ones.  For such a
  string $x=(x_1,\ldots,x_n)$ let $i$ be the minimum index such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$. The codeword $C(x)$ for $x$ is the
  binary string that consists of the ($\ceil{\log n}$-bit binary
  encoding of the) index $i$ followed by the $n-t$ bits
  $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$. (See \figref{runs-i}.)

  Observe that $C(x)$ has length 
  \[
    \ceil{\log n} + n - t \le n-s \enspace .
  \]
   For any
  such $x$, we can reconstruct $(x_1,\ldots,x_n)$ from $C(x)$ by
  reading it from left to right. Indeed, the leftmost $\ceil{\log n}$
  bits from $C(x)$ tell us, in binary, the value of an index $i$ for
  which $x_i = x_{i + 1} = \dots = x_{i + t - 1} = 1$; the following
  $n - t$ bits in sequence give us the values of the remaining unknown
  bits
  $x_1, x_2, \dots, x_{i - 1}, x_{i + t}, x_{i + t + 1}, \dots,
  x_n$. Thus, $C$ is a partial code whose domain is the set of binary
  strings of length $n$ having a run of $t$ or more ones.  Also,
  $C$ is prefix-free, since all codewords are unique and have the same length.

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, the probability that there
  exists any index $i\in\{1,\ldots,n-t-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most
  \[
    \Pr\{|C(x)|\le n-s\} \le 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

Simple as it is, the proof of \thmref{runs-i} contains the main ideas
used in most encoding arguments: 
\begin{enumerate}
\item The arguments usually show that a particular \emph{bad event} is
  unlikely. In \thmref{runs-i} the bad event is the occurrence of a
  substring of $t$ consecutive ones.

\item The code is a partial prefix-free code whose domain is the bad
  event, whose elements we call the \emph{bad outcomes}. In this case, the code
  $C$ is only capable of encoding strings containing a run of $t$
  consecutive ones, and a particular string containing such a run is a
  bad outcome.

\item The code usually begins with a concise description of the bad
  outcome, and is then followed by a straightforward encoding of the
  information that is not implied by the bad outcome. In
  \thmref{runs-i}, the bad outcome is completely described by the index
  $i$ at which the run of $t$ ones begins, and this implies that
  the bits $x_i,\ldots,x_{i+t-1}$ are all equal to 1, so these bits do
  not need to be specified in the second part of the codeword.
\end{enumerate}

\subsection{A Note on Ceilings}\seclabel{ceilings}

Note that \thmref{runs-i} also has an easy proof using the union
bound: If we let $\mathcal{E}_i$ denote the event
$x_i=x_{i+1}=\cdots=x_{i+t}=1$, then
\begin{align*}
  \Pr \left\{\bigcup_{i=0}^{n-t-1} \mathcal{E}_i\right\}
  & \le \sum_{i=0}^{n-t-1} \Pr \{\mathcal{E}_i\} && \text{(using the union bound)}\\
  & = \sum_{i=0}^{n-t-1} 2^{-t}  &&\text{(using the independence of the $x_i$'s)}\\
  & \le n2^{-t} && \text{(the sum has $n-t\le n$ identical terms)}\\
  & \le n2^{-\lceil\log n\rceil-s}&& \text{(from the definition of $t$)}\\
  & \le 2^{-s} \enspace .
\end{align*}
This traditional proof also works with the sometimes smaller value
$t=\ceil{\log n+s}$ (note the lack of a ceiling over the logarithmic
term), in which case the final inequality becomes an equality.

In the encoding proof of \thmref{runs-i}, the ceiling in the
expression for $t$ is an artifact of encoding the integer $i$ which is
taken from a set of size $n$. When sketching an encoding argument, we
think of this as requiring $\log n$ bits. Nonetheless, when 
 the time comes to carefully write down a
proof we include a ceiling over this term since bits are a discrete
quantity.

In \secref{el}, however, we will 
formally justify that the informal
intuition we use in blackboard proofs is actually valid; we can think
of the encoding of $i$ using $\log n$ bits even if $\log n$ is not an
integer.  In general we can imagine encoding a choice from among $r$
options using $\log r$ bits for any $r\in\N$.  From this point
onwards, we omit ceilings this way in all our theorems and
proofs. This simplifies calculations and provides tighter results.
For now, it allows us to state the following cleaner version of
\thmref{runs-i}:

\begin{customthm}{\ref*{thm:runs-i}b}\thmlabel{runs-ii}
  Let $x=(x_1,\ldots,x_n)\in\{0,1\}^n$ be chosen uniformly at random
  and let $t = \ceil{\log n + s}$. Then, the probability that there
  exists an $i\in\{1,\ldots,n-t+1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{customthm}

\subsection{Encoding Sparse Bit Strings}\seclabel{sparse-bit-strings}

At this point we should also point out an extremely useful trick 
for encoding sparse bit strings. For any $\alpha\in(0,1)$,
there exists a code $C_\alpha\from \{0,1\}^n\rightarrow \{0,1\}^*$
such that, for any bit string $x\in\{0,1\}^n$ having $n_1(x)$ ones and
$n_0(x)$ zeros,
\begin{equation}
  |C_\alpha(x)| = \lceil n_1(x)\log(1/\alpha) + n_0(x)\log(1/(1-\alpha)) \rceil \enspace .
  \eqlabel{sparse-bitstring-i}
\end{equation}
This code is the \emph{Shannon-Fano code}
for $\mathrm{Bernoulli}(\alpha)$ bit strings of length $n$
\cite{fano:transmission,shannon:mathematical}. 
More generally, for any probability density $p : X \to [0, 1]$,
there is a Shannon-Fano code $C : X \to \{0, 1\}^*$ 
such that
\[
  |C(x)| = \ceil{\log (1/p_x)} \enspace .
\]
Moreover, we can construct such a code deterministically, even when
$X$ is countably infinite.

Again, as we will see in \secref{el}, we can omit the ceiling in the
expression for $|C_\alpha(x)|$.  This is true for any value of $n$. In
particular, for $n=1$, it gives us a ``code'' for encoding a single
bit where the cost of encoding a 1 is $\log(1/\alpha)$ and the cost of
encoding a 0 is $\log(1/(1-\alpha))$.  Indeed, the ``code'' for bit
strings of length $n>1$ is just what we get when we apply this 1-bit
code to each bit of the bit string.

If we wish to encode bit strings of length $n$ and we know in advance
that the strings contain exactly $k$ one bits, then we can obtain
an optimal code 
by taking $\alpha=k/n$. The resulting fixed length
code has length
\begin{equation}
  k\log (n/k) + (n-k)\log(n/(n-k))  \eqlabel{sparse-bitstring-ii} \enspace .
\end{equation}
\Eqref{sparse-bitstring-ii} brings us to our next topic: binary entropy.

\subsection{Binary Entropy}

The \emph{binary entropy function} $H\from (0,1)\to(0,1]$ is defined
by
\[
  H(\alpha) = \alpha\log(1/\alpha) + (1-\alpha)\log(1/(1-\alpha)) 
\]
and it will be quite useful.  The binary entropy function and two upper
bounds on it that we derive below are illustrated in \figref{entropy}.

\begin{figure}
  \centering{\includegraphics{entropy}}
  \caption{Binary entropy, $H$, and two useful approximations.}
  \figlabel{entropy}
\end{figure}

We have already encountered a quantity that can be
expressed in terms of the binary entropy.  From
\eqref{sparse-bitstring-ii}, a bit string of length $n$ with
exactly $k$ one bits can be encoded with a fixed-length code of 
$nH(k/n)$ bits.

The binary entropy function can be difficult to work with, so it is
helpful to have some manageable approximations.  One of these is
derived as follows:
\begin{align}
  H(\alpha) & = \alpha\log(1/\alpha) + (1-\alpha)\log(1/(1-\alpha)) \notag \\
            & = \alpha\log(1/\alpha) + (1-\alpha)\log(1+\alpha/(1-\alpha)) \notag \\
            & \le \alpha\log(1/\alpha) + \alpha\log e \eqlabel{entropy-i} 
\end{align}
since $1+x\le e^x$ for all $x\in\mathbb{R}$. \Eqref{entropy-i} is a
useful approximation when $\alpha$ is close to zero, in which case 
$H(\alpha)$ is also close to zero.

For $\alpha$ close to $1/2$ (in which case $H(\alpha)$ is close to 1),
we obtain a good approximation from the Taylor series expansion at
$1/2$.  Indeed, a simple calculation shows that
\[
  H'(\alpha) = \log (1/\alpha) - \log (1/(1-\alpha))
\]
and that
\[
  H^{(i)}(\alpha) = \frac{(i-2)!}{\ln 2} \left( \frac{(-1)^{i-1}}{\alpha^{i-1}} - \frac{1}{(1-\alpha)^{i-1}}\right) ,
\]
for $i \geq 2$. Hence, $H^{(i)}(1/2) = 0$, for $i \geq 1$ odd, and
\[
  H^{(i)}(1/2) = -\frac{(i-2)!\, 2^i}{\ln 2}, 
\]
for $i \geq 2$ even. The Taylor series expansion at $1/2$ now gives
\begin{align}
  H(\alpha) &= 
  H(1/2) + \sum_{i = 1}^{\infty} \frac{H^{(i)}(1/2)}{i!}
     \left(\alpha-(1/2)\right)^i \notag\\
 &= 
1 - \frac{1}{\ln 2}\sum_{i = 1}^{\infty} \frac{(2i-2)! 2^{2i}}{(2i)!2^{2i}}
     \left(2\alpha-1\right)^{2i}\notag\\
&= 1-\frac{1}{2\ln 2}\sum_{i=1}^{\infty}\frac{(2\alpha-1)^{2i}}{i(2i-1)} .
              \notag \\ 
\intertext{ In particular, for $\alpha=(1-\eps)/2$,}
H(\alpha) & = 1-\frac{1}{2\ln 2}\sum_{i=1}^{\infty}\frac{\eps^{2i}}{i(2i-1)} 
              \notag \\ 
            & < 1-\frac{\eps^2}{2\ln 2} \enspace .\eqlabel{entropy-ii}
\end{align}

\subsection{Basic Chernoff Bound}

With all the pieces in place, we can now give an encoding argument for
a well-known and extremely useful result typically attributed to
Chernoff \cite{chernoff:bound}.

\begin{thm}\thmlabel{chernoff-basic}
  Let $x\in\{0,1\}^n$ be chosen uniformly at random. Then, for any
  $\eps \geq 0$,
  \[
    \Pr\{n_1(x) \le n(1-\eps)/2\} \le e^{-\eps^2n/2} \enspace .
  \]
\end{thm}

\begin{proof}
  Encode the bit string $x$ using a Shannon-Fano code $C_\alpha$ with
  $\alpha=(1-\eps)/2$. 
  Then, the length of the codeword for $x$ is
  \[
    |C_\alpha(x)| = n_1(x)\log(1/\alpha) + n_0(x)\log (1/(1-\alpha))
    \enspace .
  \]
  Since $\alpha < 1/2$, we have $\log(1/\alpha) > \log(1/(1-\alpha))$, so
  $|C_\alpha(x)|$ is maximal when $n_1(x)$ is maximal. 
    Thus, if $n_1(x) \le \alpha n$,
  then
  \begin{align*}
    |C_\alpha(x)| & \le \alpha n\log(1/\alpha) + (1-\alpha)n\log(1/(1-\alpha))\\
                  & = n H(\alpha) \le n\left(1-\frac{\eps^2}{2\ln 2}\right) = n - s \enspace ,
  \end{align*}
  where the second inequality is an application of \eqref{entropy-ii},
  and where $s = \eps^2 n/(2 \ln 2)$.  Now, $x$ was chosen uniformly at
  random from a set of size $2^n$. By the Uniform Encoding Lemma, we
  obtain that
  \[
    \Pr\{n_1(x) \leq \alpha n\} \le \Pr\{|C_\alpha(x)| \le n - s\} \le 2^{-s} = e^{-\eps^2n/2} \enspace . \qedhere
  \]
\end{proof}
In \secref{chernoff}, after developing a Non-Uniform Encoding Lemma,
we will extend this argument to $\mathrm{Bernoulli}(\alpha)$ bit
strings. 

\subsection{Factorials and Binomial Coefficients}
\seclabel{stirling}

Before moving on to some more advanced encoding arguments, it will be
helpful to remind the reader of a few inequalities that can be derived
from Stirling's Approximation of the factorial~\cite{robbins:stirling}.  
Recall that Stirling's
Approximation states that
\begin{equation}
  n! = \left(\frac{n}{e}\right)^n\sqrt{2\pi n}\left(1+\varTheta\left(\frac{1}{n}\right)\right) \enspace .
  \eqlabel{stirling}
\end{equation}

In many cases, we are interested in representing a set of size $n!$
using a fixed-length code.  By \eqref{stirling}, and using once again that
$1+x \leq e^x$ for all $x \in \R$, the length of the codewords in such a code
is
\begin{align}
  \log n!
  & = n\log n - n\log e + (1/2)\log n + \log \sqrt{2 \pi} + \log(1+\varTheta(1/n)) \notag \\
  & = n\log n - n\log e + (1/2)\log n + \log \sqrt{2 \pi} + \varTheta(1/n) \notag \\
  & = n\log n - n\log e + \varTheta(\log n) \eqlabel{stirling-loose}
    \enspace .
\end{align}

We are sometimes interested in codes for the $\binom{n}{k}$ subsets of
$k$ elements from a set of size $n$. Note that there is an easy
bijection between such subsets and binary strings of length $n$ with
exactly $k$ ones. Therefore, we can represent these using the
Shannon-Fano code $C_{k/n}$ and each of our codewords will have length
$nH(k/n)$.  In particular, this implies that
\begin{equation}
  \log\binom{n}{k} \le nH(k/n) \le k\log n - k\log k + k\log e 
  \eqlabel{log-n-choose-k}
\end{equation}
where the last inequality is an application of \eqref{entropy-i}. The
astute reader will notice that we just used an encoding argument to
prove an upper-bound on $\binom{n}{k}$ \emph{without knowing the 
formula $\binom{n}{k}=\frac{n!}{k! (n - k)!}$}. Alternatively, we 
could obtain a slightly worse bound by applying 
\eqref{stirling} to this formula.

\subsection{Encoding the Natural Numbers}

So far, we have only explicitly been concerned with codes for finite
sets. In this section, we give an outline of some
prefix-free codes for the set of natural numbers. Of course, if $p :
\N \to [0, 1]$ is a probability density, then the Shannon-Fano code
for $p$ could serve. However, it seems easier to simply design our
codes by hand, rather than find appropriate distributions.

A code is prefix-free if and only if any message consisting of a
sequence of its codewords can be decoded unambiguously and
instantaneously as it is read from left to right: Consider some
sequence of codewords $M = y_1 y_2 \cdots y_k$ from a prefix-free code
$C$. Since $C$ is prefix-free, then $C$ has no codeword $z$ which is a
prefix of $y_1$, so reading $M$ from left to right, the first codeword
of $C$ which we recognize is precisely $y_1$. Continuing in this
manner, we can decode the whole message $M$. Conversely, if for each
codeword $y$ of $C$, a message consisting of this single codeword can
be decoded unambiguously and instantaneously from left to right, we
know that $y$ has no prefix among the codewords of $C$, \emph{i.e.}
$C$ is prefix-free. 
This idea allows us to more easily design
the codes in this section, which were originally given by
Elias~\cite{elias:coding}.

The \emph{unary encoding} of an integer $i \in \N$, denoted by $U(i)$,
begins with $i$ 1 bits which are punctuated with a 0 bit. This code is
not particularly useful in itself, but it can be improved as follows:
The \emph{Elias $\gamma$-code} for $i$, denoted by $E_\gamma(i)$,
begins with the unary encoding of the number of bits in $i$, and then
the binary encoding of $i$ itself (minus its leading bit). The
\emph{Elias $\delta$-code} for $i$, denoted by $E_\delta(i)$ begins
with an Elias $\gamma$-code for the number of bits in $i$, and then
the binary encoding of $i$ itself (minus its leading bit). This
process can be continued recursively to obtain the \emph{Elias
  $\omega$-code}, which we denote by $E_\omega$. Each of these codes
has a decoding procedure as in the preceding paragraph, which
establishes their prefix-freeness.

The most important properties of these codes are their codeword
lengths:
\begin{align*}
  |U(i)| &= i + 1 \enspace , \\
  |E_\gamma(i)| &= 2 \log i + O(1) \enspace , \\
  |E_\delta(i)| &= \log i + 2 \log \log i + O(1) \enspace , \\
  |E_\omega(i)| &= \log i + \log \log i + \dots + \underbrace{\log \cdots \log}_{\text{$\log^* i$ times}}i + O(\log^* i) \enspace .
\end{align*}
It may be worth noting that the lengths of unary codes correspond to
the lengths of Shannon-Fano codes for a geometric distribution with
density $p_i = 1/2^{i + 1}$, that is, 
\[
  \log (1/p_i) = \log 2^{i + 1} = |U(i)| \enspace ,
\]
and the lengths of Elias $\gamma$-codes correspond to the lengths of
Shannon-Fano codes for a discrete Cauchy distribution with density
$p_i = c/i^2$ for a normalization constant $c$, that is,
\[
  \log (1/p_i) = 2 \log i - \log c = |E_\gamma(i)| + O(1) \enspace .
\]
The lengths of Elias $\gamma$-codes and $\omega$-codes do not seem to
arise as the lengths of Shannon-Fano codes for named distributions.

\section{Applications of the Uniform Encoding Lemma}
\seclabel{applications-i}

We now start with some applications of the Uniform Encoding Lemma. In
each case, we will design and analyze a partial prefix-free code
$C\from X\nrightarrow \{0, 1\}^*$, where $X$ depends on
the context.

\subsection{Graphs with no Large Clique or Independent Set}\seclabel{ramsey}

The \emph{Erd\H{o}s-R\'enyi random graph} $G_{n,p}$ is the
probability space on
graphs with vertex set $V=\{1,\ldots,n\}$ in which each edge $\{u,
w\} \in \binom{V}{2}$ is present with probability $p$ and absent with
probability $1-p$, independently of the other edges.  Erd\H{o}s
\cite{erdos:some} used the random graph $G_{n,\frac{1}{2}}$ to prove
that there are graphs that have neither a large clique nor a large 
independent set. Here we show how this can be done using an encoding
argument.

\begin{thm}\thmlabel{erdos-renyi-i}
  For $n \ge 3$ and $s \in \N$, the probability that $G \in G_{n,\frac{1}{2}}$
  contains a clique or an independent set of size $t = \ceil{3\log n +
    \sqrt{2s}}$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that compresses the $\binom{n}{2}$ bits
  of $G$'s adjacency matrix, as they appear in row-major order.
  
  Suppose the graph $G$ contains a clique or an independent set $S$ of size
  $t$. The encoding $C(G)$ begins with a bit indicating whether $S$
  is a clique or independent set; followed by the set of vertices of $S$; 
  then the
  adjacency matrix of $G$ in row major-order, omitting
  the $\binom{t}{2}$ bits implied by the edges or non-edges in
  $S$. Such a codeword has length
  \begin{equation} 
    |C(G)|  = 1 + t\log n + \binom{n}{2}-\binom{t}{2} \enspace .  
    \eqlabel{tst-1}
  \end{equation}
  Before diving into the detailed arithmetic, we intuitively
  argue why we're heading in the right direction: Roughly,
  \eqref{tst-1} is of the form:
  \[ |C(G)|  = \binom{n}{2} + t\log n - \varOmega(t^2) \enspace . \]
  That is, we need to invest $O(t\log n)$ bits
  to encode the vertex set of a clique or an independent set of 
  size $t$, but we save $\varOmega(t^2)$ bits in the encoding of $G$'s 
  adjacency matrix.
  Clearly, for $t>c\log n$, with $c$ sufficiently large, this has the form 
  \[ |C(G)|  = \binom{n}{2} - \varOmega(t^2) \enspace . \]
  At this point, it is just a matter of pinning down the dependence on $c$.
  A detailed calculation beginning from \eqref{tst-1} gives
  \begin{align*}
    |C(G)| 
    & = \binom{n}{2} + 1 + t\log n - (1/2)(t^2 - t) \\
    & = \binom{n}{2} + 1 - (1/2)(t^2 - t - 2t \log n) \enspace .
  \end{align*}
  The function $f(x) = (1/2)(x^2 - x - 2x \log n) - 1$ is increasing
  for $x \geq \log n + 1/2$, so recalling that
$t = \ceil{3\log n + \sqrt{2s}}$, we get
  \begin{align*}
    f(t) &\ge f(3\log n + \sqrt{2s}) \\
    &= (1/2)(9 \log^2 n + 6 \sqrt{2s} \log n + 2s - 3 \log n - \sqrt{2s} - 6 \log^2 n - 2 \sqrt{2s} \log n) - 1 \\
    &= (1/2)(3 \log^2 n + 4 \sqrt{2s} \log n - 3 \log n - \sqrt{2s}) + s - 1 \\
    &\ge s
  \end{align*}
  for $n \ge 3$. Therefore, our code has length
  \begin{align*}
    |C(G)| & = \binom{n}{2} - f(t) \\
    & \le \binom{n}{2} - s \enspace .
  \end{align*}
  Applying the Uniform Encoding Lemma completes the proof.
\end{proof}

\begin{rem}
  The bound in \thmref{erdos-renyi-i} can be strengthened a little,
  since the elements of $S$ can be encoded using only
  $\log\binom{n}{t}$ bits, rather than $t\log n$.  With a more careful
  calculation, using \eqref{log-n-choose-k}, the proof then works with
  $t = 2\log n +O(\log\log n) + \sqrt{s}$. This comes closer to Erdős'
  original result, which was at the threshold $2\log n - 2\log\log n +
  O(1)$~\cite{erdos:some}.
\end{rem}


\subsection{Balls in Urns}
\seclabel{urns}

The random experiment of  throwing $n$ balls uniformly and
independently at random into $n$ urns is a useful abstraction of many
questions encountered in algorithm design, data structures, and 
load-balancing~\cite{mitzenmacher.upfal:probability,motwani.raghavan:randomized}.  Here we
show how an encoding argument can be used to prove the classic result
that, when we do this, no urn contains more than $O(\log n/\log\log
n)$ balls.

\begin{thm}\thmlabel{urns}
  Let $n, s \in \N$, and let $t$ be such that $t\log(t/e) \ge \log n + s$.
  Suppose we
  throw $n$ balls independently and uniformly at random into $n$
  urns. Then, for sufficiently large $n$, the probability that any urn
  contains more than $t$ balls is at most $2^{-s}$.
\end{thm}

Before proving \thmref{urns}, we note that, for any constant $\eps >0$
and all sufficiently large $n$, taking
\[
  t = \Ceil{\frac{(1+\eps)\log n}{\log\log n}}
\] 
satisfies the requirements of \thmref{urns},  since then
\begin{align*}
  t \log t &\ge \frac{(1 + \eps) \log n}{\log \log n} \log \left( \frac{(1 + \eps) \log n}{\log \log n}\right) \\
  &= (1 + \eps) \log n \frac{\log \log n}{\log \log n} - (1 + \eps) \log n \frac{\log \left(\frac{\log \log n}{1 + \eps}\right)}{\log \log n} \\
  &\geq \log n + \eps \log n - (\eps/2) \log n \\
  &= \log n + (\eps/2) \log n
\end{align*}
for 
a sufficiently large choice of $n$. Then,
\[
  t \log (t/e) = t \log t - t \log e \ge \log n + (\eps/2) \log n 
  - o(\log n) \ge \log n + s \enspace ,
\]
for sufficiently large $n$, as claimed.

\begin{proof}[Proof of \thmref{urns}]
  For each $i\in\{1,\ldots,n\}$, let $b_i$ denote the index of the urn
  chosen for the $i$-th ball. The sequence $b = (b_1,\ldots,b_n)$ is
  sampled uniformly at random from a set of size $n^n$, and this
  choice will be used in our encoding argument.

  Suppose that urn $j$ contains $t$ or more balls. Then,
  we encode the sequence $b$
  with the value $j$, followed by a code that describes $t$ of
  the balls in urn $j$, followed by the remaining $n-t$ values in
  $b$ that cannot be deduced from the preceding
  information.  Thus, we get
  \begin{align*}
    |C(b)| &= \log n + \log\binom{n}{t} + (n-t)\log n \\
           &= \log n + t\log n - t\log t + t\log e + (n-t)\log n
             \tag{using \eqref{log-n-choose-k}} \\
           & = n \log n + \log n - t\log t + t\log e \\
           & \le n \log n - s \tag{by the choice of $t$} \\
           & = \log n^n - s
  \end{align*}
  bits. We conclude the proof by applying the Uniform Encoding Lemma.
\end{proof}

\subsection{Linear Probing}

Studying balls in urns as in the previous section is useful when 
analyzing hashing with chaining
(see \emph{e.g.}~\cite[Section~5.1]{morin:open}). A more practically
efficient form of hashing is \emph{linear probing}.  In a linear
probing hash table, we hash the elements of the set
$X = \{x_1, \ldots, x_n\}$ into a hash table of size $m=cn$, for some
fixed $c> 1$. We are given a hash function
$h : X \to \{1, \ldots, m\}$ which we assume to be a uniform random
variable. To insert $x_i$, we try to place it at table position
$j=h(x_i)$. If this position is already occupied by one of
$x_1,\ldots,x_{i-1}$, we try table location
$(j+1)\bmod m$, followed by $(j+2)\bmod m$, and so on, until 
we find an empty spot
for $x_i$.  
To find a given element $x \in X$ in the hash table, we compute
$j = h(x)$, and we start a linear search from position $j$ 
until we encounter either $x$ or the first empty position.
Assuming that the hash table has been created by inserting
the elements from $X$ successively according to the
algorithm above, 
we want to study the expected search time for some item
$x \in X$.

We call a maximal consecutive sequence of occupied table locations a
\emph{block}. (The table locations $m-1$ and $0$ are considered
consecutive.)

\wolfgang{I rephrased the theorem for block size \emph{exactly}
$t$. I think otherwise the encoding argument would not be correct
as written,
because you could not guarantee that the hash values of the elements
in the block can be represented with $\log t$ bits each.}
\begin{thm}\thmlabel{linear-probing}
  Let $n \in \N$, $c > e$.
  Suppose that a set $X = \{x_1, \dots, x_n\}$ of $n$ items 
  has been inserted into a hash table of size $m = cn$, using linear 
  probing.
  Let $t \in \N$ such that
  \[
    t \log (c/e) - \log t \ge s + O(1) \enspace ,
  \]
  and fix some $x\in X$. 
  Then the probability that the block containing
  $x$ has size $t$
  is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument for 
  the sequence
  \[
    h = (h(x_1),h(x_2),\ldots,h(x_n)) \enspace ,
  \]
  that is drawn uniformly at random from a set of size
  $m^n = (cn)^n$.
  
  Suppose that $x$ lies in a block with $t$ elements.
  We encode $h$ by the first index $b$ of the block containing $x$;
  followed by the $t-1$ elements $y_1,\dots,y_{t-1}$ of this block
  (excluding $x$); followed by information to decode the hash
  values of $x$ and of $y_1,\dots,y_{t-1}$; followed by
  the $n-t$ hash values for the remaining elements in $X$.

  Since the values $h(x),h(y_1),h(y_2),\ldots,h(y_{t-1})$ are in the range
  $b,\ldots,b+t-1$ (modulo $m$), they can be encoded using
  $t\log t$ bits.  Therefore, 
  we obtain a codeword of length 
  \begin{align*}
    |C(h)| & = \overbrace{\log m}^{b} + \overbrace{\log\binom{n}{t-1}}^{y_1,\ldots,y_{t-1}} + \overbrace{t\log t}^{h(x),h(y_1),\ldots,h(y_{t-1})} + \overbrace{(n-t)\log m}^{\text{everything else}} \\
           & \le \log m + (t-1)\log n - 
             (t-1)\log(t-1) + (t-1)\log e + t\log t + (n-t)\log m \tag{by \eqref{log-n-choose-k}}\\
	  &=
         (n-t+1)\log m + (t-1)\log (m/c) + (t-1)\log e + \log (t-1)+ t\log(t/(t-1)) \tag{$m = cn$}\\
           & = n\log m - (t-1)\log c + (t-1)\log e + \log t + O(1) \\
           & = \log m^n - (t-1)\log(c/e) + \log t + O(1) \\
           & \le \log m^n - s \enspace ,
  \end{align*}
  since we assumed that $t$ satisfies
  \[
    t \log (c/e) - \log t \ge s + O(1) \enspace .
  \]
  The proof is complete by applying the Uniform Encoding Lemma.
\end{proof}

\begin{rem}
  The proof of \thmref{linear-probing} only works if the factor
  $c$ in the size $m = cn$ of the linear probing hash table is 
  $c > e$.
  We know from
  previous analysis that this is not necessary, and that any $c>1$ is
  sufficient~\cite[Theorem~9.8]{flajolet.sedgewick:aofa}. 
  We leave it as an open problem to find an encoding proof
  of \thmref{linear-probing} that works for any $c>1$.
\end{rem}

\wolfgang{I rephrased the corollary. I think the statement should
be that for any \emph{fixed} $x$, the search time is $O(1)$.
In contrast, there always exists \emph{some} $x$ such that
the search time for $x$ is $\Omega(\log n)$, no? 
If I am wrong, please correct me.}
\begin{cor}
  Let $n \in \N$, $c > e$.
  Suppose that a set $X = \{x_1, \dots, x_n\}$ of $n$ items 
  has been inserted into a hash table of size $m = cn$, using linear 
  probing.
  Fix some $x\in X$. 
  Then, the expected search time for $x$ in the hash table is
  $O(1)$.
\end{cor}
\begin{proof}
  Let $T$ denote the size of the block containing $x$ in the
  hash table. Let $t_0$ be a large enough constant. 
  Then, by \thmref{linear-probing}, the probability
  that $T = t + t_0$ is at most $2^{-t\log(c/e)/2}$,
  since then
  \[(t  + t_0)\log(c/e) - \log(t+t_0)
  \geq
(t  + t_0)\log(c/e)/2
    \geq t\log(c/e)/2 + O(1) \enspace. 
    \]
  Thus, the expected search time for $x$ is
  \begin{align*}
   E\{T\} &= \sum_{t = 1}^\infty t\Pr\{T = t\} = \sum_{t=1}^{t_0} t\Pr\{T = t\} + \sum_{t = 1}^\infty (t+t_0)\Pr\{T = t + t_0\} \\
            &\le t_0^2 + \sum_{t = 1}^\infty (t+t_0)2^{-t \log (c/e)/2} = t_0^2 + \sum_{t = 1}^\infty (t+t_0)(c/e)^{-t/2} = O(1) \enspace . \qedhere
  \end{align*}
\end{proof}

\subsection{Cuckoo Hashing}

Cuckoo hashing is relatively new hashing scheme that offers an
alternative to classic perfect hashing \cite{pagh.rodler:cuckoo}. 
We present a clever proof, due to Mihai Pătraşcu, that cuckoo hashing
succeeds with probability $1-O(1/n)$ \cite{patrascu:cuckoo}
(see also \cite{haimberger:cuckoo} for a detailed exposition of the 
argument).

We again hash the elements of the set $X = \{x_1, \ldots, x_n\}$. The
hash table consists of two arrays $A$ and $B$, each of size $m = 2n$, and
two hash functions $h, g : X \to \{1, \ldots, m\}$ which are uniform
random variables. To insert an element $x$ into the hash table, we
insert it into $A[h(x)]$; if $A[h(x)]$ already contains an element
$y$, we insert $y$ into $B[g(y)]$; if $B[g(y)]$ already contains some
element $z$, we insert $z$ into $A[h(z)]$, etc. If an empty location
is eventually found, the algorithm terminates successfully. If the
algorithm runs for too long without successfully completing the
insertion, then we say that the insertion failed, and the hash table
is rebuilt using different newly sampled hash functions. Any element
$x$ either is held in $A[h(x)]$ or $B[g(x)]$, so we can search for $x$
in constant time.  The following
pseudocode describes this procedure more precisely:

\noindent{$\textsc{Insert}(x):$}
\begin{algorithmic}[1]
  \IF{$x = A[h(x)]$ \OR $x = B[g(x)]$}
    \RETURN
  \ENDIF

  \FOR{MaxLoop iterations}
    \IF{$A[h(x)]$ is empty}
      \STATE{$A[h(x)] \gets x$} 
      \RETURN
    \ENDIF
    \STATE{$x \leftrightarrow A[h(x)]$}
    \IF{$B[g(x)]$ is empty}
      \STATE{$B[g(x)] \gets x$}
      \RETURN
    \ENDIF
    \STATE{$x \leftrightarrow B[g(x)]$}
  \ENDFOR
  \STATE{$\textsc{Rehash}()$}
  \STATE{$\textsc{Insert}(x)$}
\end{algorithmic}

The threshold `MaxLoop' is to be specified later. To study the
performance of insertion in cuckoo hashing, we consider the random
bipartite \emph{cuckoo graph} $G = (A, B, E)$, where $|A| = |B| = m$
and $|E| = n$, with each vertex corresponding either to a location in
the array $A$ or $B$ above, and with edge multiset
$E = \{(h(x_i), g(x_i)) : 1 \leq i \leq n\}$.

An \emph{edge-simple} path is a path that uses each edge at most once. 
One can check that if a successful insertion 
takes at least $2t$ steps, then the cuckoo
graph contains an edge simple path with at least $t$ edges.
Thus, in bounding the length of edge-simple
paths in the cuckoo graph, we bound the worst case insertion time.

\begin{lem}\lemlabel{cuckoo-path-length}
  Let $s \in \N$. Suppose that we insert a set $X = \{x_1, \dots, x_n\}$
  into a hash table using cuckoo hashing.
  Let $G$ be the resulting cuckoo graph. Then, 
  $G$ has an edge-simple path of length at least
  $s + \log n + O(1)$ with probability at most $2^{-s}$.
\end{lem}
\begin{proof}
  We encode $G$ by presenting its set of edges. Since 
  each endpoint of
  an edge is chosen independently and uniformly at random from a set
  of size $m$, the set of all edges is chosen uniformly at random from
  a set of size $m^{2n}$.

  Suppose some vertex $v \in A \cup B$ is the endpoint of an 
  edge-simple path
  of length $t$; such a path has $t + 1$ vertices and $t$ edges. 
  Each edge in the path corresponds to an element in $X$.
  In
  the encoding, we present the indices of the elements in $X$ corresponding
  to the $t$ edges of the path 
  in order; then,
  we indicate whether $v \in A$ or $v \in B$; and we give the $t + 1$
  vertices in order starting from $v$; followed by the remaining
  $2n - 2t$ endpoints of edges of the graph. This code has length
  \begin{align*}
    |C(G)| &= t \log n + 1 + (t + 1) \log m + (2n - 2t) \log m\\
           &= 2n \log m + t \log n - t \log m + \log m + O(1) \\
           &= \log m^{2n} - t + \log n + O(1) \tag{since $m = 2n$} \\
           &\leq \log m^{2n} - s
  \end{align*}
  for $t \geq s + \log n + O(1)$. We finish by applying the Uniform
  Encoding Lemma.
\end{proof}
This immediately implies that a successful insertion takes time at
most $4 \log n + O(1)$ with probability $1 - O(1/n)$. Moreover,
selecting `MaxLoop' to be $4\log n + O(1)$, we see that a rehash
happens only with probability $O(1/n)$.

One can prove that
the cuckoo hashing insertion algorithm fails if and only if  
some subgraph of
the cuckoo graph contains more edges than vertices, since edges
correspond to keys, and vertices correspond to array locations.
\begin{lem}\lemlabel{cuckoo-failure}
  The cuckoo graph has a subgraph with more edges than vertices with
  probability $O(1/n)$. In other words, cuckoo hashing insertion
  succeeds with probability $1 - O(1/n)$.
\end{lem}
\begin{proof}
  Suppose that some vertex $v$ is part of a subgraph with more edges
  than vertices, and in particular a minimal such subgraph with $t +
  1$ edges and $t$ vertices. Such a subgraph appears exactly as in
  \figref{cuckoo-cycles}. By inspection, we see that for every such 
  subgraph, there are two edges
  $e_1$ and $e_2$ whose removal disconnects the graph into two paths
  of length $t_1$ and $t_2$ starting from $v$, where $t_1 + t_2 = t -
  1$.

  \begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
      \includegraphics{cuckoo1}
      \caption{A cycle with a chord.}
    \end{subfigure}
    \quad\quad
    \begin{subfigure}[b]{0.6\textwidth}
      \includegraphics{cuckoo2}
      \caption{Two cycles connected by a path.}
    \end{subfigure}
    \caption{The potential minimal subgraphs of the cuckoo graph.}
    \figlabel{cuckoo-cycles}
  \end{figure}

  We encode $G$ by giving the vertex $v$; and presenting Elias
  $\delta$-codes for the values of $t_1$ and $t_2$ and the
  positions of the endpoints of $e_1$ and $e_2$; then the indices
  of the edges of
  the above paths in order; then the vertices of the paths in order;
  and the indices of the edges $e_1$ and $e_2$; and finally the 
  remaining $2n - 2(t +
  1)$ endpoints of edges in the graph. Such a code has length
  \begin{align*}
    |C(G)| &= \log m + O(\log t) + (t - 1)(\log n + \log m) + 2\log n + (2n - 2(t + 1))\log m \\
           &= 2n \log m + (t + 1) \log n - (t + 2) \log m + O(\log t) \\
           &= 2n \log m + (t + 1) \log n - (t + 2) \log n - t + O(\log t) \tag{since $m = 2n$} \\
           &\le \log m^{2n} - \log n + O(1) \enspace .
  \end{align*}
  We finish by applying the Uniform Encoding Lemma.
\end{proof}

\subsection{2-Choice Hashing}

We showed in \secref{urns} that if $n$ balls are thrown independently
and uniformly at random into $n$ urns, then the maximum number of
balls in any urn is $O(\log n/\log \log n)$ with high probability. In
\emph{2-choice hashing}, each ball is instead given a choice of two
urns, and the urn containing the fewer balls is preferred.

More specifically, we are given two hash functions $h, g : X \to \{1,
\ldots, m\}$ which are uniform random variables. Each value of $h$ and
$g$ points to one of $m$ urns. The element $x \in X$ is added to the
urn containing the fewest elements between $h(x)$ and $g(x)$ during an
insertion. The worst case search time is at most the maximum number of
hashing collisions, or the maximum number of elements in any urn.

Perhaps surprisingly, the simple change of having two choices instead
of only one results in an exponential improvement over the strategy of
\secref{urns}. The concept of 2-choice hashing was first studied by 
Azar \emph{et
  al.}~\cite{azar:multiplechoice}, who showed that the expected
maximum size of an urn is $\log \log n + O(1)$. Our encoding argument
is based on V\"{o}cking's use of witness trees to analyze 2-choice
hashing~\cite{vocking:witness}.

Let $G = (V, E)$ be the random multigraph with $V = \{1, \ldots, m\}$,
where $m = cn$ for some constant $c > 4$, and
$E = \{(h(x), g(x)) : x \in X\}$.

\begin{lem}\lemlabel{two-choice-two-cycles}
  The probability that $G$ has a subgraph with more edges than 
  vertices is
  $O(1/n)$.
\end{lem}
\begin{proof}
  The proof is similar to that of \lemref{cuckoo-failure}. More
  specifically, we can encode $G$ by giving the same encoding as in
  \lemref{cuckoo-failure}, with an additional bit for each edge $uv$
  in the encoding, indicating whether $u = h(x)$ and $v = g(x)$, or $u
  = g(x)$ and $v = h(x)$. Our code thus has length
  \begin{align*}
    |C(G)| &= \log n + (t - 1)(\log n + \log m) + 2 \log n + (2n - 2(t + 1))\log m + t + O(\log t) \\
           &= 2n \log m - \log n - t \log c + t + O(\log t) \\
           &\le \log m^{2n} - \log n + O(1) \enspace ,
  \end{align*}
  since $\log c > 1$.
\end{proof}

\begin{lem}\lemlabel{two-choice-component-size}
  $G$ has a component of size at least $(2/\log(c/4))\log n + O(1)$
  with probability $O(1/n)$.
\end{lem}
\begin{proof}
  Suppose $G$ has a connected component with $t - 1$ edges and at most
  $t$ vertices. This component has a spanning tree $T$. Pick an
  arbitrary vertex as the root of $T$. To encode $G$, we specify
  first the $t - 1$ edges and $t$ vertices encountered in a pre-order
  traversal of $T$; then a bit string encoding the shape of $T$; and
  finally the remaining $2(n - t + 1)$ endpoints of edges in $G$.

  We can encode the shape of $T$ with a bit string of length
  $2(t - 1)$, tracing a pre-order traversal of the tree, where a 0 bit
  indicates that the path to the next node goes up, and a 1 bit indicates
  that the path goes down. In total, our code has length
  \begin{align*}
    |C(G)| &= t \log m + (t - 1) \log n + 2(t - 1) + 2 (n - t + 1) \log m \\
           &= 2n\log m + t \log c - \log n + 2t - 2 - 2t \log c + 2\log n + 2 \log c \tag{since $m = cn$}\\
           &= \log m^{2n} - t \log c + 2t + \log n + O(1) \\
           &\le \log m^{2n} - s \enspace ,
  \end{align*}
  as long as $t$ is such that
  \[
    t \geq \frac{s + \log n + O(1)}{\log (c/4)} \enspace .
  \]
  In particular, for $s = \log n$, the Uniform
  Encoding Lemma tells us that $G$ has a component of size at least
  $(2/\log (c/4))\log n + O(1)$ with probability $O(1/n)$.
\end{proof}

Suppose that when $x$ is inserted, it is placed in an urn with $t$
other elements. Then, we say that the \emph{age} of $x$ is $a(x) = t$.
\begin{thm}
  The cost of searching in 2-choice hashing in a table of size $cn$
  with $c > 4$ is at most $\log \log n + O(1)$ with probability $1 -
  O(1/n)$.
\end{thm}
\begin{proof}
  Suppose that some element $x$ has $a(x) = t$. This leads to a binary
  \emph{witness tree} $T$ of height $t$ as follows: The root of $T$ is
  the element $x$. When $x$ was inserted into the hash table, it had
  to choose between the urns $h(x)$ and $g(x)$, both of which
  contained at least $t - 1$ elements; in particular, $h(x)$ has an
  element $x_h$ with $a(x_h) = t - 1$, and $g(x)$ has an element $x_g$
  with $a(x_g) = t - 1$. The elements $x_h$ and $x_g$ become left and
  right children of $x$ in $T$. The process continues recursively. If
  some element appears more than once on a level, we only recurse on
  its leftmost occurrence \wolfgang{Figure?}. 
  Note that each vertex of $T$ corresponds to
  an edge $(h(x), g(x))$ of $G$. Moreover, the subgraph of $G$
  induced by the edges
  \[
    \{(h(x), g(x)) : x \text{ is a vertex of } T\}
  \]
  is connected.

  Suppose that some node appears more than once on a level of $T$. By
  the discussion above \wolfgang{Which discussion? Figure?}, this
  means that some component of $G$ has a cycle. If this happens twice,
  then $G$ has a subgraph with more edges than vertices. By
  \lemref{two-choice-two-cycles}, this happens with probability
  $O(1/n)$. If instead this happens at most once, then $T$ is a
  complete binary tree with at most one subtree removed \wolfgang{This
    is a bit vague, maybe: "is a complete binary tree with at most one
    subtree removed"?} \tommy{Okay}, so $T$ has at least $2^t$
  nodes. If we choose $t = \ceil{\log \log n + d}$, then $T$ has at
  least $2^d \log n$ nodes, which we know from
  \lemref{two-choice-component-size} happens with probability $O(1/n)$
  for a sufficiently large choice of the constant $d$.
\end{proof}

\begin{rem}
  The arguments in this section can be refined by more carefully
  encoding the shape of trees using Cayley's formula, which says that
  there are $t^{t - 2}$ rooted labelled trees on $t$
  nodes~\cite{cayley:theorem}. In particular, a rooted tree with $t$
  nodes and $m$ choices for distinct node labels can be encoded using
  $\log \binom{m}{t} + (t - 2) \log t$ bits instead of
  $t \log m + 2(t - 1)$ bits. We would then recover the same results
  for hash tables of size $m = cn$ for $c > e$ instead of $c > 4$. In
  fact, it \wolfgang{reference?}\tommy{Okay} is known that it suffices
  that $c > 0$ for searching to take time $1/c + O(\log \log n)$ in
  2-choice hashing~\cite{berenbrink:densehashing}. We leave it as an
  open problem to find an encoding argument for this result when
  $c > 0$.
\end{rem}

\begin{rem}
  Robin Hood hashing is another hashing solution which achieves
  $O(\log \log n)$ worst case running time for all
  operations~\cite{devroye:robin}. The original analysis is difficult,
  but might be amenable to a similar kind of analysis as we used in
  this section. Indeed, when a Robin Hood hashing operation takes a
  significant amount of time, a large witness tree is again implied,
  which suggests an easy encoding argument. Unfortunately, this
  approach appears to involve unwieldy hypergraph encoding.
  \wolfgang{so, the problem is what?}
\end{rem}


\subsection{Bipartite Expanders}

Expanders are families of sparse graphs which share many connectivity
properties with complete graphs.  \wolfgang{This description seems to
  undersell expanders. Maybe: "...of sparse graphs with share many
  connectivity properties with complete graphs"}\tommy{Got it.}  These
graphs have received much research attention, and have led to many
applications in computer science. See, for instance, the survey by
Hoory, Linial, and Wigderson~\cite{hoory.linial.ea:expander}.

\wolfgang{maybe a sentence like: "the existence of expanders was
  originally established through probabilistic
  arguments."}\tommy{Okay} The existence of expanders was originally
established through probabilistic arguments~\cite{pinsker:expanders}.
We offer an encoding argument to prove that a certain random bipartite
graph is an expander with high probability. There are many different
notions of expansion. We will consider what is commonly known as
\emph{vertex expansion} in bipartite graphs: For some fixed
$0 < \alpha \leq 1$, a bipartite graph $G = (A, B, E)$ is called a
\emph{$(c, \alpha)$-expander} if
\begin{align*}
  \min_{\substack{{A' \subseteq A}\\{|A'| \leq \alpha |A|}}} \frac{|N(A')|}{|A'|} \geq c \enspace ,
\end{align*}
where $N(A') \subseteq B$ is the set of neighbours of $A'$ in $G$.
\wolfgang{say it in words, figure?}

Let $G = (A, B, E)$ be a random bipartite multigraph where
$|A| = |B| = n$ and where each vertex of $A$ is connected to three
vertices of $B$ chosen independently and uniformly at random (with
replacement). The following theorem shows that $G$ is an expander.
The proof of this theorem usually involves a messy sum that contains
binomial coefficients and probabilities: see, for example, Motwani and
Raghavan~\cite[Theorem~5.3]{motwani.raghavan:randomized},
Pinsker~\cite[Lemma~1]{pinsker:expanders}, or Hoory, Linial, and
Wigderson~\cite[Lemma~1.9]{hoory.linial.ea:expander}.

\begin{thm}
  There exists a constant $\alpha >0$ such that $G$ is a
  $(3/2,\alpha)$-expander with probability at least $1 - O(n^{-1/2})$.
\end{thm}

\begin{proof}
  We encode the graph $G$ by presenting its edge set. Since each edge
  is selected uniformly at random, the graph $G$ is chosen uniformly at
  random from a set of size $n^{3n}$.
  
  If $G$ is not a $(3/2, \alpha)$-expander, then there is some set $A'
  \subseteq A$ with $|A'|=k\le \alpha n$ and
  \[
  \frac{|N(A')|}{|A'|} < 3/2 \enspace .
  \]
  To encode $G$, we first give $k$ using an Elias $\gamma$-code; together 
  with the sets $A'$ and $N(A')$; and the edges between $A'$ and
  $N(A')$. Then we encode the rest of $G$, skipping the $3k\log n$
  bits devoted to elements in $A'$.  The key savings here come because
  $N(A')$ should take $3k\log n$ bits to encode, but can actually be
  encoded in roughly $3k\log(3k/2)$ bits. Our code then has length
  \begin{align*}
    |C(G)| &= 2\log k + \log {n \choose k} + \log {n \choose 3k/2} + 3k \log (3k/2) + (3n - 3k) \log n + O(1) \\
           &\le 2\log k + k\log n - k\log k + k\log e + (3k/2)\log n - (3k/2) \log (3k/2) \\
           & \hphantom{{}=2\log k} + (3k/2) \log e + 3k \log (3k/2) + (3n - 3k) \log n + O(1) \tag{by \eqref{log-n-choose-k}} \\
           &= 3n\log n - (k/2)\log n + (k/2)\log k + \beta k + 2\log k + O(1) \\
           &= \log n^{3n} - s(k)
  \end{align*}
  bits, where $\beta = (3/2) \log (3/2) + (5/2) \log e$. Since
  \[
    \frac{d^2}{dk^2} s(k) = \frac{4 - k}{2 k^2} \log e \enspace ,
  \]
  the function $s(k)$ is concave for all $k \geq 4$. Thus, $s(k)$ is
  minimized either when $k = 1, 2, 3, 4$, or $k = \alpha n$. We have
  \begin{align*}
    s(1) &= (1/2)\log n + c_1 \enspace , & 
    s(2) &= \log n + c_2 \enspace , \\
    s(3) &= (3/2) \log n + c_3 \enspace , &
    s(4) &= 2 \log n + c_4 \enspace ,
  \end{align*}
  for constants $c_1, c_2, c_3, c_4$. For $k=\alpha n$ we have
  \[
    s(\alpha n) = (\alpha n/2)\log \left(\frac{1}{2^{2\beta}
        \alpha}\right) - 2 \log \alpha n
  \]
  and $2^{-s(\alpha n)} = 2^{-\varOmega(n)}$ for
  $\alpha < (1/2)^{2\beta} \approx 0.002$. In each case, the Uniform
  Encoding Lemma gives the desired result, and in particular,
  $2^{-s(k)} \leq O(n^{-1/2})$.
\end{proof}

\subsection{Permutations and Binary Search Trees}

We define a \emph{permutation} $\sigma$ of size $n$ to be a sequence
of $n$ pairwise distinct integers, sometimes denoted by
$\sigma = (\sigma_1, \dots, \sigma_n)$. This slightly unusual
definition will serve us for the purpose of encoding. Except when
explicitly stated, we will assume that the set of integers involved in
a permutation of size $n$ is precisely $\{1, \dots, n\}$. The number
of such permutations is $n!$.

\subsubsection{Analysis of Insertion Sort}
\seclabel{insertion-sort}

Recall the insertion-sort algorithm for sorting a list
$\sigma = (\sigma_1,\ldots,\sigma_n)$ of $n$ elements:

\noindent{$\textsc{InsertionSort}(\sigma)$}
\begin{algorithmic}[1]
  \FOR{$i\gets 2$ \TO $n$}
     \STATE{$j \gets i$}
     \WHILE{$j>1$ \AND $x_{j-1} > x_j$}
         \STATE{$\sigma_j \leftrightarrow \sigma_{j-1}$
            \COMMENT{ swap }}
         \STATE{$j\gets j-1$}
     \ENDWHILE
  \ENDFOR
\end{algorithmic}

A typical task in the average-case analysis of algorithms is to
determine the number of times Line~4 executes if $\sigma$ is a
uniformly random permutation of size $n$.  The answer
$\binom{n}{2}/2$ is an easy application of linearity of expectation:
For every one of the $\binom{n}{2}$ pairs $p,q\in\{1,\ldots,n\}$ with
$p<q$, the values initially stored at positions $\sigma_p$ and
$\sigma_q$ will eventually be swapped if and only if
$\sigma_p > \sigma_q$, which happens with probability $1/2$ in a
uniformly random permutation. Such a pair is called an
\emph{inversion}, so the number of times Line~4 executes is the number
of inversions of $\sigma$.

A more advanced question is to ask for a concentration result on the
number of inversions. This is a harder question to tackle; because $>$
is transitive, the $\binom{n}{2}$ events being studied have a lot of
interdependence. In the following, we show how to obtain a
concentration result with an encoding argument.  The argument
presented here follows the same outline as Vit\'{a}nyi's analysis of
bubble sort \cite{vitanyi:analysis}, though without all the trappings
of Kolmogorov complexity.

\begin{thm}\thmlabel{insertion-sort}
  A uniformly random permutation $\sigma$ of size $n$ has at most
  $\alpha n^2 - n + 2$ inversions with probability at most
  $2^{n \log(\alpha e^2) + O(\log n)}$. In particular, for a fixed
  $\alpha < 1/e^2$, this probability is $2^{-\varOmega(n)}$.
  %
  %For a uniformly random permutation $\sigma$ of size $n$, the
  %probability that Line~4 of $\textsc{InsertionSort}(\sigma)$ executes
  %fewer than $\alpha n^2 - n + 2$ times is at most $2^{n\log(\alpha
  %  e^2)+O(\log n)}$ \wolfgang{This is a bit hard to parse}.  
  %  In particular, for a fixed $\alpha < 1/e^2$,
  %this probability is $2^{-\Omega(n)}$.
\end{thm}

\begin{proof}
  We encode the permutation $\sigma$ by recording the execution of
  \textsc{InsertionSort} on $\sigma$. In particular, we record for
  each $i\in\{2,\ldots,n\}$, the number of times $m_i$ that Line~4
  executes during the $i$-th iteration of
  $\textsc{InsertionSort}(\sigma)$. With this information, one can run
  the following algorithm to recover $\sigma$ \wolfgang{More
    explanation here? I would have expected $i$ to go from $n$ downto
    $2$ and $j$ to go up, corresponding to a reverse execution of
    insertion sort. For the pseudocode below, correctness is not
    obvious to me (but maybe I should try some examples by hand)}
  \tommy{You're right: regardless of whether or not the other
    algorithm was correct, this one makes more sense}:
  
  \noindent{$\textsc{InsertionSortReconstruct}(m_2, \dots, m_n)$}:
  \begin{algorithmic}[1]
    \STATE{$\sigma \gets (1, \dots, n)$}
    \FOR{$i \gets n \textbf{ down to } 2$}
      \FOR{$j \gets i - m_i + 1$ \TO $i$}
        \STATE{$\sigma_j \leftarrow \sigma_{j-1}$
          \COMMENT{ swap }}
      \ENDFOR
    \ENDFOR
    \RETURN{$\sigma$}
  \end{algorithmic}

  %\noindent{$\textsc{InsertionSortReconstruct}(m_2,\ldots,m_n)$}:
  %\begin{algorithmic}[1]
  %  \STATE{$\sigma \gets (1,\ldots,n)$}
  %  \FOR{$i\gets 2$ \TO $n$}
  %     \FOR{$j\gets i \textbf{ down to } i-m_i+1$}
  %         \STATE{$\sigma_j \leftrightarrow \sigma_{j-1}$
  %            \COMMENT{ swap }}
  %     \ENDFOR
  %  \ENDFOR
  %  \RETURN{$\sigma$}
  %\end{algorithmic}
   
  To make this work, we have to be slightly clever with this
  encoding. Rather than encode $m_2,m_3,\ldots,m_n$ directly, we first
  encode $m=\sum_{i=2}^{n} m_i$ using $2\log n$ bits (since
  $m < n^2$). Given $m$, what remains is to describe the partition of
  $m$ into $n-1$ non-negative integers $m_2,\ldots,m_n$; there are
  $\binom{m+n-2}{n-2}$ such partitions.\footnote{To see this, draw
    $m+n-2$ white dots on a line, then choose $n-2$ dots to colour
    black. This splits the remaining $m$ white dots up into $n-1$
    groups, which determine the values of $m_2,\ldots,m_n$.}
  
  Therefore, the values of $m_2,\ldots,m_n$ can be encoded using
  \[
    |C(\sigma)| = 2\log n + \log\binom{m+n-2}{n-2}
  \]
  bits and this is sufficient to recover the permutation $\sigma$.  By
  applying \eqref{log-n-choose-k}, we obtain
  \begin{align*}
    |C(\sigma)| & \le (n-2)\log(m+n-2) - (n-2)\log(n-2)  + (n-2)\log e + O(\log n) \\
      & \le n\log(m+n-2) - n\log n   + n\log e + O(\log n) \\
      & \le n\log(\alpha n^2) - n\log n  + n\log e + O(\log n) \tag{since $m \le \alpha n^2 - n + 2$}\\
      & = 2n\log n + n\log\alpha - n\log n  + n\log e + O(\log n) \\
      & = n\log n + n\log\alpha + n\log e + O(\log n) \\
      & = \log n! + n\log\alpha + 2n\log e + O(\log n) \tag{by \eqref{stirling-loose}} \\
      & = \log n! + n \log (\alpha e^2) + O(\log n) \enspace .
  \end{align*}
  Again, we finish by applying the Uniform Encoding Lemma.
\end{proof}


\begin{rem}
  \thmref{insertion-sort} is not sharp; it only gives a non-trivial
  probability when $\alpha < 1/e^2$.  To obtain a sharp bound, one can
  use the fact that $m_2,\ldots,m_n$ are independent and $m_i$ is
  uniform over $\{0,\ldots,i-1\}$ and then use the method of bounded
  differences \cite{mcdiarmid:on} to show that $m$ is concentrated in
  an interval of size $O(n^{3/2})$.
\end{rem}

\subsubsection{Records}

A \emph{(max) record} \wolfgang{I know it as a "peak"}\tommy{From what
  I can quickly see, the usual definition of peak doesn't agree with
  our definition of max records.}  in a permutation $\sigma$ of size
$n$ is some value $\sigma_i$ with $1 \leq i \leq n$ such that
\[
  \sigma_i = \max\{\sigma_1, \dots, \sigma_i\} \enspace .
\]
It is easy to see that the probability that $\sigma_i$ is a record is
exactly $1/i$ if $\sigma$ is chosen uniformly at random, in which case
the expected number of records in such a permutation is
\[
  H_n = \sum_{i = 1}^n 1/i = \ln n + O(1) \enspace ,
\]
the $n$-th harmonic number. It is harder to establish concentration
with non-negligible probability. To do this, one first needs to show
the independence of certain random variables, which quickly becomes
tedious. We instead give an encoding argument to show concentration of
the number of records, inspired by a technique used by Lucier \emph{et
  al.} to study the height of random binary search trees
\cite{lucier.jiang.li:quicksort}.

First, we describe an incremental and recursive manner of encoding a
permutation $\sigma$ of size $n$: Begin by providing the first value
of the permutation $\sigma_1$; followed by the set of indices of the
permutation for which $\sigma$ takes on a value strictly smaller than
$\sigma_1$, including an explicit encoding of the induced permutation
on the elements at those indices; and a recursive encoding of the
permutation induced on the elements strictly larger than
$\sigma_1$. The number of recursive invocations is equal to the number
of records in $\sigma$.

If $\sigma$ contains $k$ elements strictly smaller than $\sigma_1$,
then the length $\ell(n)$ of the codeword for $\sigma$ satisfies the
recurrence
\[
  \ell(n) = \log n + \log \binom{n - 1}{k} + \log k! + \ell(n - k - 1) \enspace ,
\]
with $\ell(0) = 0$ and $\ell(1) = 0$. This solves to $\ell(n) = \log
n!$, so the encoding described above is no better than a fixed-length
encoding for $\sigma$. However, we can readily modify this encoding to
obtain a result about the concentration of records in a uniformly
random permutation.

\begin{thm}\thmlabel{records}
  A uniformly random permutation $\sigma$ of size $n$ has at least $c
  \log n$ records with probability at most
  \[
    2^{-c (1 - H(1/c)) \log n + O(\log \log n)}
  \]
  for $c > 2$.
\end{thm}
\begin{proof}
  We encode the permutation $\sigma$ chosen uniformly at random from a
  set of size $n!$.

  Suppose that $\sigma$ has $t = \ceil{c\log n}$ records
  $r_1 < r_2 < \cdots < r_t$, for some fixed $c > 2$. First, give a
  bit string $x = (x_1, \dots, x_t) \in \{0, 1\}^t$, where $x_1 = 0$
  and $x_i = 1$ if and only if $r_i$ lies in the second half of the
  interval $[r_{i - 1}, n]$. If $n_1(x)$ represents the number of ones
  in the bit string $x$, it follows that $n_1(x) \leq \log n$, so
  $n_1(x)/t \leq 1/c$ \wolfgang{What is $n_1$ again (maybe good to
    remind the reader)? And how do you define $r_0$ (i.e., how do you
    get $x_1$?  Finally, maybe it is better to define $x$ in terms of
    $x_i = 1$, since afterwards you talk about
    $n_1$?}. \tommy{Alright}

  To begin our encoding of $\sigma$, we encode the bit string $x$ by
  giving the set of $n_1(x)$ ones in $x$; followed by the incremental
  recursive encoding of $\sigma$ from earlier. Now, our knowledge of
  the value of $x_i$ halves the size of the space of options for
  encoding the record $r_i$. In other words, our knowledge of $x$
  allows us to encode each record using roughly one less bit per
  record. More precisely, if the number of choices for each record
  $r_i$ in the original encoding is $m_i$, such that $m_1 > \dots >
  m_t$, then the number of bits spent encoding records in the new code
  is at most
  \begin{align*}
    \sum_{i = 1}^t \log \ceil{m_i/2} &\leq \sum_{i = 1}^t \log (m_i/2
                                       + 1)
    \\
                                     &\leq \sum_{i = 1}^t \log (m_i/2) + \sum_{i = 1}^t O(1/m_i) \tag{since $\log (1 + x) = \log x + O(1/x)$} \\
                                     &\leq \sum_{i = 1}^t \log (m_i/2) + O(H_t) \\
                                     &= \sum_{i = 1}^t \log
                                       m_i - t + O(\log \log n) \enspace ,
  \end{align*}
  since $c$ is a constant. Thus, the total length of the code is
  \begin{align*}
    |C(\sigma)| &\leq \binom{t}{n_1(x)} + \log n! - t + O(\log \log n) \\
                &\leq \log n! - t (1 - H(n_1(x)/t)) + O(\log \log n) \tag{by \eqref{log-n-choose-k}}\\
                &\leq \log n! - c (1 - H(1/c))\log n + O(\log \log n) \enspace ,
  \end{align*}
  where this last inequality follows since $c > 2$, so $H(n_1(x)/t)
  \leq H(1/c)$. We finish by applying the Uniform Encoding Lemma.
\end{proof}

\begin{rem}\remlabel{records}
  The preceding result only works for $c > 2$, but we already know
  that the number of records in a uniformly random permutation is
  concentrated around $\ln n + O(1)$, where $\ln n = \alpha \log n$
  for $\alpha = 0.6931...$. We leave as an open problem whether or not
  this significant gap can be closed through an encoding argument.
\end{rem}

\subsubsection{The Height of a Random Binary Search Tree}
\seclabel{height}

Every permutation $\sigma$ determines a binary search tree
$\text{BST}(\sigma)$ created through the sequential insertion of the
keys $\sigma_1, \ldots, \sigma_n$. Specifically, if $\sigma^L$
(respectively, $\sigma^R$) denotes the permutation of elements
strictly smaller (respectively, strictly larger) than $\sigma_1$, then
$\text{BST}(\sigma)$ has $\sigma_1$ as its root, with
$\text{BST}(\sigma^L)$ and $\text{BST}(\sigma^R)$ as left and right
subtrees.

Lucier \emph{et al.}~\cite{lucier.jiang.li:quicksort} use an encoding
argument via Kolmogorov complexity to study the height of
$\text{BST}(\sigma)$. They show that for a uniformly chosen
permutation $\sigma$, the tree $\text{BST}(\sigma)$ has height at most
$c \log n$ with probability $1 - O(1/n)$ for $c = 15.498\dots$; we can
extend our result on records to obtain a tighter result \wolfgang{This
  is a bit strange. Why this specific constant $c$? Why is there no
  tradeoff between the probability and $c$?} \tommy{Their result works
  in essentially the same way as ours, but there is some tradeoff
  between the probability and $c$. In fact, $c > 2/\log(4/3)$ suffices
  in our case to get nonzero probability, but then the concentration
  is weaker than $O(1/n)$. I chose to obtain the same concentration
  result as Lucier et al but with a better constant}.

For a node $u$, let $s(u)$ denote the number of nodes in the tree
rooted at $u$. Then, $u$ is called \emph{balanced} if
$s(u^L), s(u^R) > s(u)/4$, where $u^L$ and $u^R$ are the left and
right subtrees of $u$ respectively. In other words, since each node
$u$ determines an interval $[v, w]$, where $v$ is the smallest node in
the subtree rooted at $u$, and $w$ is the largest such node, then $u$
is balanced if and only if
\[
  u \in \left[\frac{w + v}{2} - \frac{w - v}{4}, \frac{w + v}{2} + \frac{w - v}{4}\right] \enspace ,
\]
\emph{i.e.}~$u$ is called balanced if it occurs in the middle interval
of length $(w - v)/2$ of its subrange.

%de is said to be balanced if it
%occurs in the middle half of its subrange \wolfgang{"middle half" sounds
%a bit strange to me. Maybe "middle interval of size $s/2$?}.

\begin{thm}\thmlabel{bst-height}
  Let $\sigma$ be a uniformly random permutation of size $n$. Then,
  $\text{BST}(\sigma)$ has height $O(\log n)$ with high probability.
\end{thm}
\begin{proof}
  Suppose that the tree $\text{BST}(\sigma)$ has a root-to-leaf path
  $Y = (y_1, \ldots, y_t)$ of length $t = \ceil{c \log n}$ for some
  fixed $c > 2/\log (4/3)$.

  Our encoding for $\sigma$ has two parts. The first part includes an
  encoding of the value $y_t$, and a bit string
  $x = (x_1, \ldots, x_t)$, where $x_i = 1$ if and only if $y_i$ is
  balanced. From our definition, if $y_i$ is balanced, then
  $s(y_{i + 1}) \leq (3/4) s(y_i)$, and since $n_1(x)$ counts the
  number of balanced nodes along $Y$, then
  \[
    1 \leq (3/4)^{n_1(x)} n \iff n_1(x) \leq \log_{4/3} n \enspace .
  \]

  The second part of our encoding is recursive: First, encode the
  value of the root $r$ using $\log \ceil{n/2}$ bits. If the
  path $Y$ goes to the left of $r$, then specify the values in the
  right subtree of $r$, including an explicit encoding of the
  permutation induced by these values; and recursively encode the
  permutation of values in the left subtree of $r$. If, instead, $Y$
  goes to the right of $r$, proceed symmetrically.
 
  The first part of our encoding uses at most
  \[
    \log n + t H(n_1(x)/t)
  \]
  bits. The same analysis as in the proof of \thmref{records} shows that
  the second part of our encoding has length at most
  \[
    \log n! - t + O(\log \log n) \enspace .
  \]
  Applying the Uniform Encoding Lemma, we see that
  $\text{BST}(\sigma)$ has height at most $c \log n$ with probability
  $1 - O(1/n)$ for $c > 2/\log (4/3)$ satisfying
  \[
    c \left(1 - H\left(\frac{1}{c \log (4/3)}\right)\right) > 1 \enspace ,
  \]
  and a computer-aided calculation shows that $c = 8.12669\dots$ is the
  minimal solution so this inequality.
\end{proof}

\begin{rem}
  Devroye~\cite{devroye:records} shows how the length of the path to
  the key $i$ in $\text{BST}(\sigma)$ relates to the number of records
  in $\sigma$. Specifically, he notes that the number of records in
  $\sigma$ is the number of nodes along the rightmost path in
  $\text{BST}(\sigma)$. Since the height of a tree is the length of
  its longest root-to-leaf path, we obtain as a corollary that the
  number of records in a uniformly random permutation is $O(\log n)$
  with high probability; the result from \thmref{records} only
  improves upon the implied constant.
\end{rem}

\begin{rem}
  We know that the height of the binary search tree built from the
  sequential insertion of elements from a uniformly random permutation
  of size $n$ is concentrated around $\alpha \ln n + O(\log \log n)$,
  for $\alpha = 4.311...$~\cite{reed:height}. Perhaps if the gap in
  our analysis of records in \remref{records} can be closed through an
  encoding argument, then so too can the gap in our analysis of random
  binary search tree height.
\end{rem}

\subsubsection{Hoare's Find Algorithm}

In this section, we analyze the number of comparisons made in the
execution of Hoare's classic \textsc{Find} algorithm~\cite{hoare:find}
which returns the $k$-th smallest element in an array of $n$
elements. The analysis is similar to that of the preceding section.

We refer to an easy algorithm $\textsc{Partition}$, which takes as
input an array $\sigma = (\sigma_1, \dots, \sigma_n)$ and partitions
it into the arrays $\sigma^L$ and $\sigma^R$ which contain the values
strictly smaller and strictly larger than $\sigma_1$,
respectively. The element $\sigma_1$ is called a \emph{pivot}. The
algorithm $\textsc{Partition}$ can be implemented so as to perform
only $n - 1$ comparisons as follows:

\noindent{$\textsc{Partition}(\sigma)$}:
\begin{algorithmic}[1]
  \STATE{$\sigma^L, \sigma^R \gets \textbf{nil}$}
  \FOR{$i \gets 2$ \TO $n$}
    \IF{$\sigma_i > \sigma_1$}
      \STATE{push $\sigma_i$ onto $\sigma^R$}
    \ELSE
      \STATE{push $\sigma_i$ onto $\sigma^L$}
    \ENDIF
  \ENDFOR
  \RETURN{$\sigma^L, \sigma^R$}
\end{algorithmic}

Using this, we give the algorithm $\textsc{Find}$:

\noindent{$\textsc{Find}(k, \sigma)$}:
\begin{algorithmic}[1]
  \STATE{$\sigma^L, \sigma^R \gets \textsc{Partition}(\sigma)$}
  \IF{$|\sigma^L| \geq k$}
    \RETURN{$\textsc{Find}(k, \sigma^L)$}
  \ELSIF{$|\sigma^L| < k - 1$}
    \RETURN{$\textsc{Find}(k - |\sigma^L| - 1, \sigma^R)$}
  \ENDIF
  \RETURN{$\sigma_k$}
\end{algorithmic}

Suppose that the algorithm $\textsc{Find}$ sequentially identifies $t$
pivots $x_1, \dots, x_t$ before finding the solution. Let
$\sigma^{(i)}$ denote the value of $\sigma$ in the $i{\text{th}}$
recursive call and let $n_i=|\sigma^{(i)}|$, so that
$\sigma^{(0)}=(\sigma_1,\ldots,\sigma_n)$ and $n_0=n$.  We will say
that the $i{\text{th}}$ pivot is \emph{good} if its rank, in
$\sigma^{(i)}$, is in the interval $[n_i/4, 3n_i/4]$. Note that a good
pivot causes the algorithm to recurse in a problem of size at most
$3n_i/4$.

\begin{lem}
  Fix some constants $t_0 \geq 1$ and $0 < \alpha < 1/2$. Suppose
  that, for each $t_0 \leq i \leq t$, the number of good pivots among
  $x_1, \ldots, x_i$ is at least $\alpha i$. Then, $\textsc{Find}$
  makes $O(n)$ comparisons.
\end{lem}
\begin{proof}
  If $x_j$ is a good pivot, then the conditions of the lemma give
  that $n_j \le (3/4) n_{j - 1}$. Therefore,
  \[
  n_i \leq (3/4)^{\alpha i} n
  \]
  for each $t_0 \leq i \leq t$, and the total number of comparisons
  made by $\textsc{Find}$ is at most \wolfgang{Why $+1$? Above, you
    said that Partition makes $n-1$ comparisons? Or do you count the
    two comparisons that evaluate the size of $\sigma^L$? Should they
    count? They are not comparisons among elements of the
    permutation?} \tommy{Yes, alright, they should not count.}
  \[
    \sum_{i=0}^t n_i \le t_0 n + \sum_{i=t_0}^t n_i \le O(n) + n
    \sum_{i = t_0}^t (3/4)^{\alpha i} = O(n) \enspace . \qedhere
  \]
\end{proof}

\begin{thm}
  For a uniformly random permutation $\sigma$ and any $k$,
  $\textsc{Find}(k, \sigma)$ executes $O(n)$ comparisons with high
  probability.
\end{thm}
\begin{proof}
  We again encode the permutation $\sigma$. Suppose that the
  conditions of the preceding lemma are not satisfied,
  \emph{i.e.}~there is an $i$ such that the number of good pivots
  among $x_1, \dots, x_i$ is less than $\alpha i$. We encode $\sigma$
  in two parts. The first part of our encoding gives the value of $i$
  using an Elias $\delta$-code, followed by the set of indices of the
  good pivots among $x_1, \dots, x_i$, which costs
  \[
  \log i + i H(\alpha) + O(\log \log i) \enspace .
  \]
  Note that the pivots $x_1, \dots, x_i$ trace a path from the root in
  $\text{BST}(\sigma)$. Therefore, the second part of our encoding is
  the recursive encoding presented in \secref{height}, in which each
  pivot can be encoded using one less bit. In total, our code then has
  length
  \[
    |C(\sigma)| \le \log n! - i + i H(\alpha) + \log i + O(\log \log
    i) = \log n! - \varOmega(i) \enspace ,
  \]
  since $\alpha < 1/2$. The proof is completed by applying the Uniform
  Encoding Lemma, and by observing that $k_0 \leq i$ can be made
  arbitrarily large.
\end{proof}


\subsection{$k$-SAT and the Lov\'{a}sz Local Lemma}

\wolfgang{Here, I think we are missing an introductory sentence.
"We now consider the question of satisfiability of propositional
formulas. Let us start with some definitions.". Or something
like this.} \tommy{Okay, that sounds good}

We now consider the question of satisfiability of propositional
formulas. Let us start with some definitions.

A \emph{variable} $x$ is either $\textsf{true}$ or
$\textsf{false}$. The negation of $x$ is denoted by $\neg x$. A
\emph{literal} is either a variable or its negation. A
\emph{conjunction} of literals is an ``and'' of literals, denoted by
$\land$. A \emph{disjunction} of literals is an ``or'' of literals,
denoted by $\lor$. A \emph{formula} $\varphi$ is an expression
including conjunctions and disjunctions of literals, and the set of
variables involved in this formula is called the \emph{support} of
$\varphi$. A \emph{clause} is a disjunction of literals, \emph{i.e.}
the ``or'' of a set of variables or their negations, \emph{e.g.}
\begin{align}
  x_1 \lor \neg x_2 \lor x_3 \enspace . \eqlabel{clause-example}
\end{align}
Two clauses will be said to intersect if their supports intersect. The
truth value which a formula $\varphi$ evaluates to under the
assignment of values $\alpha$ to its support will be denoted by
$\varphi(\alpha)$, and such a formula is said to be \emph{satisfiable}
if there exists an $\alpha$ with $\varphi(\alpha) = \textsf{true}$.
For example, the clause in \eqref{clause-example} is satisfied for all
truth assignments except
\[
  (x_1, x_2, x_3) = (\textsf{false}, \textsf{true}, \textsf{false})
  \enspace ,
\]
and indeed any clause is satisfied by all but one truth assignments
for its support. The formulas we are concerned with are conjunctions
of clauses, which are said to be in \emph{conjunctive normal form}
(CNF). More specifically, when each clause has exactly or at most $k$
literals, we call it a $k$-CNF formula.

The $k$-SAT decision problem asks to determine whether or not a given
$k$-CNF formula is satisfiable. In general, this problem is hard. Of
course, any satisfying truth assignment to the variables in a CNF
formula induces a satisfying truth assignment for each of its
clauses. Moreover, if the supports for each clause are pairwise
disjoint, then the formula is trivially satisfiable
%, the converse is true if each clause is
%pairwise disjoint
\wolfgang{I do not understand this. What is "the converse"?  It the
  clauses are pairwise disjoint, then the union of their satisfying
  assignments gives a satisfying assignment for $\varphi$?  Also, I am
  not sure it makes sense to say "each clause is pairwise disjoint".}
\tommy{Okay, I'm more specific here}, and as we will see, even if each
clause is only nearly pairwise disjoint, \emph{i.e.}  their
intersection has size less than $2^k/e$.

This result has been well known as a consequence of the Lov\'{a}sz
Local Lemma~\cite{lovasz:locallemma}, whose original proof is
non-constructive, and so does not produce a satisfying truth
assignment (in polynomial time) when applied to an instance of
$k$-SAT. Some efficient constructive solutions to $k$-SAT have been
known, but only for suboptimal clause intersection sizes.
\wolfgang{Maybe say that Moser first considered the case of $k$-SAT,
  and that Moser and Tardos then generalized it to the full LLL (if I
  remember the history correctly)?} \tommy{Yes, you're right}
Moser~\cite{moser:ksat} first presented a constructive solution to
$k$-SAT with near optimal clause intersection sizes, and Moser and
Tardos~\cite{moser:locallemma} then generalized this result to the
full Lov\'{a}sz Local Lemma for optimal clause intersection sizes. The
analysis which we reproduce in this section comes from Fortnow's
rephrasing of Moser's proof for $k$-SAT using the incompressibility
method~\cite{fortnow:ksat}.

Moser's algorithm is remarkably naive, and it can be described in only a
few sentences: Pick a uniformly random truth assignment for the
variables of $\varphi$. For each unsatisfied clause, attempt to fix it
by producing a new uniformly random truth assignment for its support,
and recursively fix any intersecting clause which is made unsatisfied
by this reassignment. We describe this process more carefully in the
algorithms $\textsc{Solve}$ and $\textsc{Fix}$ below.

\noindent{$\textsc{Solve}(\varphi)$}:
\begin{algorithmic}[1]
  \STATE{$\alpha \gets $ uniformly random truth assignment in $\{\textsf{true}, \textsf{false}\}^n$}
  \WHILE{$\varphi(\alpha) = \textsf{false}$}
    \STATE{$D \gets $ an unsatisfied clause in $\varphi$}
    \STATE{$\alpha \gets \textsc{Fix}(\varphi, \alpha, D)$}
  \ENDWHILE
  \RETURN{$\alpha$}
\end{algorithmic}

\noindent{$\textsc{Fix}(\varphi, \alpha, D)$}:
\begin{algorithmic}[1]
  \STATE{$\beta \gets $ uniformly random truth assignment in $\{\textsf{true}, \textsf{false}\}^k$}
  \STATE{replace the assignments in $\alpha$ for $D's$ support with the values in $\beta$}
  \WHILE{$\varphi(\alpha) = \textsf{false}$}
    \STATE{$D' \gets $ an unsatisfied clause in $\varphi$ intersecting $D$}
    \STATE{$\alpha \gets \textsc{Fix}(\varphi, \alpha, D')$}
  \ENDWHILE
  \RETURN{$\alpha$}
\end{algorithmic}

\begin{thm}
%  Let $a \in \{\textsf{true}, \textsf{false}\}^n$ and $b_i \in
%  \{\textsf{true}, \textsf{false}\}^{k}$ for each $1 \le i \le s$ be
%  chosen uniformly at random.
%
  Given a $k$-CNF formula $\varphi$ with $m$ clauses and $n$ variables
  such that each clause intersects at most $r \le 2^{k - 3}$ other
  clauses, then the total number of invokations of $\textsc{Fix}$ in
  the execution of $\textsc{Solve}(\varphi)$ is at least
  $s + m \log m$ with probability at most $2^{-s}$.\wolfgang{That is a
    bit unclear. You mean the \emph{total number} of invocations of
    FIX, right? Not just from Solve?} \tommy{Okay, attempted to
    clarify}
\end{thm}
\begin{proof}
  Suppose that $\textsc{Solve}(\varphi)$ calls $\textsc{Fix}$
  $t = \ceil{s + m\log m}$ times. Let
  $\alpha \in \{\textsf{true}, \textsf{false}\}^n$ be the initial
  truth assignment for $\varphi$, and let
  $\beta_1, \ldots, \beta_t \in \{\textsf{true}, \textsf{false}\}^k$
  be the local truth assignments produced in each call to
  $\textsc{Fix}$. The string
  $\gamma = (\alpha, \beta_1, \ldots, \beta_t)$ is uniformly chosen
  from a set of size $2^{n + tk}$, and will be the subject of our
  encoding.

  The execution of $\textsc{Solve}(\varphi)$ determines a (rooted
  ordered) \emph{recursion tree} $T$ on $t + 1$ nodes as follows: The
  root of $T$ corresponds to the initial call to
  $\textsc{Solve}(\varphi)$, and every node has as children its
  sequence of calls to $\textsc{Fix}$.
  %and its children $D_1, \ldots, D_d$
  %recursively correspond 
  \wolfgang{Explain a bit more what "recursively correspond" means?}
  \tommy{I simplified the description. I'm not sure if it's clear
    enough}
  %to the sequence of calls to
  %$\textsc{Fix}(\varphi, \alpha, D_1), \ldots, \textsc{Fix}(\varphi,
  %%\alpha, D_d)$.
  In particular, each (non-root) node in the tree is assigned a clause
  and its uniformly random truth assignment produced during the call
  to $\textsc{Fix}$. Moreover, a pre-order traversal of this tree
  describes the order of function calls in the algorithm's execution.

  The string $\gamma$ can be recovered in a bottom-up manner from our
  knowledge of the tree $T$ and the final truth assignment
  $\alpha'$. Specifically, let $D_1, \dots, D_t$ be the clauses
  encountered in a pre-order traversal of $T$. In particular, $D_t$ is
  the last fixed clause in the execution. Since $D_t$ was not
  satisfied before its reassignment, this allows us to deduce $k$
  values \wolfgang{It is not really $\alpha$, right? The assignment
    can oscillate quite a bit during execution, no? Maybe say, "the
    previous assignment"?} \tommy{Alright} of the previous assignment
  before $D_t$ was fixed. Pruning $D_t$ from the tree and continuing
  in this manner at $D_{t - 1}$, we eventually recover the original
  truth assignment $\alpha$ produced in $\textsc{Solve}(\varphi)$.

  Therefore, to encode $\gamma$, we first give the final truth
  assignment $\alpha'$; and a description of the shape of the tree
  $T$; and the sequence of at most $m$ clauses which are children of
  the root of $T$; and the at most $t$ clauses involved in the calls
  to $\textsc{Fix}$ in a pre-order traversal of $T$.  \wolfgang{Oh, I
    see. There is some notation clash, because above you defined
    $\alpha$ to be the \emph{initial} assignment...} \tommy{Ah
    yeah. Fixed}

  The key savings come from the fact that each clause intersects at
  most $r$ other clauses, so each clause (which is not a child of the
  root) can be encoded using $\log r$ bits. Each clause which is a
  child of the root can be encoded using $\log m$ bits, and since the
  order of these children might be significant, we use $m \log m$ bits
  to encode the full sequence of these clauses. Finally, as in
  \lemref{two-choice-component-size}, the shape of $T$ can be encoded
  using $2t$ bits. In total, the code has length
  \begin{align*}
    |C(\gamma)| &\le n + 2t + m\log m+ t \log r \\
    &\le n + 2t + m\log m + t(k - 3) \tag{since $r \le 2^{k - 3}$} \\
    &= n + tk - t + m\log m \\
    &\le n + tk - s \enspace .
  \end{align*}
  The result is obtained by applying the Uniform Encoding Lemma.
\end{proof}

\begin{rem}
  By more carefully encoding of the shape of the recursion tree above,
  Messner and Thierauf~\cite{messner:ksat} gave an encoding argument
  for the above result in which $r < 2^k/e$. Specifically, their 
  refinement follows
  from a more careful counting of the number of trees with nodes of
  bounded degree.
\end{rem}

\section{The Non-Uniform Encoding Lemma and Shannon-Fano Codes}
\seclabel{nuel}

Thus far, we have focused on applications that could always be
modelled as choosing some element $x$ uniformly at random from a
finite set $X$. To encompass even more applications, it is helpful to
have an Encoding Lemma that deals with \emph{non-uniform}
distributions over $X$. First, we recall the following useful classic
results: \wolfgang{Notation clash: $X$ used to be a set, now it is a
  random variable.} \tommy{When I wrote about this before, I used
  $\Omega$ instead of $X$ for the sample spaces, so that I could use
  $X$ for random variables. Maybe we should go that route? If not, we
  can use $Y$ for random variables I guess. Or $\mathbf{X}$? But then
  maybe I'd have to go back and bold some other random variables (like
  hash functions)? I'd appreciate a suggestion.}
\begin{thm}[Markov's Inequality]
  For any non-negative random variable $Y$ with finite
  expectation, and any $a > 0$,
  \[
    \Pr\{Y \geq a\} \leq (1/a) \E\{Y\} \enspace .
  \]
\end{thm}

We will say that a real-valued function $\ell : X \to \R$
\emph{satisfies Kraft's condition} if
\[
  \sum_{x \in X} 2^{-\ell(x)} \leq 1 \enspace .
\]
\begin{lem}[Kraft's Inequality]
  If $C : X \nrightarrow \{0,1\}^*$ is a partial prefix-free code,
  then $|C(\cdot)|$ satisfies Kraft's condition. Conversely, for any
  function $\ell : X \to \N$ satisfying Kraft's condition, there
  exists a prefix-free code $C : X \to \{0, 1\}^*$ such that
  $|C(x)| = \ell(x)$ for all $x \in X$.
\end{lem}

The following generalization of the Uniform Encoding Lemma, which was
originally proven by Barron~\cite[Theorem~3.1]{barron:dissertation},
serves for non-uniform input distributions:
\begin{lem}[Non-Uniform Encoding Lemma]\lemlabel{nuel}  
  Let $C\from X\nrightarrow\{0,1\}^*$ be a partial prefix-free code
  and let $x\in X$ be drawn randomly where $p_x > 0$ denotes the
  probability of drawing $x$.  Then
  \[
    \Pr\{ |C(x)| \le \log(1/p_x)-s\} \le 2^{-s} \enspace .
  \]
\end{lem}

\begin{proof}
  We use Chernoff's trick, Markov's inequality, and Kraft's
  inequality, as follows:
  \begin{align*}
    \Pr\{ |C(x)| \le\log(1/p_x)-s \}
    & = \Pr\{|C(x)| -\log(1/p_x) \le -s \} \\
    & = \Pr\{\log(1/p_x)-|C(x)| \ge s \} \\
    & = \Pr\left\{2^{\log(1/p_x)-|C(x)|} \ge 2^s \right\}  \tag{Chernoff's trick} \\
    & \le \frac{\E\left\{2^{\log(1/p_x)-|C(x)|}\right\}}{2^s} \tag{Markov's inequality} \\
    & = \frac{1}{2^s} \left(\sum_{x\in X : C(x) \neq \bot} p_x\cdot 2^{\log(1/p_x)-|C(x)|}\right) \\
    & = \frac{1}{2^s} \left(\sum_{x\in X : C(x) \neq \bot }2^{-|C(x)|}\right) \enspace .
  \end{align*}
  By Kraft's inequality,
  $\sum_{x \in X : C(x) \neq \bot} 2^{-|C(x)|} \leq 1$, and the result
  is obtained.
\end{proof}

The Non-Uniform Encoding Lemma is a strict generalization
of the Uniform Encoding Lemma: Take $p_x=1/|X|$ for all $x\in X$ and
we obtain the Uniform Encoding Lemma.

As in \secref{sparse-bit-strings}, we will be interested in using a
Shannon-Fano code $C_\alpha$ to encode $\mathrm{Bernoulli}(\alpha)$
bit strings of length $n$. Recall that for such a string $x$, this
code has length
\[
  n_1(x) \log (1/\alpha) + n_0(x) \log(1/(1 - \alpha)) \enspace ,
\]
since we are not concerned with ceilings.

\section{Applications of the Non-Uniform Encoding Lemma}
\seclabel{applications-ii}

\subsection{Chernoff Bound}
\seclabel{chernoff}

We will now prove the so-called \emph{additive version} of the
Chernoff bound on the tail of a binomial random
variable~\cite{chernoff:bound}. \thmref{chernoff-basic} established
the special case of this result for $\mathrm{Bernoulli}(1/2)$ bit
strings.

\begin{thm}\thmlabel{chernoff}
  If $B$ is a $\mathrm{Binomial}(n,p)$ random variable, then for any
  $\eps \geq 0$,
  \[
    \Pr\{B\le(p-\eps)n\} \le 2^{-nD(p-\eps \| p)} \enspace ,
  \]
  where 
  \[ 
    D(p \| q)= p\log (p/q) + (1-p)\log ((1 - p)/(1 - q))
  \]
  is the \emph{Kullback-Liebler divergence} or \emph{relative entropy}
  between $\mathrm{Bernoulli}(p)$ and $\mathrm{Bernoulli}(q)$ random
  variables.
\end{thm}

\begin{proof}
  By definition, $B=\sum_{i=1}^n x_i$, where $x_1,\ldots,x_n$ are
  independent $\mathrm{Bernoulli}(p)$ random variables.  We will use
  an encoding argument on the bit string $x=(x_1,\ldots,x_n)$. The
  proof is almost identical to that of \thmref{chernoff-basic}---now,
  we encode $x$ using a Shannon-Fano code $C_\alpha$, with
  $\alpha = p - \eps$. Such a code has length
  \[
    |C_{p - \eps}(x)| = n_1(x) \log (1/(p - \eps)) + n_0(x) \log (1/(1
    - p + \eps)) \enspace .
  \]
  Now, $x$ appears with probability
  $p_x = p^{n_1(x)} (1 - p)^{n_0(x)}$, so
  \begin{align*}
    |C_{p - \eps}(x)| &= \log(1/p_x) + n_1(x) \log (p/(p - \eps)) + (n - n_1(x)) \log ((1 - p)/(1 - p + \eps)) \\
                      &= \log (1/p_x) + n_1(x) \log \left(1 + \frac{\eps}{p - \eps}\right) + (n_1(x) - n) \log \left(1 + \frac{\eps}{1 - p}\right) \enspace ,
  \end{align*}
  and $|C_{p - \eps}(x)|$ increases as a function of
  $n_1(x)$. Therefore, if $n_1(x) \le (p - \eps)n$, then
  \begin{align*}
    |C_{p - \eps}(x)| &\le \log (1/p_x) - n (p - \eps) \log ((p - \eps)/p) - n (1 - p + \eps) \log ((1 - p + \eps)/(1 - p)) \\
                      &= \log (1/p_x) - n D(p - \eps \| p) \enspace .
  \end{align*}
  The Chernoff bound is obtained by applying the Non-Uniform Encoding
  Lemma.
\end{proof}

\subsection{Percolation on the Torus}

Percolation theory studies the emergence of large components in random
graphs. For a general study of percolation theory, see the book by
Grimmett~\cite{grimmett:percolation}.  We give an encoding argument
proving that percolation occurs on the torus when edge survival rate
is greater than $2/3$, \emph{i.e.}~in random subgraphs of the torus
grid graph in which each edge is included independently at random with
probability at least $2/3$, only at most one large component
emerges. Our line of reasoning follows what is known as a
\emph{Peierls argument}.

Suppose that $\sqrt{n}$ is an integer. The \emph{$\sqrt{n} \times \sqrt{n}$
  torus grid graph} is defined to be the graph with vertex set
$\{1, \ldots, \sqrt{n}\}^2$, where $(i, j)$ is adjacent to $(k, l)$
if
\begin{itemize}[topsep=0pt]
\item $|i - k| \equiv 1 \pmod{\sqrt{n}}$ and $|j - l| = 0$, or
\item $|i - k| = 0$ and $|j - l| \equiv 1 \pmod{\sqrt{n}}$.
\end{itemize}

\begin{thm}\thmlabel{percolation-long-cycles}
  Suppose that $\sqrt{n}$ is an integer.  Let $G$ be a subgraph of the
  $\sqrt{n} \times \sqrt{n}$ torus grid graph in which each edge is
  chosen with probability $p < 1/3$. Then, the probability that $G$
  contains a cycle of length at least
  \[
    \frac{s + \log n + O(1)}{\log (1/(3p))}
  \]
  is at most $2^{-s}$.
\end{thm}
\begin{proof}
  Let $A$ be the bitstring of length $2n$ encoding the existence of
  edges in $G$. Suppose that $G$ contains a cycle $C'$ of length
  $t \geq (s + \log n + O(1))/\log (1/(3p))$. Encode $A$ by giving a
  single vertex $u$ in $C'$; the sequence of directions that the cycle
  moves along from $u$; and a Shannon-Fano code with parameter $p$ for
  the remaining edges of $G$.

  There are four possibilities for the direction of the
  first step taken by $C'$ from $u$, but only three for each
  subsequent choice. Thus, this sequence can be specified by
  $2 + (t - 1) \log 3$ bits. The total length of our code is then
  \begin{align*}
    |C(G)| &= \log n + 2 + (t - 1) \log 3 + (n_1(A) - t) \log (1/p) +
             n_0(A) \log (1/(1 - p)) \\
           &= \log (1/p_G) + \log n - t \log (1/(3p)) + O(1) \\
           &\leq \log (1/p_G) - s
  \end{align*}
  by our choice of $t$. We finish by applying the Non-Uniform Encoding
  Lemma.
\end{proof}

The torus grid graph can be drawn in the obvious way without crossings
on the surface of a torus. This graph drawing gives rise to a dual
graph, in which each vertex corresponds to a face in the primal
drawing, and two vertices are adjacent if and only their primal faces
are incident to the same edge. This dual graph is isomporphic to the
original torus grid graph.

The obvious drawing of the torus grid graph also induces drawings for
any of its subgraphs. Such a subgraph also has a dual, where each
vertex corresponds to a face in the dual torus grid graph, and two
vertices are adjacent if and only if their corresponding faces are
incident to the same edge of the original subgraph.

\begin{thm}
  Suppose that $\sqrt{n}$ is an integer.  Let $G$ be a subgraph of the
  $\sqrt{n} \times \sqrt{n}$ torus grid graph in which each edge is
  chosen with probability greater than $2/3$. Then, $G$ has at most
  one component of size $\omega(\log^2 n)$ with high probability.
\end{thm}
\begin{proof}
  See \figref{peierls} for a visualization of this phenomenon. Suppose
  that $G$ has at least two components of size $\omega(\log^2
  n)$. Then, there is a cycle of faces separating these components
  whose length is $\omega(\log n)$. From the discussion above, such a
  cycle corresponds to a cycle of $\omega(\log n)$ missing edges in
  the dual graph, as in \figref{peierlsrare}. From
  \thmref{percolation-long-cycles}, we know that this does not happen
  with high probability.
\end{proof}

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \includegraphics{torusrare}
    \caption{When $p = 0.33 < 1/3$, long cycles are rare. Dotted lines show
      missing edges in the dual.}
    \figlabel{peierlsrare}
  \end{subfigure}
  \quad\quad
  \begin{subfigure}[t]{0.4\textwidth}
    \includegraphics{torusdense}
    \caption{When $p = 0.67 > 2/3$, there is likely only one large
      component.}
  \end{subfigure}
  \caption{Random subgraphs of the $20 \times 20$ torus grid graph.}
  \figlabel{peierls}
\end{figure}

\subsection{Triangles in $G_{n,p}$}

Recall as in \secref{ramsey} that the Erd\H{o}s-R\'{e}nyi random graph
$G_{n,p}$ is the space of undirected graphs with vertex set
$V=\{1,\ldots,n\}$ and in which each edge $\{u, w\} \in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.

The expected number of triangles (cycles of length 3) in $G_{n,p}$ is
easily seen to be $p^3\binom{n}{3}$.  For $p=(6c)^{1/3}/n$, this
expectation is $c-O(1/n)$.  Unfortunately, even when $c$ is a large
constant, it still takes some work to show that there is a constant
probability that $G_{n,p}$ contains at least one triangle. Indeed,
this typically requires the use of the second moment method, which
involves computing the variance of the number of triangles in
$G_{n,p}$. To show that $G_{n, p}$ has a triangle with more
significant probability is even more complicated \wolfgang{What is
  "exponential probability"?} \tommy{Well, okay, is it better if I
  make it more vague? I'm not exactly sure how difficult it would be
  to give a result with probability $1 - 2^{-\varOmega(c)}$, but every
  other proof for $1 - 2^{\varOmega(c^3)}$ hasn't been as easy as
  ours}, and a proof of this result would still typically rely on an
advanced probabilistic inequality~\cite{alon:probabilistic}. Here we
show how this can be accomplished with an encoding argument.

\begin{thm}\thmlabel{triangles-up}
  For $p=c/n$, $G \in G_{n,p}$ contains at least one triangle with
  probability at least $1-2^{-\varOmega(c^3)}$.
\end{thm}

\begin{proof}
  In this argument, we will produce an encoding of $G$'s adjacency
  matrix, $A$.

  Refer to \figref{triangles}.  If $G$ contains no triangles, then we
  look at the number of ones in the $n/2\times n/2$ submatrix $M$
  determined by rows $1,\ldots,n/2$ and columns $n/2+1,\ldots,n$. Note
  that $n_1(M)$, the number of ones in $M$, is a
  $\mathrm{Binomial}(n^2/4, c/n)$ random variable.  There are two
  cases to consider:
  
  \begin{figure}
    \centering{\includegraphics{triangles}}
    \caption{The random graph $G_{n,c/n}$ contains triangles when $c$
      is large enough.  The highlighted 0 bits in the last five rows
      can be deduced from pairs of 1 bits in the first 5 rows.}
    \figlabel{triangles} 
  \end{figure}

  \begin{enumerate}\setcounter{enumi}{-1}
  \item The number of ones in $M$ is less than $cn/8$.  In this case,
    the number of ones in this submatrix is much less than the
    expected number, $cn/4$.  Here one can apply the same argument
    used to prove Chernoff's bound (\thmref{chernoff}) or simply apply
    Chernoff's bound. We leave this as an exercise to the reader.

  \item The number of ones in the submatrix is greater than $cn/8$.
    Notice that, for $i<j<k$ if $A_{i,j}=1$ and $A_{i,k}=1$, then the
    fact that there are no triangles implies that $A_{j,k}=0$.

    Let $m_i$ be the number of ones in the $i$-th row of the
    submatrix.  By specifying rows $1,\ldots,n/2$, we eliminate the
    need to specify
    \[
      m = \sum_{i=1}^{n/2}\binom{m_i}{2} \ge (n/2) \binom{2
        n_1(M)/n}{2} \ge (n/2)\binom{c/4}{2} = \varOmega(c^2n) \enspace ,
    \]
    zeros in rows $n/2+1,\ldots,n$. We thus encode $G$ by giving a
    Shannon-Fano code with parameter $p$ for the first $n/2$ rows of
    $A$; and a Shannon-Fano code with parameter $p$ for the rest of
    $A$, excluding the bits which can be deduced from the preceding
    information. Such a code has length
    \[
      |C(G)| = n_1(A) \log(1/p) + (n_0(A)-m)\log(1/(1-p))
    \]
    which results in a savings of
    \[
      s = \log(1/p_G) - |C(G)| = m\log(1/(1-p)) \ge
      \varOmega(c^2n)\log(1/(1-p)) = \varOmega(c^3) \enspace . \qedhere
    \]
  \end{enumerate}
\end{proof}

\begin{thm}\thmlabel{triangles-down}
  When $p = 1/(\alpha n)$, $G \in G_{n, p}$ has no triangle with
  probability at least $1 - \alpha^{-3}$.
\end{thm}
\begin{proof}
  Suppose that $G$ contains a triangle. We encode the adjacency matrix
  $A$ of $G$. First, we specify the triangle's vertices; and finish
  with a Shannon-Fano code with parameter $p$ for the remaining
  vertices of the graph. This code has length
  \begin{align*}
    |C(G)| &= 3 \log n + (n_1(A) - 3) \log (1/p) + n_0(A) \log (1/(1 - p)) \\
           &= \log (1/p_G) + 3 \log n - 3 \log (1/p) \\
           &= \log (1/p_G) - 3 \log \alpha \\
           &= \log (1/p_G) - \log \alpha^3 \enspace . \qedhere
  \end{align*}
\end{proof}

Together, \thmref{triangles-up} and \thmref{triangles-down} establish
the fact that $1/n$ is a \emph{threshold function} for
triangle-freeness, \emph{i.e.}~if $p = o(1/n)$, then $G \in G_{n, p}$
has no triangle with high probability, and if $p = \omega(1/n)$, then
$G$ has a triangle with high probability. \wolfgang{What is a
  \emph{threshold function}?} \tommy{Elaborated a little bit}

\section{Encoding with Kraft's Condition}
\seclabel{el}

As promised in \secref{ceilings}, we finally discuss why it has made
sense to omit ceilings in all of our encoding arguments.

Let $[0, \infty]$ denote the set of extended non-negative real
numbers, supporting the extended arithmetic operations
$a + \infty = \infty$ for all $a \in [0, \infty]$, and
$2^{-\infty} = 0$.

 \wolfgang{I am not sure I know what is "usual" here. For
  example, what is $0 \cdot \infty$ or $\infty/\infty$? Is there a
  reference?}  \tommy{Okay, it was a little hacky to use extended real
  numbers here since things become unclear when $\ell(x) = \infty$ and
  $p_x = 0$. But that ambiguity has always been present since we
  agreed that $|\bot| = \infty$. I've added some specifications that
  $p_x > 0$ for every $x \in X$, and hopefully that clears up maybe
  some uncertainty you were feeling here}.  Recall from \secref{nuel}
that a function $\ell : X \to [0, \infty]$ \emph{satisfies Kraft's
  condition} if
\[
  \sum_{x \in X} 2^{-\ell(x)} \leq 1 \enspace .
\]

The main observation is that neither the (Non-)Uniform Encoding Lemma
nor any of its applications has required the specification of any
prefix-free code: We know, by construction, that every such code is
prefix-free, but we could also deduce from Kraft's inequality that,
since our described codes satisfy Kraft's condition, a prefix-free
code with the same codeword lengths exists. Instead, we will see that
codeword lengths satisfying Kraft's condition suffice, and such
codeword lengths need not be integers.

\begin{lem}\lemlabel{el}
  Let $\ell : X \to [0, \infty]$ satisfy Kraft's condition and let
  $x\in X$ be drawn randomly where $p_x > 0$ denotes the probability
  of drawing $x$.  Then
  \[
    \Pr\{ \ell(x) \le \log(1/p_x)-s\} \le 2^{-s} \enspace .
  \]
\end{lem}
\begin{proof}
  The proof is identical to that of \lemref{nuel}.
\end{proof}

The \emph{sum} of two functions $\ell : X \to [0, \infty]$ and
$\ell' : X' \to [0, \infty]$ is a function
$\ell + \ell' : X \times X' \to [0, \infty]$ defined by
$(\ell + \ell') (x, x') = \ell(x) + \ell'(x')$. Note that for any
partial codes
$C : X \nrightarrow \{0, 1\}^*, C' : X' \nrightarrow \{0, 1\}^*$, any
$x \in X$, and any $x' \in X'$,
\[
(|C| + |C'|)(x, x') = |C(x)| + |C'(x')| = |(C \cdot C')|(x, x') \enspace .
\]
In other words, the sum of the functions of codeword lengths describes
the length of codewords in concatenated codes.

\begin{lem}\lemlabel{composition-tight}
  If $\ell : X \to [0, \infty]$ and $\ell' : X' \to [0,
  \infty]$ satisfy Kraft's condition, then so does $\ell + \ell'$.
\end{lem}
\begin{proof}
  Kraft's condition still holds:
  \[
  \sum_{(x, x') \in X \times X'} 2^{-(\ell + \ell')(x, x')} = \sum_{x
    \in X} \sum_{x' \in X'} 2^{-\ell(x) - \ell'(x')} = \sum_{x \in X}
  2^{-\ell(x)} \sum_{x' \in X'} 2^{-\ell'(x')} \leq 1 \enspace
  . \qedhere
  \]
\end{proof}
This is analogous to the fact that the concatenation of prefix-free
codes is prefix-free.

\begin{lem}\lemlabel{fixed-length-tight}
  For any probability density $p : X \to (0, 1)$, the function
  $\ell : X \to [0, \infty]$ with $\ell(x) = \log (1/p_x)$ satisfies
  Kraft's condition.
\end{lem}
\begin{proof}
  \[
  \sum_{x \in X} 2^{-\ell(x)} = \sum_{x \in X} 2^{-\log (1/p_x)} =
  \sum_{x \in X} p_x = 1 \enspace . \qedhere
  \]
\end{proof}
This tells us that we can ignore the ceiling in every instance of a
fixed-length code and every instance of a Shannon-Fano code while
encoding.

We now give a tight notion corresponding to Elias codes.
\begin{thm}[Beigel~\cite{beigel:elias}]\thmlabel{elias-tight}
  Fix some $0 < \eps < e - 1$. Let $\ell : \N \to \R$ be defined as
  \[
  \ell(i) = \log i + \log \log i + \cdots + \underbrace{\log \cdots
    \log}_{\text{$\log^* i$ times}} i - (\log \log (e - \eps)) \log^*
  i + O(1) \enspace .
  \]
  Then, $\ell$ satisfies Kraft's condition. Moreover, the function
  $\ell' : \N \to \R$ with
  \[
  \ell'(i) = \log i + \log \log i + \cdots + \underbrace{\log \cdots
    \log}_{\text{$\log^* i$ times}} i - (\log \log e) \log^* i + c
  \]
  does not satisfy Kraft's condition for any choice of the constant
  $c$.
\end{thm}

It is not hard to see how \lemref{el}, \lemref{composition-tight},
\lemref{fixed-length-tight}, and \thmref{elias-tight} can be used to
give encoding arguments with real-valued codeword lengths. For
example, recall how the result of \thmref{runs-i} carried an artifact
of binary encoding. Using our new tools, we can now refine this and
recover the exact result.
\begin{customthm}{\ref*{thm:runs-i}b}
  Let $x=(x_1,\ldots,x_n)\in\{0,1\}^n$ be chosen uniformly at random
  and let $t = \ceil{\log n + s}$. Then, the probability that there
  exists an $i\in\{1,\ldots,n-t+1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{customthm}
\begin{proof}
  Let $\ell : \{0, 1\}^n \to [0, \infty]$ be such that if $x$ contains
  a run of $t = \ceil{\log n + s}$ ones, then
  $\ell(x) = \log n + n - t$, and otherwise $\ell(x) = \infty$. We
  will show that $\ell$ satisfies Kraft's condition.

  %Recall that $x$ is said to contain a run of $t$ ones if there exists
  %some $i \in \{1, \ldots, n - t + 1\}$ such that $x_i = \cdots = x_{i
  %  + t - 1} = 1$.

  Let the function $f : \{1, \ldots, n - t + 1\} \to [0, \infty]$ have
  $f(i) = \log n$ for all $i$, and
  $g : \{0, 1\}^{n - t} \to [0, \infty]$ have $g(y) = n - t$ for all
  $y$. Both of these functions satisfy Kraft's condition by
  \lemref{fixed-length-tight}. By \lemref{composition-tight}, so does
  the function
  \[
    h = f + g : \{1, \ldots, n - t + 1\} \times \{0, 1\}^{n - t} \to
    [0, \infty] \enspace ,
  \]
  where $h(i, y) = \log n + n - t$ for all $i$ and $y$. Crucially,
  each element of the set $\{1, \ldots, n - t + 1\} \times \{0, 1\}^{n
    - t}$ corresponds to an $n$-bit binary string containing a run of
  $t$ ones: the element $(i, y) \in \{1, \ldots, n - t + 1\} \times
  \{0, 1\}^{n - t}$, where $y = (y_1, \ldots, y_{n - t})$, corresponds
  to the binary string
  \[
  (y_1, \dots, y_{i - 1}, \underbrace{1, 1, \dots, 1}_{\text{$t$ times}},
  y_i, \dots, y_{n - t}) \enspace .
  \]
  Therefore, $\ell$ satisfies Kraft's condition. By our choice of
  $t$, we have that $\ell(x) \leq n - s$ if and only if $x$ contains a
  run of $t$ ones. We finish by applying \lemref{el}.
\end{proof}

\section{Summary and Conclusions}
\seclabel{summary}

We have described a simple method for producing encoding
arguments. Using our encoding lemmas, we gave original proofs for
several previously established results.

Typically, one would invoke the incompressibility method after
developing some of the theory of Kolmogorov complexity. Our technique
requires only a basic understanding of prefix-free codes and one
simple lemma. We are also the first to suggest a simple and tight
manner of encoding using only Kraft's condition with real-valued
codeword lengths. In this light, we posit that there is no reason to
develop an encoding argument through the incompressibility method: our
Uniform Encoding Lemma is simpler, the Non-Uniform Encoding Lemma is
more general, and our technique from \secref{el} is less
wasteful. Indeed, though it would be easy to state and prove our
Non-Uniform Encoding Lemma in the setting of Kolmogorov complexity, it
seems as if the general encoding lemma from \secref{el} only can exist
in our simplified framework.

\begin{comment}
Using our encoding lemmas, we gave original proofs for several
previously established results. Specifically, we showed:
\begin{itemize}
\item if $n$ balls are thrown into $n$ urns uniformly at random, then
  no urn contains more than $O(\log n/\log \log n)$ balls with high
  probability;
\item linear probing succeeds in $O(1)$ expected time;
\item searching in 2-choice hashing takes time $O(\log \log n)$ with
  high probability;
\item a certain random bipartite graph is an expander with high
  probability;
\item the number of records in a uniformly random permutation is
  $O(\log n)$ with high probability;
\item the height of the binary search tree built from the sequential
  insertion of a uniformly random permutation of $n$ integers is
  $O(\log n)$ with high probability;
\item Chernoff's bound on the tail of a binomial random variable;
\item percolation occurs in random dense subgraphs of the torus;
\item a strong threshold for triangle-freeness in the
  Erd\H{o}s-R\'{e}nyi random graph.
\end{itemize}
\end{comment}

\section*{Acknowledgement}

This research was initiated in response to an invitation for the first
author to give a talk at the Ninth Annual Probability, Combinatorics
and Geometry Workshop, held April 4--11 at McGill University's
Bellairs Institute.  Many ideas that appear in the current paper were
developed during the workshop. The author is grateful to the other
workshop participants for providing a stimulating working environment.
In particular, Xing~Shi~Cai pointed out the application to runs in
binary strings (\thmref{runs-i}) and G\'abor~Lugosi stated and proved
the Non-Uniform Encoding Lemma (\lemref{nuel}) in response to the first
author's half-formed ideas about a non-uniform version of \lemref{uel}.

\bibliography{encoding}{}
\bibliographystyle{plain}

\end{document}
