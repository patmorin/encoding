\documentclass[lotsofwhite]{patmorin}
\usepackage{pat}
\usepackage[utf8]{inputenc}
\usepackage[noend]{algorithmic}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\keywords}[1]{\vspace{2em}\noindent\textbf{Keywords:} #1}
%\newcommand{\from}{\colon}

\title{\MakeUppercase{Encoding Arguments}}
\author{Pat Morin}
\date{}

\begin{document}
\begin{titlepage}
\maketitle

\begin{abstract}
\setlength{\baselineskip}{15.84pt}
This expository article surveys a number of applications of ``encoding
arguments,'' in which probabilistic statements are proven using the fact
a uniformly random choice from a set of size $N$ can not be encoded with
fewer than $\log N$ bits on average.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}

There is no doubt that probability theory plays a fundamental role
in computer science: Some of the fastest and simplest fundamental
algorithms and data structures are randomized; average-case analysis of
algorithms relies entirely on tools from probability theory; and many
difficult combinatorial questions have strikingly simple solutions using
probabilistic arguments.

Unfortunately, many of these beautiful results are inaccessible to most
computer scientists because of a view that ``the math is too hard.''
For instance, ACM's CS2013 Final Report does not require a full course
in probability theory \cite[Page~50]{acm2013}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on discrete
probability, as part of the discrete structures curriculum.

In this expository paper, we survey applications of ``encoding arguments''
that tranforms the problem of upper-bounding the probability of a specific
event, $\mathcal{E}$, into the problem of devising a code for the set
of elementary events that happens to be particularly short whenever
$\mathcal{E}$ occurs.  Encoding arguments have several advantages over
traditional probabilistic analysis:

\begin{enumerate}
  \item Encoding arguments are almost ``probability-free.''  Except for
  applying a simple Encoding Lemma, there is no probability involved.
  In particular, there is no chance to make common mistakes such as
  multiplying probabilities of non-independent events or (equivalently)
  multiplying expectations.  

  The proof of the encoding lemma itself is trivial and the only
  probability it uses is that the fact, if a finite set $X$ contains $r$
  special elements and we pick an element uniformly at random from $X$,
  then the probability of picking a special element is $r/|X|$.

  \item Encoding arguments usually yield strong results;
  $\Pr\{\mathcal{E}\}$ typically decreases at least exponentially in
  the parameter of interest. Traditionally, these strong concentration
  results require (at least) carefuly calculations on probabilities of
  independent events and/or the application of concentration inequalities.
  The subject of concentration inequalities is advanced enough to be
  the topic of entire textbooks \cite{A,B}.
  
  \item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing an
  efficient code---an algorithmic problem. Consider the following 
  two problems:
    \begin{enumerate}

    \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
       a random graph on $n$ vertices contains a clique of size $k=\lceil
       4\log n\rceil$.

    \item Design a encoding for graphs on $n$ vertices so that a graph,
       $G$, that contains a clique of size $k=\lceil 4\log n\rceil$
       is encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note:
       Your encoding and decoding algorithms don't have to be efficient,
       just correct.)
    \end{enumerate}
  Many computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that
  they can use Boole's Inequality will still be stuck
  wrestling with the formula $\binom{n}{4\log n}2^{-\binom{k}{2}}$.  
\end{enumerate}

Our motivation for this work is that encoding arguments are an easily
accessible, yet versatile tool for answering many questions.  Most of
these arguments can be applied after learning almost no probability
theory beyond the Encoding Lemma mentioned above.

The remainder of this article is organized as follows: In \secref{uel},
we present necessary background, including the \emph{Uniform
Encoding Lemma}, which is the basis of most of our encoding arguments.
\Secref{applications-i} presents  applications of the Uniform Encoding
Lemma to a variety of problems.  \Secref{nuel} presents a more
general Non-Uniform Encoding Lemma that can handle a larger variety of
applications, some of which are presented in \secref{applications-ii}.
\Secref{summary} summarizes and concludes with some directions for
future research.



\section{Background}
\seclabel{uel}

This section presents the necessary background on prefix-free codes and binomial coefficients.

\subsection{Prefix-free Codes}

A \emph{code}, $C\from X\to \{0,1\}^*$ is a one-to-function from a set
$X$ to the set of binary strings.  The elements of the range of $C$ are
called $C$'s \emph{codewords}.

A code, $C$, is \emph{prefix-free} if, for every $x,y\in X$ the binary
string $C(x)$ is not a prefix of $C(y)$.  It can be helpful to think
of prefix-free codes as (rooted ordered) binary trees whose leaves
are labelled with the elements in $S$.  The codeword for a particular
$x\in X$ is obtained by tracing the root-to-leaf path leading to $x$
and outputting a 0 (respectively, 1) each time this path goes from a
parent to its left (respectively, right) child. (See \figref{bintree}.)

\begin{figure}
  \centering{\includegraphics{bintree}}
  \caption{A prefix-free code for the set
    $S=\{\mathtt{a},\mathtt{b},\mathtt{c},\mathtt{d},\mathtt{e},\mathtt{f}\}$
    and the corresponding leaf-labelled binary tree.}
  \figlabel{bintree}
\end{figure}

If $C$ is prefix-free, then the number of $C$'s codewords that have length
\emph{at most} $k$ is not more than $2^k$. To see this why this is so,
observe that $C$ can be modified into a code $\hat C$, in which every
codeword of length $\ell <k$ is extended---by appending $k-\ell$ zeros---so that
it has length exactly $k$. The prefix-freeness of $C$ ensures that $\hat
C$ is also a prefix-free code). The number of $\hat C$'s codewords of
length $k$ is equal to the the number of $C$'s codewords of length at
most $k$; since codewords are just binary strings, there are not more
than $2^k$ of these.

As we will see, the following lemma is surprisingly versatile:
\begin{lem}[Uniform Encoding Lemma]\lemlabel{uel}
  Let $C\from X\to \{0,1\}^*$ be a prefix-free code. When an element $x\in X$
  is chosen uniformly at random $\Pr\{|C(x)|\le \log|S|-s\}\le
  2^{-s}$.
\end{lem}

\begin{proof}
  Since $C$ is a one-to-one function, choosing $x$ uniformly from $S$
  implies that $C(x)$ is chosen uniformly from $C$'s codewords. Let
  $k=\log|X|-s$. Since $C$ has $|X|$ codewords and at most $2^{k}$ of these
  have length at most $k$, the probability that $x$ is one of these 
  codewords is at most
  \[
     \frac{2^k}{|X|} = \frac{2^{\log|S|-s}}{|X|} = 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

\subsection{Runs in Binary Strings}

As a warm-up exercise to illustrate the use of the Uniform Encoding
Lemma we will show that a random $n$-bit string is unlikely to contain
a run of significantly more than $\log n$ one bits.

\begin{thm}\thmlabel{runs-i}
  Let $x=\langle x_1,\ldots,x_n\rangle\in\{0,1\}^n$ be chosen
  uniformly at random and let $t=1+\lceil\log n\rceil + s$. Then, the
  probability that there exists an $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  We will prove this theorem by constructing a prefix-free code
  for $x$. If there is no $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+k-1}=1$, then $C(x)$ is a zero bit followed
  by the $n$ bits $x_1,\ldots,x_n$.  However, if there is such an index,
  $i$, then $C(x)$ is a one bit, followed by (the binary encoding of)
  $i$, followed by the $n-t$ bits $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$.

  Observe that, in the latter case, $C(x)$ has length 
  \[
      1 + \lceil\log n\rceil + n - t = n-s \enspace .
  \]
  It is easy to check that, for any $x$, we can reconstruct
  $x_1,\ldots,x_n$ from $C(x)$, so $C$ is indeed a code for $\{0,1\}^n$.
  It is also easy to check that $C$ is prefix free, since the first bit of
  $C(x)$ determines the length of $C(x)$ (which is either $n+1$ or $n-s$).

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, the probability
  that there exists any index $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+k-1}=1$ is at most
  \[
      \Pr\{|C(x)|\le n-s\} \le 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

\subsection{Stirling Approximations}
\seclabel{stirling}

Before moving on to some more advanced applications, it will be helpful
to remind the reader of a few inequalities that can be derived from
Stirling's Approximation of $n!$.  Recall that Stirling's Approximation
states that
\begin{equation}
  n! = \left(\frac{n}{e}\right)^n\sqrt{2\pi n}\left(1+O\left(\frac{1}{n}\right)\right) 
   \eqlabel{stirling}
\end{equation}

\Eqref{stirling} immediately implies that there is a code for sets
of size $n!$ in which every codeword has length
\begin{align}
  \lceil \log n!\rceil 
      & \le n\log n - n\log e + (1/2)\log n + O(1)  
             \eqlabel{stirling-tight} \\
      & \le n\log n - n\log e + O(\log n)  
             \eqlabel{stirling-loose} \enspace .
\end{align}

Similarly, for any $k\in\{1,\ldots,n-1\}$, there is a code for sets of 
size $\binom{n}{k}$ in which every codeword has length
\begin{align*}
  \left\lceil\log \binom{n}{k}\right\rceil 
     & \le \log n! - \log k! - \log (n-k)! + 1 \\
     & = n\log n - k\log k - (n-k)\log(n-k) + (1/2)\log\left(\frac{n}{k(n-k)}\right) + O(1) \\
     & = n\log n - k\log k - (n-k)\log(n-k) + O(1) \\
     & = n\log n - k\log k - (n-k)\log n + (n-k)(\log n-\log(n-k)) + O(1) \\
     & = k\log n - k\log k + \underbrace{(n-k)(\log n-\log(n-k))}_\text{yuck!}  + O(1) 
         \numberthis \eqlabel{hassle} \\ 
\end{align*}

The subexpression $(n-k)(\log n-\log(n-k))$ is awkward here. We can
simplify it as follows:
\begin{align*}
   (n-k)(\log n-\log(n-k))
      & = (n-k)\log \left(\frac{n}{n-k}\right) \\
      & = (n-k)\log \left(1+\frac{k}{n-k}\right) \\
      & \le (n-k)(k/(n-k))\log e & \text{(since $1+x \le e^x$)} \\
      & = k\log e 
\end{align*}
Using this simplification, \eqref{hassle} becomes
\begin{equation}
  \left\lceil\log \binom{n}{k}\right\rceil 
    \le k\log n - k\log k + k\log e + O(1) \eqlabel{log-n-choose-k}
     \enspace .
\end{equation} 


\section{Applications of the Uniform Encoding Lemma}
\seclabel{applications-i}

We now start with some applications of the Uniform Encoding Lemma.

\subsection{Graphs with no Large Clique or Independent Set}


A \emph{Erd\H{o}s-R\'enyi random graph}, $G_{n,p}$ is a graph with vertex
set $V=\{1,\ldots,n\}$ and in which each edge $uw\in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.  Erd\H{o}s \cite{X} used the random
graph $G_{n,\frac{1}{2}}$ to prove the existence of graphs having no
large clique and no large independent set. Here we show how this can be
done using an encoding argument.

\begin{thm}\thmlabel{erdos-renyi-i}
  For all $n\ge 4$ and $s\ge 1$, the probability that the random graph,
  $G_{n,\frac{1}{2}}$ contains a clique or an independent set of size $t =
  \lceil 3\log n + \sqrt{2s}\rceil$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that compresses the $r=\binom{n}{2}$ bits
  of $G$'s adjacency matrix, as they appear in row-major order.  If $G$
  has no clique or independent set of size $t$, then the code consists of
  a 0 bit followed by the $\binom{n}{2}$ bits of $G$'s adjacency matrix
  in row-major order.
  
  Otherwise, $G$ contains a clique or independent set, $S$, of size
  $t$.  Then the code consists of a 1; another bit indicating whether
  $S$ is a clique or independent set; the vertices of $S$; then the
  adjacency matrix of $G$ in row major-order but leaving out all of the
  $\binom{t}{2}$ bits implied by the edges or non-edges in $S$.
  
  In the second case, this encoding requires 
  \begin{align*}
     b & = 2 + t\log n + \binom{n}{2}-\binom{t}{2} \\
       & = \binom{n}{2} + 2 + t\log n - (1/2)(t^2 - t) \\
       & = \binom{n}{2} + 2 + t\log n 
            - (1/2)\left(9\log^2 n +3\sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(3\log^2 n + \sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(t\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(t(\log n-1)\right) - s \\
       & \le \binom{n}{2} - s 
  \end{align*}
  bits.   Applying the Uniform Encoding
  Lemma completes the proof.
\end{proof}

\begin{rem}
The bound in \thmref{erdos-renyi-i} can be strengthened a little,
since the elements of $S$ can be encoded with fewer than $t\log n$
bits; in particular $\lceil\log\binom{n}{t}\rceil=t\log n - t\log t +
t\log e - O(\log t)$ bits suffice.  With a careful calculation, using
the approximations described in \secref{stirling}, the
proof then works with $t=2\log n +O(\log\log n) + \sqrt{s}$. This comes
closer to Erdős' original result, which was at the threshold $2\log n -
2\log\log n + O(1)$.
\end{rem}



\subsection{Balls in Urns}

\begin{thm}
  Suppose we throw $n$ balls indepedently and uniformly at random into $n$
  urns. Then, the probability that any urn contains more than $t=blah$
  balls is at most $2^{-s}$.
\end{thm}

\begin{proof}
  For each $i\in\{1,\ldots,n\}$, let $b_i$ denote the index of the urn in
  which the $i$th ball lands. Notice that the sequence $b_1,\ldots,b_n$
  is chosen uniformly at random from a set of size $n^n$, and it is this
  choice that will be used in our encoding argument.

  If no urn contains $t$ or more balls, then our code is a 0 bit followed
  by the obvious encoding. If some urn, $i$, contains $t$ or more balls,
  then our code is a 1 bit followed by the value $i$ ($\lceil \log
  n\rceil$ bits) followed by a code for $t$ of the balls in urn $i$
  ($\log\binom{n}{t}$) bits, followed by the remaining $n-t$ values of
  $b_1,\ldots,b_n$ that cannot be deduced from the preceding information.
  In total, this is
  \begin{align*}
    b &\le 1 + \lceil\log n\rceil + \log\binom{n}{t} 
           + \lceil(n-t)\log n\rceil \\
     & = \log n + t\log n - t\log t + t\log e  
           + (n-t)\log n + O(1)
             & \text{(by applying \eqref{log-n-choose-k})} \\
     & = n\log n - t\log t + t\log e + O(1) \\
     & \le  n\log n - s
  \end{align*}
\end{proof}

\subsection{Analysis of Insertion Sort}

Recall the insertion-sort algorithm for sorting a list, $A_1,\ldots,A_n$
of $n$ elements:

\noindent{$\textsc{InsertionSort}(A_1,\ldots,A_n)$}
\begin{algorithmic}[1]
  \FOR{$i\gets 2$ \TO $n$}
     \STATE{$j \gets i$}
     \WHILE{$j>1$ \AND $A_{j-1} > A_j$}
         \STATE{$A_j \leftrightarrow A_{j-1}$
            \COMMENT{ swap }}
         \STATE{$j\gets j-1$}
     \ENDWHILE
  \ENDFOR
\end{algorithmic}

A typical question asks the expected number of times Line~4 executes
if $A_1,\ldots,A_n$ is a uniformly random permutation of $n$ distinct
elements.  The answer $\binom{n}{2}/2$ is an easy application of
linearity of expectation.\footnote{For every one of the $\binom{n}{2}$
pairs $p,q\in\{1,\ldots,n\}$ with $p<q$, the values initially stored at
positions $A_p$ and $A_q$ will eventually be swapped if and only if $A_p >
A_q$, which happens with probability $1/2$ in a random permutation.}

A more advanced question is to ask for a concentration result on the
number of executions of Line~4. This is a harder question to tackle;
because $>$ is transitive, the $\binom{n}{2}$ events being studied have
a lot of interdependence. In the following, we show how to obtain some
concentration result with an encoding argument:

\begin{thm}\thmlabel{insertion-sort}
For a random permutation, $A_1,\ldots,A_n$ of $n$ distinct elements,
the probability that Line~4 of \textsc{InsertionSort} executes fewer than
$\alpha n^2 - n + 2$ times is at most $2^{n\log(\alpha e^2)+O(\log n)}$.
\end{thm}

\begin{proof}
Let $\pi$ be the permutation of $\{1,\ldots,n\}$ that defines the
sorted order of $A_1,\ldots,A_n$, so that $A_{\pi_j}$ is the element
that appears at position $j$ after sorting.

We encode the permutation $\pi$ by recording the execution of
InsertionSort on this permutation. In particular, we record, for each
$i\in\{2,\ldots,n\}$, the number of times, $m_i$, that Line~4 executes
during the $i$th iteration of \textsc{InsertionSort}. With this information,
one can run the following version of \textsc{InsertionSort} to recover $\pi$:

\noindent{$\textsc{InsertionSortReconstruct}(m_2,\ldots,m_n)$}:
\begin{algorithmic}[1]
  \STATE{$\pi \gets \langle 1,\ldots,n\rangle$}
  \FOR{$i\gets 2$ \TO $n$}
     \FOR{$j\gets i \textbf{ down to } i-m_i+1$}
         \STATE{$\pi_j \leftrightarrow \pi_{j-1}$
            \COMMENT{ swap }}
     \ENDFOR
  \ENDFOR
\end{algorithmic}
 
To make this work, we have to be slightly clever with this
encoding. Rather than encode $m_2,m_3,\ldots,m_n$ directly, we first
encode $m=\sum_{i=2}^{n} m_i$ using $\lceil 2\log n\rceil$ bits (since $m < n^2$). Given $m$, what
remains is to describe the partition of $m$ into $n-1$ non-negative
integers $m_2,\ldots,m_n$; there are $\binom{m+n-2}{n-2}$ such
partitions.\footnote{To see this, draw $m+n-2$ white dots on a line,
then choose $n-2$ dots to colour black. This splits the remaining $m$ white dots
up into $n-1$ groups, which determine the values of $m_2,\ldots,m_n$.}

Therefore, the values of $m_2,\ldots,m_n$ can be encoded using
\[
    b = \lceil 2\log n\rceil + \log\binom{m+n-2}{n-2}
\]
bits and this is sufficient to recover the permutation that defines
$A_1,\ldots,A_n$.  By applying \eqref{log-n-choose-k} to $b$, we obtain
\begin{align*}
  b & \le (n-2)\log(m+n-2) - (n-2)\log(n-2)  + (n-2)\log e + O(\log n) \\
    & \le n\log(m+n-2) - n\log(n-2)  + n\log e + O(\log n) \\
    & \le n\log(\alpha n^2) - n\log(n-2)  + n\log e + O(\log n) \\
    & \le n\log(\alpha n^2) - n\log n  + n\log e + O(\log n) \\
    & = 2n\log n + n\log\alpha - n\log n  + n\log e + O(\log n) \\
    & = n\log n + n\log\alpha + n\log e + O(\log n) \\
    & = \log n! + n\log\alpha + 2n\log e + O(\log n) \\
    & = \log n! + n(\log \alpha e^2) + O(\log n)  \qedhere
\end{align*}
\end{proof}

\begin{rem}
Although it doesn't require any advanced probability,
\thmref{insertion-sort} is not sharp; it only gives a non-trivial
probability when $\alpha < 1/e^2$.  To obtain a sharp bound, one can use
the fact that $m_2,\ldots,m_n$ are independent and $m_i$ is uniform over
$\{0,\ldots,i-1\}$ and then use the method of bounded differences \cite{S}
to show that $m$ is concentrated in an interval of size $O(n^{3/2})$.
\end{rem}



\section{The Non-Uniform Encoding Lemma}
\seclabel{nuel}

\section{Applications of the Non-Uniform Encoding Lemma}
\seclabel{applications-ii}


\section{Summary and Conclusions}
\seclabel{summary}


\end{document}
