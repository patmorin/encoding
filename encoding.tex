\documentclass[lotsofwhite]{patmorin}
\usepackage{pat}
\usepackage[utf8]{inputenc}
\usepackage[noend]{algorithmic}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

%\newcommand{\keywords}[1]{\vspace{2em}\noindent\textbf{Keywords:} #1}
%\newcommand{\from}{\colon}

\title{\MakeUppercase{Encoding Arguments}}
\author{Pat Morin}
\date{}

\begin{document}
\begin{titlepage}
\maketitle

\begin{abstract}
\setlength{\baselineskip}{15.84pt}
This expository article surveys a number of applications of ``encoding
arguments,'' in which probabilistic statements are proven using the fact
a uniformly random choice from a set of size $N$ can not be encoded with
fewer than $\log N$ bits on average.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}

There is no doubt that probability theory plays a fundamental role
in computer science: Some of the fastest and simplest fundamental
algorithms and data structures are randomized; average-case analysis of
algorithms relies entirely on tools from probability theory; and many
difficult combinatorial questions have strikingly simple solutions using
probabilistic arguments.

Unfortunately, many of these beautiful results are inaccessible to most
computer scientists because of a view that ``the math is too hard.''
For instance, ACM's CS2013 Final Report does not require a full course
in probability theory \cite[Page~50]{acm2013}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on discrete
probability, as part of the discrete structures curriculum.

In this expository paper, we survey applications of ``encoding arguments''
that tranforms the problem of upper-bounding the probability of a specific
event, $\mathcal{E}$, into the problem of devising a code for the set
of elementary events that happens to be particularly short whenever
$\mathcal{E}$ occurs.  Encoding arguments have several advantages over
traditional probabilistic analysis:

\begin{enumerate}
  \item Encoding arguments are almost ``probability-free.''  Except for
  applying a simple Encoding Lemma, there is no probability involved.
  In particular, there is no chance to make common mistakes such as
  multiplying probabilities of non-independent events or (equivalently)
  multiplying expectations.  

  The proof of the encoding lemma itself is trivial and the only
  probability it uses is that the fact, if a finite set $X$ contains $r$
  special elements and we pick an element uniformly at random from $X$,
  then the probability of picking a special element is $r/|X|$.

  \item Encoding arguments usually yield strong results;
  $\Pr\{\mathcal{E}\}$ typically decreases at least exponentially in
  the parameter of interest. Traditionally, these strong concentration
  results require (at least) carefuly calculations on probabilities of
  independent events and/or the application of concentration inequalities.
  The subject of concentration inequalities is advanced enough to be
  the topic of entire textbooks \cite{A,B}.
  
  \item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing an
  efficient code---an algorithmic problem. Consider the following 
  two problems:
    \begin{enumerate}

    \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
       a random graph on $n$ vertices contains a clique of size $k=\lceil
       4\log n\rceil$.

    \item Design a encoding for graphs on $n$ vertices so that a graph,
       $G$, that contains a clique of size $k=\lceil 4\log n\rceil$
       is encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note:
       Your encoding and decoding algorithms don't have to be efficient,
       just correct.)
    \end{enumerate}
  Many computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that
  they can use Boole's Inequality will still be stuck
  wrestling with the formula $\binom{n}{4\log n}2^{-\binom{k}{2}}$.  
\end{enumerate}

Our motivation for this work is that encoding arguments are an easily
accessible, yet versatile tool for answering many questions.  Most of
these arguments can be applied after learning almost no probability
theory beyond the Encoding Lemma mentioned above.

The remainder of this article is organized as follows: In \secref{uel},
we present necessary background, including the \emph{Uniform
Encoding Lemma}, which is the basis of most of our encoding arguments.
\Secref{applications-i} presents  applications of the Uniform Encoding
Lemma to a variety of problems.  \Secref{nuel} presents a more
general Non-Uniform Encoding Lemma that can handle a larger variety of
applications, some of which are presented in \secref{applications-ii}.
\Secref{summary} summarizes and concludes with some directions for
future research.



\section{Background}
\seclabel{uel}

This section presents the necessary background on prefix-free codes and binomial coefficients.

\subsection{Prefix-free Codes}

A \emph{code}, $C\from X\to \{0,1\}^*$ is a one-to-function from a set
$X$ to the set of binary strings.  The elements of the range of $C$ are
called $C$'s \emph{codewords}.

A code, $C$, is \emph{prefix-free} if, for every $x,y\in X$ the binary
string $C(x)$ is not a prefix of $C(y)$.  It can be helpful to think
of prefix-free codes as (rooted ordered) binary trees whose leaves
are labelled with the elements in $S$.  The codeword for a particular
$x\in X$ is obtained by tracing the root-to-leaf path leading to $x$
and outputting a 0 (respectively, 1) each time this path goes from a
parent to its left (respectively, right) child. (See \figref{bintree}.)

\begin{figure}
  \centering{\includegraphics{bintree}}
  \caption{A prefix-free code for the set
    $S=\{\mathtt{a},\mathtt{b},\mathtt{c},\mathtt{d},\mathtt{e},\mathtt{f}\}$
    and the corresponding leaf-labelled binary tree.}
  \figlabel{bintree}
\end{figure}

If $C$ is prefix-free, then the number of $C$'s codewords that have length
\emph{at most} $k$ is not more than $2^k$. To see this why this is so,
observe that $C$ can be modified into a code $\hat C$, in which every
codeword of length $\ell <k$ is extended---by appending $k-\ell$ zeros---so that
it has length exactly $k$. The prefix-freeness of $C$ ensures that $\hat
C$ is also a prefix-free code). The number of $\hat C$'s codewords of
length $k$ is equal to the the number of $C$'s codewords of length at
most $k$; since codewords are just binary strings, there are not more
than $2^k$ of these.

Observe that every finite set $X$ has a prefix-free code in which every
codeword has length $\lceil\log |X|\rceil$. We simply enumerate the
elements of $X$ in some order $x_0,x_1,\ldots,x_{|X|-1}$ and assign to
each $x_i$ the binary representation of $i$ (padded with leading zeros),
which has length $\lceil\log |X|\rceil$, since $i\in\{0,\ldots,|X|-1\}$.
We will use this type of fixed-length code implicitly in many arguments.

As we will see, the following lemma is surprisingly versatile:
\begin{lem}[Uniform Encoding Lemma]\lemlabel{uel}
  Let $C\from X\to \{0,1\}^*$ be a prefix-free code. If an element $x\in
  X$ is chosen uniformly at random, then $\Pr\{|C(x)|\le \log|S|-s\}\le
  2^{-s}$.
\end{lem}

\begin{proof}
  Since $C$ is a one-to-one function, choosing $x$ uniformly from $S$
  implies that $C(x)$ is chosen uniformly from $C$'s codewords. Let
  $k=\log|X|-s$. Since $C$ has $|X|$ codewords and at most $2^{k}$ of these
  have length at most $k$, the probability that $x$ is one of these 
  codewords is at most
  \[
     \frac{2^k}{|X|} = \frac{2^{\log|S|-s}}{|X|} = 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

\subsection{Runs in Binary Strings}

As a warm-up exercise to illustrate the use of the Uniform Encoding
Lemma we will show that a random $n$-bit string is unlikely to contain
a run of significantly more than $\log n$ one bits.

\begin{thm}\thmlabel{runs-i}
  Let $x=\langle x_1,\ldots,x_n\rangle\in\{0,1\}^n$ be chosen
  uniformly at random and let $t=1+\lceil\log n\rceil + s$. Then, the
  probability that there exists an $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  We will prove this theorem by constructing a prefix-free code
  for $x$. If there is no $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+k-1}=1$, then $C(x)$ is a zero bit
  followed by the $n$ bits $x_1,\ldots,x_n$.  However, if there is
  such an index, $i$, then $C(x)$ is a one bit, followed by (the
  $\log n$-bit binary encoding of) $i$, followed by the $n-t$ bits
  $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$.

  Observe that, in the latter case, $C(x)$ has length 
  \[
      1 + \lceil\log n\rceil + n - t = n-s \enspace .
  \]
  It is easy to check that, for any $x$, we can reconstruct
  $x_1,\ldots,x_n$ from $C(x)$, so $C$ is indeed a code for $\{0,1\}^n$.
  It is also easy to check that $C$ is prefix free, since the first bit of
  $C(x)$ determines the length of $C(x)$ (which is either $n+1$ or $n-s$).

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, the probability
  that there exists any index $i\in\{1,\ldots,n-k-1\}$ such that
  $x_i=x_{i+1}=\cdots=x_{i+k-1}=1$ is at most
  \[
      \Pr\{|C(x)|\le n-s\} \le 2^{-s} \enspace . \qedhere 
  \]
\end{proof}

Simple as it is, the proof of \thmref{runs-i} contains the main ideas
used in most encoding arguments:

\begin{enumerate}
  \item The arguments usually show that a particular \emph{bad event}
  is unlikely.  In \thmref{runs-i} the bad event is the occurrence of a long
  string of consecutive 1's.

  \item The length of the codeword is irrelevant to the argument if
  the bad event does not occur.  In \thmref{runs-i}
  the codeword has length $n+1$ in this case.
 
  \item The code often starts with a 1 or 0 to flag the occurrence or
  non-occurrence of the bad event.

  \item When the bad event does occur, the code usually begins with
  a concise description of the bad event, and is then followed by a
  straightforward encoding of the information that is not implied by the
  bad event. In \thmref{runs-i}, the bad event is completely described
  by the index $i$ at which the run of 1 bits begins, and this implies
  that the bits $x_i,\ldots,x_{i+t-1}$ are all equal to 1, so these
  bits do not need to be specified in the second part of the codeword. 
\end{enumerate}

\subsection{Stirling Approximations}
\seclabel{stirling}

Before moving on to some more advanced applications, it will be helpful
to remind the reader of a few inequalities that can be derived from
Stirling's Approximation of $n!$.  Recall that Stirling's Approximation
states that
\begin{equation}
  n! = \left(\frac{n}{e}\right)^n\sqrt{2\pi n}\left(1+O\left(\frac{1}{n}\right)\right) 
   \eqlabel{stirling}
\end{equation}

\Eqref{stirling} immediately implies that there is a code for sets
of size $n!$ in which every codeword has length
\begin{align}
  \log n!
      & \le n\log n - n\log e + (1/2)\log n + \log(1+O(1/n)) \notag \\
      & \le n\log n - n\log e + (1/2)\log n + O(1/n)  
             & \text{(since $1+x < e^x$)}
               \eqlabel{stirling-tight} \\
      & \le n\log n - n\log e + O(\log n)  
             \eqlabel{stirling-loose} \enspace .
\end{align}

Similarly, for any $k\in\{1,\ldots,n-1\}$, there is a code for sets of 
size $\binom{n}{k}$ in which every codeword has length
\begin{align*}
  \log \binom{n}{k}
     & = \log n! - \log k! - \log (n-k)! \\
     & \le n\log n - k\log k - (n-k)\log(n-k) + (1/2)\log\left(\frac{n}{k(n-k)}\right) + O(1/n) \\
     & = n\log n - k\log k - (n-k)\log(n-k) + O(1/n) \\
     & = n\log n - k\log k - (n-k)\log n + (n-k)(\log n-\log(n-k)) + O(1/n) \\
     & = k\log n - k\log k + \underbrace{(n-k)(\log n-\log(n-k))}_\text{yuck!}  + O(1/n) 
         \numberthis \eqlabel{hassle} \\ 
\end{align*}

The subexpression $(n-k)(\log n-\log(n-k))$ is awkward here. We can
simplify it as follows:
\begin{align*}
   (n-k)(\log n-\log(n-k))
      & = (n-k)\log \left(\frac{n}{n-k}\right) \\
      & = (n-k)\log \left(1+\frac{k}{n-k}\right) \\
      & \le (n-k)(k/(n-k))\log e & \text{(since $1+x \le e^x$)} \\
      & = k\log e 
\end{align*}
Using this simplification, \eqref{hassle} becomes
\begin{equation}
  \left\lceil\log \binom{n}{k}\right\rceil 
    \le k\log n - k\log k + k\log e + O(1/n) \eqlabel{log-n-choose-k}
     \enspace .
\end{equation} 


\section{Applications of the Uniform Encoding Lemma}
\seclabel{applications-i}

We now start with some applications of the Uniform Encoding Lemma.

\subsection{Graphs with no Large Clique or Independent Set}


A \emph{Erd\H{o}s-R\'enyi random graph}, $G_{n,p}$ is a graph with vertex
set $V=\{1,\ldots,n\}$ and in which each edge $uw\in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.  Erd\H{o}s \cite{X} used the random
graph $G_{n,\frac{1}{2}}$ to prove the existence of graphs having no
large clique and no large independent set. Here we show how this can be
done using an encoding argument.

\begin{thm}\thmlabel{erdos-renyi-i}
  For all $n\ge 4$ and $s\ge 1$, the probability that the random graph,
  $G_{n,\frac{1}{2}}$ contains a clique or an independent set of size $t =
  \lceil 3\log n + \sqrt{2s}\rceil$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that compresses the $r=\binom{n}{2}$ bits
  of $G$'s adjacency matrix, as they appear in row-major order.  If $G$
  has no clique or independent set of size $t$, then the code consists of
  a 0 bit followed by the $\binom{n}{2}$ bits of $G$'s adjacency matrix
  in row-major order.
  
  Otherwise, $G$ contains a clique or independent set, $S$, of size
  $t$.  Then the code consists of a 1; another bit indicating whether
  $S$ is a clique or independent set; the vertices of $S$; then the
  adjacency matrix of $G$ in row major-order but leaving out all of the
  $\binom{t}{2}$ bits implied by the edges or non-edges in $S$.
  
  In the second case, this encoding requires 
  \begin{align*}
     b & = 2 + t\log n + \binom{n}{2}-\binom{t}{2} \\
       & = \binom{n}{2} + 2 + t\log n - (1/2)(t^2 - t) \\
       & = \binom{n}{2} + 2 + t\log n 
            - (1/2)\left(9\log^2 n +3\sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(3\log^2 n + \sqrt{2s}\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(t\log n + 2s - t\right) \\
       & = \binom{n}{2} + 2 
            - (1/2)\left(t(\log n-1)\right) - s \\
       & \le \binom{n}{2} - s 
  \end{align*}
  bits.   Applying the Uniform Encoding
  Lemma completes the proof.
\end{proof}

\begin{rem}
The bound in \thmref{erdos-renyi-i} can be strengthened a little,
since the elements of $S$ can be encoded with fewer than $t\log n$
bits; in particular $\lceil\log\binom{n}{t}\rceil=t\log n - t\log t +
t\log e - O(\log t)$ bits suffice.  With a careful calculation, using
the approximations described in \secref{stirling}, the
proof then works with $t=2\log n +O(\log\log n) + \sqrt{s}$. This comes
closer to Erdős' original result, which was at the threshold $2\log n -
2\log\log n + O(1)$.
\end{rem}



\subsection{Balls in Urns}
\seclabel{urns}

The question of what happens when one throws $n$ balls, randomly and
indepently into $n$ urns is a useful abstraction of many algorithmic,
data structures and load-balancing problems.

\begin{thm}\thmlabel{urns}
  Let $t$ be such that $t\log(t/e)-\log n-4 \ge s$ and suppose we throw
  $n$ balls independently and uniformly at random into $n$ urns. Then,
  for sufficiently large $n$, the probability that any urn contains more 
  than $t$ balls is at most $2^{-s}$.
\end{thm}

Note that, for any constant $\epsilon >0$ and all sufficiently large
$n$, taking $t = (1+\epsilon)\log n/\log\log n$ satisfies the requirements
of \thmref{urns}.

\begin{proof}
  For each $i\in\{1,\ldots,n\}$, let $b_i$ denote the index of the urn in
  which the $i$th ball lands. Notice that the sequence $b_1,\ldots,b_n$
  is chosen uniformly at random from a set of size $n^n$, and it is this
  choice that will be used in our encoding argument.

  If no urn contains $t$ or more balls, then our code is a 0 bit followed
  by the obvious encoding. If some urn, $i$, contains $t$ or more balls,
  then our code is a 1 bit followed by the $\lceil \log n\rceil$-bit
  encoding of $i$, followed by a $\lceil\log\binom{n}{t}\rceil$-bit
  code that describes $t$ of the balls in urn $i$, followed by the
  remaining $n-t$ values of $b_1,\ldots,b_n$ that cannot be deduced from
  the preceding information.  In total, this is
  \begin{align*}
    b &\le \log n + \log\binom{n}{t} 
           + (n-t)\log n + 3 \\
     & = \log n + t\log n - t\log t + t\log e  
           + (n-t)\log n + 3 + O(1/n)
             & \text{(using \eqref{log-n-choose-k})} \\
     & = \log n + t\log n - t\log t + t\log e  
           + (n-t)\log n + 4
             & \text{(for large enough $n$)} \\
     & = (n+1)\log n - t\log t + t\log e + 4 \\
     & \le  n\log n - s &\text{(by the choice of $t$)} \\
     & =  \log n^n - s \enspace . & \qedhere
  \end{align*}
\end{proof}

\subsection{Linear Probing}

While balls in urns is a valid abstraction of hashing with chaining, a more practically efficient form of a hashing is \emph{linear probing}.  
In a linear-probing hash table, we hash $n$ items $x_1,\ldots,x_n$ into
a linear probing hash table of size $m=cn$, $c\ge 1$. When we insert
$x_i$ we try and place it at table position $j=h(x_i)$ unless it is
already occupied by one of $x_1,\ldots,x_{i-1}$ in which case we try
table location $(j+1)\bmod m$, $(j+2)\bmod m$, and so on until we find
an empty spot for $x_i$.  We want to study the expected search time for
some item $x_i$ in this hash table.

We call a maximal consecutive (the table locations $m-1$ and
$0$ are considered consecutive) sequence of occupied table locations
a \emph{block}.

\begin{thm}
  Fix some $x\in\{x_1,\ldots,x_n\}$. Then the probability that the block
  containing $x$ has size $t\ge 1+(s+\log t + O(1))/\log(c/e)$ is at most $2^{-s}$.
\end{thm}

\begin{proof}
  This is an encoding argument that encodes the vector 
  \[
     R = h(x_1),h(x_2),\ldots,h(x_n)
  \]
  which is drawn uniformly at random from a set $X$ of size $n^{m}=n^{cn}$.
  
  If the block containing $x$ does not have size $t$ then we encode $R$
  as a 0 bit followed by the obvious encoding of $h(x_1),\ldots,h(x_n)$.
  
  Otherwise, we encode $R$ as a 1 bit followed by the first index,
  $b$, of the block containing $x$; followed by the $t-1$ elements,
  $y_1,\ldots,y_{t-1}$, of this block (excluding $x$); followed
  by enough information to decode the hash values of $h(x)$ and
  $h(y_1),\ldots,h(y_{t-1})$; followed by $h(x_1),\ldots,h(x_n)$ excluding
  the $t$ hash values we can compute from the preceding information.

  Notice that $h(x),h(y_1),h(y_2),\ldots,h(y_{t-1})$ are in the range
  $b,\ldots,b+t-1$ (modulo $m$) so these can be encoded using $\lceil
  t\log t\rceil$ bits.  Therefore, if the block containing $x$ has size $t$,
  then we obtain a codeword whose length is
  \begin{align*}
    b & = \log m + \log\binom{n}{t-1} + t\log t + (n-t)\log m + O(1) \\
       & \le \log m + (t-1)\log n - 
           {\color{red}(t-1)\log(t-1)} + (t-1)\log e + {\color{red}t\log t} + (n-t)\log m + {\color{red}O(1)}\\
       & \le {\color{blue}\log m} + {\color{blue}(t-1)\log n} + (t-1)\log e + {\color{blue}(n-t)\log m} + {\color{red}\log t}+ {\color{red} O(1)}\\
       & = {\color{blue}n\log m} - {\color{blue}(t-1)\log c} + (t-1)\log e + \log t + O(1) \\
       & = n\log m - (t-1)\log(c/e) + \log t + O(1) \\
       & \le n\log m - s \enspace ,
  \end{align*}
  for $t\ge (s+\log t+O(1))/\log (c/e) + 1$.
\end{proof}



\subsection{Bipartite Expanders}

Consider a random bipartite multigraph $G=(A,B,E)$ where $|A|=|B|=n$
and the $3n$ edges in $E$ are obtained as follows:  Each vertex $u\in A$
chooses 3 random neighbours, $x(u)$, $y(u)$, and $z(u)$, in $B$, with
replacement.  All choices of edges are independent. For a set $A'\subseteq
A$, the \emph{neighbourhood} of $A'$, denoted $N(A')$, is the subset of
vertices in $B$ that are adjacent to at least one element of $A'$.

The following theorem shows that $G$ is an expander.  The proof of this
theorem usually involves a messy sum that contains binomial coefficients
and probabilities.  See, e.g., So and So \cite{S}.

\begin{thm}
  There exists a constant $\alpha >0$ such that, with probability at
  least $1-2^{-\Omega(n^{1/2})}$, for all $A'\subset A$ with $|A'|\le
  \alpha n$, $|N(A')| \ge 3|A'|/2$.
\end{thm}

\begin{proof}
Without loss of generality, let $A=\{1,\ldots,n\}$.  We use an encoding argument on the bit string
\[
   R = x(1), y(1), z(1), x(2), y(2), z(2), \ldots, x(n), y(n), z(n) \enspace ,
\]
which is chosen uniformly from a set $X$ of size $n^{3n}$.

If no set $A'$ violates the conditions of the lemma, then the code for
$R$ is a zero bit followed by the obvious $3n\log n$ bit encoding of $R$.

If some set $A'$, $|A'|=k\le \alpha n$, violates the conditions of the
lemma, then we encode a one bit, followed by $k$, $A'$, $N(A')$ and
then the edges between $A'$ and $N(A')$.  Then we encode the rest of
$R$, skipping the $3k\log n$ bits devoted to elements in $A'$.  The key
savings here comes because $N(A')$ should take $3k\log n$ bits to encode,
but can actually be encoded in roughly $3k\log(3k/2)$ bits.

The total size of the first part of our encoding is, which includes $k$,
$A'$ and $N(A')$ is
\begin{align*}
    h & = 2\log k + \log\binom{n}{k} + \log\binom{n}{3k/2} + 3k\log (3k/2) \\
       & \le k\log n - k\log k + (3/2)k\log n - (3/2)k\log k + 3k\log k + k\log(3/2) \\
      & = (5/2)k\log n + (1/2)k\log k + k\log(3/2)  \enspace .
\end{align*}
The size of the second part of our encoding is $3n\log n - 3k\log n$.  The savings, $s=s(k)$, we obtain is therefore
\begin{align*}
     s(k) & = 3k\log n - (5/2)k\log n - (1/2)k\log k - k\log(3/2) \\
       & = (1/2)k(\log n - \log(2k/3)) \\
       & = (1/2)k(\log n - \log k - \log(3/2)) \\
       & = (1/2)k\log(2n/(3k)) \enspace .
\end{align*}
The function $s(k)$ is concave and is therefore minimized when $k$ is extremal. For $k=1$, we have 
\[
    s(1)=(1/2)\log n - \log(2/3)
\]
and $2^{-s(1)} = O(n^{-1/2})$.  For $k=\alpha n$ we have
\[
   s(\alpha n) = (\alpha n/2)\log(2/(3\alpha))
\]
and $2^{-s(\alpha n)} = 2^{-\Omega(n)}$ for $\alpha < 2/3$.
\end{proof}


\subsection{Analysis of Insertion Sort}

Recall the insertion-sort algorithm for sorting a list, $A_1,\ldots,A_n$
of $n$ elements:

\noindent{$\textsc{InsertionSort}(A_1,\ldots,A_n)$}
\begin{algorithmic}[1]
  \FOR{$i\gets 2$ \TO $n$}
     \STATE{$j \gets i$}
     \WHILE{$j>1$ \AND $A_{j-1} > A_j$}
         \STATE{$A_j \leftrightarrow A_{j-1}$
            \COMMENT{ swap }}
         \STATE{$j\gets j-1$}
     \ENDWHILE
  \ENDFOR
\end{algorithmic}

A typical question asks the expected number of times Line~4 executes
if $A_1,\ldots,A_n$ is a uniformly random permutation of $n$ distinct
elements.  The answer $\binom{n}{2}/2$ is an easy application of
linearity of expectation: For every one of the $\binom{n}{2}$ pairs
$p,q\in\{1,\ldots,n\}$ with $p<q$, the values initially stored at
positions $A_p$ and $A_q$ will eventually be swapped if and only if $A_p >
A_q$, which happens with probability $1/2$ in a random permutation.

A more advanced question is to ask for a concentration result on the
number of executions of Line~4. This is a harder question to tackle;
because $>$ is transitive, the $\binom{n}{2}$ events being studied have
a lot of interdependence. In the following, we show how to obtain a
concentration result with an encoding argument.  The argument presented
here follows the same outline as Vitanyi's analysis of bubble sort
\cite{vXX}, though without all the trappings of Kolmogorov complexity.

\begin{thm}\thmlabel{insertion-sort}
For a random permutation, $A_1,\ldots,A_n$ of $n$ distinct elements,
the probability that Line~4 of \textsc{InsertionSort} executes fewer
than $\alpha n^2 - n + 2$ times is at most $2^{n\log(\alpha e^2)+O(\log
n)}$.  In particular, for a fixed $\alpha < 1/e^2$, this probability
is $2^{-\Omega(n)}$.
\end{thm}

\begin{proof}
Let $\pi$ be the permutation of $\{1,\ldots,n\}$ that defines the sorted
order of $A_1,\ldots,A_n$, so that $A_{\pi_j}$ is the element that appears
at position $j$ after sorting. Notice that $\pi$ is chosen uniformly at
random among all $n!$ permutations of of $\{1,\ldots,n\}$.

We encode the permutation $\pi$ by recording the execution of
InsertionSort on this permutation. In particular, we record, for each
$i\in\{2,\ldots,n\}$, the number of times, $m_i$, that Line~4 executes
during the $i$th iteration of \textsc{InsertionSort}. With this information,
one can run the following version of \textsc{InsertionSort} to recover $\pi$:

\noindent{$\textsc{InsertionSortReconstruct}(m_2,\ldots,m_n)$}:
\begin{algorithmic}[1]
  \STATE{$\pi \gets \langle 1,\ldots,n\rangle$}
  \FOR{$i\gets 2$ \TO $n$}
     \FOR{$j\gets i \textbf{ down to } i-m_i+1$}
         \STATE{$\pi_j \leftrightarrow \pi_{j-1}$
            \COMMENT{ swap }}
     \ENDFOR
  \ENDFOR
\end{algorithmic}
 
To make this work, we have to be slightly clever with this
encoding. Rather than encode $m_2,m_3,\ldots,m_n$ directly, we first
encode $m=\sum_{i=2}^{n} m_i$ using $\lceil 2\log n\rceil$ bits (since $m < n^2$). Given $m$, what
remains is to describe the partition of $m$ into $n-1$ non-negative
integers $m_2,\ldots,m_n$; there are $\binom{m+n-2}{n-2}$ such
partitions.\footnote{To see this, draw $m+n-2$ white dots on a line,
then choose $n-2$ dots to colour black. This splits the remaining $m$ white dots
up into $n-1$ groups, which determine the values of $m_2,\ldots,m_n$.}

Therefore, the values of $m_2,\ldots,m_n$ can be encoded using
\[
    b = \lceil 2\log n\rceil + \log\binom{m+n-2}{n-2}
\]
bits and this is sufficient to recover the permutation that defines
$A_1,\ldots,A_n$.  By applying \eqref{log-n-choose-k} to $b$, we obtain
\begin{align*}
  b & \le (n-2)\log(m+n-2) - (n-2)\log(n-2)  + (n-2)\log e + O(\log n) \\
    & \le n\log(m+n-2) - n\log n   + n\log e + O(\log n) \\
    & \le n\log(\alpha n^2) - n\log n  + n\log e + O(\log n) \\
    & = 2n\log n + n\log\alpha - n\log n  + n\log e + O(\log n) \\
    & = n\log n + n\log\alpha + n\log e + O(\log n) \\
    & = \log n! + n\log\alpha + 2n\log e + O(\log n) \\
    & = \log n! + n(\log \alpha e^2) + O(\log n)  \qedhere
\end{align*}
\end{proof}

\begin{rem}
  Although it doesn't require any advanced probability,
  \thmref{insertion-sort} is not sharp; it only gives a non-trivial
  probability when $\alpha < 1/e^2$.  To obtain a sharp bound, one
  can use the fact that $m_2,\ldots,m_n$ are independent and $m_i$ is
  uniform over $\{0,\ldots,i-1\}$ and then use the method of bounded
  differences \cite{S} to show that $m$ is concentrated in an interval
  of size $O(n^{3/2})$.
\end{rem}




\section{The Non-Uniform Encoding Lemma}
\seclabel{nuel}

\section{Applications of the Non-Uniform Encoding Lemma}
\seclabel{applications-ii}


\section{Summary and Conclusions}
\seclabel{summary}


\section*{Acknowledgement}

This research was initiated in response to an invitation to give
a talk at the Ninth Annual Probability, Combinatorics and Geometry
Workshop, held April 4--11 at McGill University's Bellairs Institute.
Many ideas that appear in the current paper were developed during the
workshop. The author is grateful to the other workshop participants for
providing a stimulating working environment.  In particular, Xing~Shi~Cai
pointed out the application to runs in binary strings (\thmref{runs-i})
and G\'abor~Lugosi stated and proved the Non-Uniform Encoding Lemma
(\lemref{nuel}) in response to the author's half-formed ideas about a
non-uniform version of \lemref{uel}.

\end{document}
