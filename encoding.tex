\documentclass[lotsofwhite]{patmorin}
\usepackage{pat}
\usepackage[utf8]{inputenc}


%\newcommand{\keywords}[1]{\vspace{2em}\noindent\textbf{Keywords:} #1}
%\newcommand{\from}{\colon}

\title{Encoding Arguments}
\author{Pat Morin}
\date{}

\begin{document}
\begin{titlepage}
\maketitle

\begin{abstract}
This expository article surveys a number of applications of ``encoding
arguments,'' in which probabilistic statements are proven using the fact
a uniformly random choice from a set of size $N$ can not be encoded with
fewer than $\log N$ bits on average.
\end{abstract}

\keywords{Encoding arguments, entropy, Kolmogorov complexity, incompressibility, random graphs, expanders, \ldots}

\end{titlepage}
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

\section{Introduction}

There is no doubt that probabilistic analysis plays a fundamental role in
computer science: Some of the fastest and simplest fundamental algorithms
and data structures are randomized; average-case analysis of algorithms
relies entirely on tools from probability theory; and many combinatorial
questions are most easily resolved using probabilistic arguments.  

Unfortunately, many of these beautiful results are inaccessible to most
computer scientists because of a view that ``the math is too hard.''
For instance, ACM's CS2013 Final Report does not require a full course
in probability theory \cite[Page~50]{acm2013}. Indeed, the report
recommends a total of 6 Tier-1 hours and 2 Tier-2 hours spent on discrete
probability, as part of the discrete structures curriculum.

In this expository paper, we survey applications of ``encoding arguments''
that tranforms the problem of upper-bounding the probability of a
specific event, $\mathcal{E}$, into the problem of devising a code that
is particularly short whenever $\mathcal{E}$ occurs.  Encoding arguments
have several advantages over traditional probabilistic analysis:

\begin{enumerate}
  \item Except for applying a simple Encoding Lemma, there is no probability
  involved.  In particular, there is no chance to make common mistakes such
  as multiplying probabilities of non-independent events or (equivalently)
  multiplying expectations.
  
  \item In their most common form, these arguments usually yield very
  strong results; the $\Pr\{\mathcal{E}\}$ typically decreases at least
  exponentially in the parameter of interest. Traditionally, these strong
  concentration results require (at least) careful on independent events
  and/or the application of concentration inequalities.
  
  \item Encoding arguments are natural for computer scientists. They
  turn a probabilistic analysis problem into the problem of designing an
  efficient encoding---an algorithmic problem. Consider the following 
  two problems:
    \begin{enumerate}

    \item Prove an upper-bound of $1/n^{\log n}$ on the probability that
       a random graph on $n$ vertices contains a clique of size $k=\lceil
       4\log n\rceil$.

    \item Design a encoding for graphs on $n$ vertices so that a graph,
       $G$, that contains a clique of size $k=\lceil 4\log n\rceil$
       is encoded using at most $\binom{n}{2}-\log^2 n$ bits. (Note:
       Your encoding and decoding algoritms don't have to be efficient,
       just correct.)
    \end{enumerate}
  Most computer science undergraduates would not know where to start
  on the first problem.  Even a good student who realizes that
  they can use Boole's Inequality will still be stuck
  wrestling with the formula $\binom{n}{4\log n}2^{-\binom{k}{2}}$.  
\end{enumerate}



\section{The Uniform Encoding Lemma}

A \emph{code}, $C\from X\to \{0,1\}^*$ is a one-to-function from
a set $X$ to the set of binary strings.  The elements of the range of
$C$ are called $C$'s \emph{codewords}.  It is clear that, for
any code $C$, the number of codewords (binary strings) of length $k$
is at most $2^k$.

A code, $C$, is \emph{prefix-free} if, for every $x,y\in X$ the binary
string $C(x)$ is not a prefix of $C(y)$.  It can be helpful to think of
prefix-free codes as binary trees whose leaves are labelled with the
elements in $S$.  The codeword for a particular $x\in X$ is obtained
by tracing the root-to-leaf path leading to $x$ and outputting a 0
(respectively, 1) each time this path goes from a parent to its left
(respectively, right) child. (See \figref{bintree}.)

If $C$ is prefix-free, then the number of $C$'s codewords that have
length \emph{at most} $k$ is not more than $2^k$. To see this why this
is so, note that $C$ can be modified into a code $\hat C$, in which
every codeword of length less than $k$ is extended so that it has length
exactly $k$. The prefix-freeness of $C$ guarantees that $\hat C$ remains
one-to-one, so it is indeed a code.


\begin{lem}[Uniform Encoding Lemma]
Let $C\from S\to \{0,1\}^*$ be a prefix-free code. When an element $x$ is chosen uniformly at random from $S$, $\Pr\{|C(x)|\le \log|S|-s\}\le 2^{-s}$.
\end{lem}

\begin{proof}
Since $C$ is a one-to-one function, choosing $x$ uniformly from $S$
means that $C(x)$ is chosen uniformly form $C$'s $|S|$ codewords. Since
$C$ has at most $q=2^{\log|S|-s}=|S|/2^s$ codewords of length at most
$\log|S|-s$, the probability that we choose one of these codewords is
at most $q/|S|=2^{-s}$.
\end{proof}

\subsection{Runs in a Random Binary String}

As a warm-up exercise, we show how the Uniform Encoding Lemma can be
used to show that a random $n$-bit string is unlikely to contain a run
of significantly more than $\log n$ one bits.

\begin{thm}\thmlabel{one-runs}
  Let $x=\langle x_1,\ldots,x_n\rangle\in\{0,1\}^n$ be chosen uniformly
  at random and let $t=1+\lceil\log n\rceil + s$. Then, the probability
  that there exists an $i$ such that $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$
  is at most $2^{-s}$.
\end{thm}

\begin{proof}
  Consider the following prefix-free code, $C\from\{0,1\}^n\to\{0,1\}^*$.
  If there is no index $i$ such that $x_i=x_{i+1}=\cdots=x_{i+t-1}=1$,
  then $C(x)$ is a zero bit followed by the $n$ bits of $x$.
  However, if there is such an index $i$, then $C(x)$ is a one bit,
  followed by (the binary encoding of) $i$, followed by the $n-k$ bits
  $x_1,\ldots,x_{i-1},x_{i+t},\ldots,x_n$.

  Observe that, in the latter case, $C(x)$ has length 
  \[
      1 + \lceil\log n\rceil + n - k = n-s \enspace .
  \]

  It is easy to check that, for any $x$, we can reconstruct
  $x_1,\ldots,x_n$ from $C(x)$, so $C$ is indeed a code for $\{0,1\}^n$.
  It is also easy to check that $C$ is prefix free, since the first bit
  of $C(x)$ determines the length of $C(x)$.

  Now, $x$ was chosen uniformly at random from a set of size $2^{n}$.
  Therefore, by the Uniform Encoding Lemma, the probability of the latter
  case is at most $2^{-s}$.
\end{proof}

\subsection{Graphs with no Large Clique or Stable Set}


A \emph{Erd\H{o}s-R\'enyi random graph}, $G_{n,p}$ is a graph with vertex
set $V=\{1,\ldots,n\}$ and in which each edge $uw\in \binom{V}{2}$
is present with probability $p$ and absent with probability $1-p$,
independently of the other edges.  Erd\H{o}s \cite{X} used the random
graph $G_{n,\frac{1}{2}}$ to prove the existence of graphs having no
large clique and no large independent set. Here we show how this can be
done using an encoding argument.

\begin{thm}\thmlabel{erdos-renyi-i}
For any $\epsilon>0$ and for all sufficiently large $n$, the probability
that the random graph, $G_{n,\frac{1}{2}}$ contains a clique or an
independent set of size $t = \lceil(2+\epsilon)\log n + \sqrt{s}\rceil$
is at most $2^{-s}$.
\end{thm}

\begin{proof}
This is an encoding argument that compresses the $r=\binom{n}{2}$ bits
of $G$'s adjacency matrix, as they appear in row-major order.  If $G$
has no clique or independent set of size $t$, then the code consists of
a 0 bit followed by the $\binom{n}{2}$ bits of $G$'s adjacency matrix
in row-major order.

Otherwise, $G$ contains a clique or independent set, $S$, of size $t$.
Then the code consists of a 1; another bit indicating whether $S$ is a
clique or IS; the vertices of $S$; then the adjacency matrix of $G$ in
row major-order but leaving out all of the $\binom{t}{2}$ bits implied
by the edges or non-edges in $S$.

In the second case, this encoding requires 
\begin{align*}
   b & = 2 + t\log n + \binom{n}{2}-\binom{t}{2} \\
     & \le \binom{n}{2} - s 
\end{align*}
bits (because of our choice of $t$).   Applying the Uniform Encoding
Lemma completes the proof.
\end{proof}

\begin{rem}
The bound in \thmref{erdos-renyi-i} can be strengthened a little, since
the elements of $S$ can be encoded with fewer than $t\log n$ bits; in
particular $\lceil\log\binom{n}{t}\rceil=t\log n - t\log t + t\log e -
O(\log t)$ bits suffice.  With a careful calculation, the proof then
works with $t=2\log n +O(\log\log n) + \sqrt{s}$. This comes closer to
Erd≈ës' original result, which was at the threshold $2\log n - 2\log\log
n + O(1)$.
\end{rem}



\end{document}
